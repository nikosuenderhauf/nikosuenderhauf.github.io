<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niko Sünderhauf | publications</title>
  <meta name="description" content="">

  <link rel="shortcut icon" href="https://nikosuenderhauf.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/main.css">
  <link rel="canonical" href="https://nikosuenderhauf.github.io/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Niko</strong> Sünderhauf
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://nikosuenderhauf.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/cv/">bio</a>
          
        
          
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/projects/">research</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/jobs/">recruiting</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/workshops/">workshops</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">I attempt to keep this list up to date. Also check my <a href="https://scholar.google.com/citations?user=WnKjfFEAAAAJ">Google Scholar</a> profile.</h5>
  </header>

  <article class="post-content publications clearfix">
    
<h3 class="year">2025</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="rana2025policy">
  
    
    <a href="https://arxiv.org/abs/2410.12124" class="title" target="_blank">Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            
              Robert Lee,
            
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL),</em>
    
    
      2025.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2410.12124" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2024policy.png"></a>
  
  
  <span class="abstract">
    We introduce oriented affordance frames, a structured representation for state and action spaces that improves spatial and intra-category generalisation and enables policies to be learned efficiently from only 10 demonstrations. More importantly, we show how this abstraction allows for compositional generalisation of independently trained sub-policies to solve long-horizon, multi-object tasks. To seamlessly transition between sub-policies, we introduce the notion of self-progress prediction, which we directly derive from the duration of the training demonstrations. We validate our method across three real-world tasks, each requiring multi-step, multi-object interactions. Despite the small dataset, our policies generalise robustly to unseen object appearances, geometries, and spatial arrangements, achieving high success rates without reliance on exhaustive training data.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2410.12124" target="_blank">arXiv</a>]
  
  
    [<a href="https://affordance-policy.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2410.12124" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="rana2025imle">
  
    
    <a href="https://arxiv.org/abs/2502.12371" class="title" target="_blank">IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Robert Lee,
            
          
        
      
        
          
            
              David Pershouse,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Robotics: Science and Systems (RSS),</em>
    
    
      2025.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2502.12371" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2025imle.png"></a>
  
  
  <span class="abstract">
    We introduce IMLE Policy, a novel behaviour cloning approach based on Implicit Maximum Likelihood Estimation (IMLE). IMLE Policy excels in low-data regimes, effectively learning from minimal demonstrations and requiring 38% less data on average to match the performance of baseline methods in learning complex multi-modal behaviours. Its simple generator-based architecture enables single-step action generation, improving inference speed by 97.3% compared to Diffusion Policy, while outperforming single-step Flow Matching.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2502.12371" target="_blank">arXiv</a>]
  
  
    [<a href="https://imle-policy.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2502.12371" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="marticorena2024rmmi">
  
    
    <a href="https://arxiv.org/abs/2408.16206" class="title" target="_blank">RMMI: Enhanced Obstacle Avoidance for Reactive Mobile Manipulation using an Implicit Neural Map</a>
    
    <span class="author">
      
        
          
            
              Nicolas Marticorena,
            
          
        
      
        
          
            
              Tobias Fischer,
            
          
        
      
        
          
            
              Jesse Haviland,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Intelligent Robots and Systems  (IROS),</em>
    
    
      2025.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2408.16206" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/marticorena2024rmmi.png"></a>
  
  
  <span class="abstract">
    We introduce a novel reactive control framework for mobile manipulators operating in complex, static environments. Our approach leverages a neural Signed Distance Field (SDF) to model intricate environment details and incorporates this representation as inequality constraints within a Quadratic Program (QP) to coordinate robot joint and base motion. A key contribution is the introduction of an active collision avoidance cost term that maximises the total robot distance to obstacles during the motion. We first evaluate our approach in a simulated reaching task, outperforming previous methods that rely on representing both the robot and the scene as a set of primitive geometries. Compared with the baseline, we improved the task success rate by 25% in total, which includes increases of 10% by using the active collision cost. We also demonstrate our approach on a real-world platform, showing its effectiveness in reaching target poses in cluttered and confined spaces using environment models built directly from sensor data
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2408.16206" target="_blank">arXiv</a>]
  
  
    [<a href="https://rmmi.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2408.16206" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="jayanga2024multi">
  
    
    <a href="https://arxiv.org/abs/2412.03911" class="title" target="_blank">Multi-View Pose-Agnostic Change Localization with Zero Labels</a>
    
    <span class="author">
      
        
          
            
              Chamuditha Jayanga Galappaththige,
            
          
        
      
        
          
            
              Jason Lai,
            
          
        
      
        
          
            
              Lloyd Windrim,
            
          
        
      
        
          
            
              Donald Dansereau,
            
          
        
      
        
          
            Niko Suenderhauf,
          
        
      
        

          
            
              Dimity Miller.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2025.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2412.03911" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/chamu2024multi.png"></a>
  
  
  <span class="abstract">
    We propose a novel label-free, pose-agnostic change detection method that integrates information from multiple viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS) representation of the scene. With as few as 5 images of the post-change scene, our approach can learn additional change channels in a 3DGS and produce change masks that outperform single-view techniques. Our change-aware 3D scene representation additionally enables the generation of accurate change masks for unseen viewpoints. Experimental results demonstrate state-of-the-art performance in complex multi-object scenes, achieving a 1.7 and 1.6 improvement in Mean Intersection Over Union and F1 score respectively over other baselines. We also contribute a new real-world dataset to benchmark change detection in diverse challenging scenes in the presence of lighting variations.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2412.03911" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2412.03911" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>arXiv Preprints</strong></p>
<ol class="bibliography"><li>

<div id="abou-chakra2025real">
  
    
    <a href="https://arxiv.org/abs/2504.03597" class="title" target="_blank">Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation</a>
    
    <span class="author">
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Lingfeng Sun,
            
          
        
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Brandon May,
            
          
        
      
        
          
            
              Karl Schmeckpeper,
            
          
        
      
        
          
            Niko Suenderhauf,
          
        
      
        
          
            
              Maria Vittoria Minniti,
            
          
        
      
        

          
            
              Laura Herlant.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2504.03597,</em>
    
    
      2025.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2504.03597" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/abou-chakra2025real.png"></a>
  
  
  <span class="abstract">
    We introduce real-is-sim, a new approach to integrating simulation into behavior cloning pipelines. In contrast to real-only methods, which lack the ability to safely test policies before deployment, and sim-to-real methods, which require complex adaptation to cross the sim-to-real gap, our framework allows policies to seamlessly switch between running on real hardware and running in parallelized virtual environments. At the center of real-is-sim is a dynamic digital twin, powered by the Embodied Gaussian simulator, that synchronizes with the real world at 60Hz. This twin acts as a mediator between the behavior cloning policy and the real robot. Policies are trained using representations derived from simulator states and always act on the simulated robot, never the real one. During deployment, the real robot simply follows the simulated robot’s joint states, and the simulation is continuously corrected with real world measurements. This setup, where the simulator drives all policy execution and maintains real-time synchronization with the physical world, shifts the responsibility of crossing the sim-to-real gap to the digital twin’s synchronization mechanisms, instead of the policy itself.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2504.03597" target="_blank">arXiv</a>]
  
  
    [<a href="https://real-is-sim.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2504.03597" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2024</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography"><li>

<div id="taras2024privacy">
  
    
    <a href="https://doi.org/10.1016/j.jrt.2024.100079" class="title" target="_blank">Inherently privacy-preserving vision for trustworthy autonomous systems: Needs and solutions</a>
    
    <span class="author">
      
        
          
            
              Adam K. Taras,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        

          
            
              Donald G. Dansereau.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Responsible Technology,</em>
    
    
      2024.
    
    </span>
  

  

  
  <a href="https://doi.org/10.1016/j.jrt.2024.100079" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/taras2024privacy.png"></a>
  
  
  <span class="abstract">
    This paper is a call to action to consider privacy in robotic vision. We propose a specific form of inherent privacy preservation in which no images are captured or could be reconstructed by an attacker, even with full remote access. We present a set of principles by which such systems could be designed, employing data-destroying operations and obfuscation in the optical and analogue domains. These cameras never see a full scene. Our localisation case study demonstrates in simulation four implementations that all fulfil this task. The design space of such systems is vast despite the constraints of optical-analogue processing. We hope to inspire future works that expand the range of applications open to sighted robotic systems.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
    [<a href="https://roboticimaging.org/Projects/Privacy/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://doi.org/10.1016/j.jrt.2024.100079" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="abou-chakra2024physically">
  
    
    <a href="https://openreview.net/forum?id=AEq0onGrN2" class="title" target="_blank">Physically Embodied Gaussian Splatting: A Realtime Correctable World Model for Robotics</a>
    
    <span class="author">
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL),</em>
    
    
      2024.
    
    </span>
  

  
  <span class="award">Oral Presentation</span>
  

  
  <a href="https://openreview.net/forum?id=AEq0onGrN2" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/abou-chakra2024physically.png"></a>
  
  
  <span class="abstract">
    We propose a novel dual "Gaussian-Particle" representation that models the physical world while (i) enabling predictive simulation of future states and (ii) allowing online correction from visual observations in a dynamic world. Our representation comprises particles that capture the geometrical aspect of objects in the world and can be used alongside a particle-based physics system to anticipate physically plausible future states. Attached to these particles are 3D Gaussians that render images from any viewpoint through a splatting process thus capturing the visual state. By comparing the predicted and observed images, our approach generates "visual forces" that correct the particle positions while respecting known physical constraints. By integrating predictive physical modeling with continuous visually-derived corrections, our unified representation reasons about the present and future while synchronizing with reality.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2406.10788" target="_blank">arXiv</a>]
  
  
    [<a href="https://embodied-gaussians.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://openreview.net/forum?id=AEq0onGrN2" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="miller2024open">
  
    
    <a href="https://arxiv.org/abs/2403.16528" class="title" target="_blank">Open-Set Recognition in the Age of Vision-Language Models</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Alex Kenna,
            
          
        
      
        

          
            
              Keita Mason.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In European Conference on Computer Vision (ECCV),</em>
    
    
      2024.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2403.16528" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller2024openset.png"></a>
  
  
  <span class="abstract">
    Are vision-language models (VLMs) open-set models because they are trained on internet-scale datasets? We answer this question with a clear no – VLMs introduce closed-set assumptions via their finite query set, making them vulnerable to open-set conditions. We systematically evaluate VLMs for open-set recognition and find they frequently misclassify objects not contained in their query set, leading to alarmingly low precision when tuned for high recall and vice versa. We show that naively increasing the size of the query set to contain more and more classes does not mitigate this problem, but instead causes diminishing task performance and open-set performance. We establish a revised definition of the open-set problem for the age of VLMs, define a new benchmark and evaluation protocol to facilitate standardised evaluation and research in this important area, and evaluate promising baseline approaches based on predictive uncertainty and dedicated negative embeddings on a range of VLM classifiers and object detectors.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2403.16528" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2403.16528" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
    [<a href="https://github.com/dimitymiller/openset_vlms" target="_blank">Code</a>]
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="padalkar2023open">
  
    
    <a href="https://robotics-transformer-x.github.io/" class="title" target="_blank">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</a>
    
    <span class="author">
      
        

          
            
              Open X-Embodiment Collaboration.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Conference on Robotics and Automation (ICRA),</em>
    
    
      2024.
    
    </span>
  

  
  <span class="award">ICRA Best Conference Paper. ICRA Best Paper Award in Robot Manipulation.</span>
  

  
  <a href="https://robotics-transformer-x.github.io/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/openx2024.png"></a>
  
  
  <span class="abstract">
    Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2310.08864" target="_blank">arXiv</a>]
  
  
    [<a href="https://robotics-transformer-x.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://robotics-transformer-x.github.io/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="garg2023robohop">
  
    
    <a href="https://oravus.github.io/RoboHop/" class="title" target="_blank">RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation</a>
    
    <span class="author">
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Mehdi Hosseinzadeh,
            
          
        
      
        
          
            
              Lachlan Mares,
            
          
        
      
        
          
            Niko Suenderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            
              Ian Reid.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Conference on Robotics and Automation (ICRA),</em>
    
    
      2024.
    
    </span>
  

  
  <span class="award">Oral Presentation at ICRA, Oral Presentation at CoRL Workshop</span>
  

  
  <a href="https://oravus.github.io/RoboHop/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/garg2024robohop.png"></a>
  
  
  <span class="abstract">
    We propose a novel topological representation of an environment based on image segments, which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph but with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a continuous sense of a ‘place’, defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
    [<a href="https://oravus.github.io/RoboHop/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://oravus.github.io/RoboHop/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="abouchakra2022particle">
  
    
    <a href="https://arxiv.org/abs/2211.04041" class="title" target="_blank">ParticleNeRF: Particle Based Encoding for Online Neural Radiance Fields in Dynamic Scenes</a>
    
    <span class="author">
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Winter Conference on Applications of Computer Vision (WACV),</em>
    
    
      2024.
    
    </span>
  

  
  <span class="award">Best Paper Honourable Mention &amp; Oral Presentation</span>
  

  
  <a href="https://arxiv.org/abs/2211.04041" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/abouchakra2022particle.png"></a>
  
  
  <span class="abstract">
    While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline methods with an emphasis on visual fidelity, our paper addresses the online use case that prioritises real-time adaptability. We present ParticleNeRF, a new approach that dynamically adapts to changes in the scene geometry by learning an up-to-date representation online, every 200ms. ParticleNeRF achieves this using a novel particle-based parametric encoding. We couple features to particles in space and backpropagate the photometric reconstruction loss into the particles’ position gradients, which are then interpreted as velocity vectors. Governed by a lightweight physics system to handle collisions, this lets the features move freely with the changing scene geometry.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2211.04041" target="_blank">arXiv</a>]
  
  
    [<a href="https://sites.google.com/view/particlenerf" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2211.04041" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography">
<li>

<div id="rana2024policy">
  
    
    <a href="https://arxiv.org/abs/2410.12124" class="title" target="_blank">Affordance-Centric Policy Learning: Sample Efficient and Generalisable Robot Policy Learning using Affordance-Centric Task Frames</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            
              Robert Lee,
            
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Learning Robotic Assembly of Industrial and Everyday Objects, Conference on Robot Learning (CoRL),</em>
    
    
      2024.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2410.12124" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2024policy.png"></a>
  
  
  <span class="abstract">
     In this paper, we propose an affordance-centric policy-learning approach that centres and appropriately orients a task frame on these affordance regions allowing us to achieve both intra-category invariance – where policies can generalise across different instances within the same object category – and spatial invariance — which enables consistent performance regardless of object placement in the environment. We propose a method to leverage existing generalist large vision models to extract and track these affordance frames, and demonstrate that our approach can learn manipulation tasks using behaviour cloning from as little as 10 demonstrations, with equivalent generalisation to an image-based policy trained on 305 demonstrations.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2410.12124" target="_blank">arXiv</a>]
  
  
    [<a href="https://affordance-policy.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2410.12124" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="olivastri2024multi">
  
    
    <a href="https://arxiv.org/abs/2411.02938" class="title" target="_blank">Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments</a>
    
    <span class="author">
      
        
          
            
              Emilio Olivastri,
            
          
        
      
        
          
            
              Jonathan Francis,
            
          
        
      
        
          
            
              Alberto Pretto,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Krishan Rana.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Lifelong Learning for Home Robots, Conference on Robot Learning (CoRL),</em>
    
    
      2024.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2411.02938" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/olivastri2024multi.png"></a>
  
  
  <span class="abstract">
    The advent of generalist Large Language Models (LLMs) and Large Vision Models (VLMs) have streamlined the construction of semantically enriched maps that can enable robots to ground high-level reasoning and planning into their representations. One of the most widely used semantic map formats is the 3D Scene Graph, which captures both metric (low-level) and semantic (high-level) information. However, these maps often assume a static world, while real environments, like homes and offices, are dynamic. Even small changes in these spaces can significantly impact task performance. To integrate robots into dynamic environments, they must detect changes and update the scene graph in real-time. This update process is inherently multimodal, requiring input from various sources, such as human agents, the robot’s own perception system, time, and its actions. This work proposes a framework that leverages these multimodal inputs to maintain the consistency of scene graphs during real-time operation, presenting promising initial results and outlining a roadmap for future research.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2411.02938" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2411.02938" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="raine2024coral">
  
    
    <a href="https://arxiv.org/abs/2404.09406" class="title" target="_blank">Human-in-the-Loop Segmentation of Multi-species Coral Imagery</a>
    
    <span class="author">
      
        
          
            
              Scarlett Raine,
            
          
        
      
        
          
            
              Ross Marchant,
            
          
        
      
        
          
            
              Brano Kusy,
            
          
        
      
        
          
            
              Frederic Maire,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Tobias Fischer.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In CVPR workshop on Learning with Limited Labelled Data for Image and Video Understanding,</em>
    
    
      2024.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2404.09406" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/raine2024coral.png"></a>
  
  
  <span class="abstract">
    We first demonstrate that recent advances in foundation models enable generation of multi-species coral augmented ground truth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN), without the need for any pre-training or custom-designed algorithms. For extremely sparsely labeled images, we propose a labeling regime based on human-in-the-loop principles, resulting in significant improvement in annotation efficiency.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2404.09406" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2404.09406" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>arXiv Preprints</strong></p>
<ol class="bibliography">
<li>

<div id="raine2024reducing">
  
    
    <a href="https://arxiv.org/abs/2411.11287" class="title" target="_blank">Reducing Label Dependency for Underwater Scene Understanding: A Survey of Datasets, Techniques and Applications</a>
    
    <span class="author">
      
        
          
            
              Scarlett Raine,
            
          
        
      
        
          
            
              Frederic Maire,
            
          
        
      
        
          
            Niko Suenderhauf,
          
        
      
        

          
            
              Tobias Fischer.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2411.11287,</em>
    
    
      2024.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2411.11287" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/raine2024reducing.png"></a>
  
  
  <span class="abstract">
    The complexity of underwater images, coupled with the specialist expertise needed to accurately identify species at the pixel level, makes the labeling process costly, time-consuming, and heavily dependent on domain experts. In recent years, some works have performed automated analysis of underwater imagery, and a smaller number of studies have focused on weakly supervised approaches which aim to reduce the expert-provided labelled data required. This survey focuses on approaches which reduce dependency on human expert input, while reviewing the prior and related approaches to position these works in the wider field of underwater perception. Further, we offer an overview of coastal ecosystems and the challenges of underwater imagery.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2411.11287" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2411.11287" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="van2024open">
  
    
    <a href="https://arxiv.org/abs/2406.05951" class="title" target="_blank">Open-Vocabulary Part-Based Grasping</a>
    
    <span class="author">
      
        
          
            
              Tjeard Oort,
            
          
        
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            
              Will N Browne,
            
          
        
      
        
          
            
              Nicolas Marticorena,
            
          
        
      
        
          
            
              Jesse Haviland,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2406.05951,</em>
    
    
      2024.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2406.05951" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/van2024open.png"></a>
  
  
  <span class="abstract">
    Many robotic applications require to grasp objects
not arbitrarily but at a very specific object part. This is especially
important for manipulation tasks beyond simple pick-and-place
scenarios or in robot-human interactions, such as object handovers. We propose AnyPart, a practical system that combines
open-vocabulary object detection, open-vocabulary part segmentation and 6DOF grasp pose prediction to infer a grasp pose on
a specific part of an object in 800 milliseconds. We contribute
two new datasets for the task of open-vocabulary part-based
grasping, a hand-segmented dataset containing 1014 object-part
segmentations, and a dataset of real-world scenarios gathered
during our robot trials for individual objects and table-clearing
tasks. We evaluate AnyPart on a mobile manipulator robot using
a set of 28 common household objects over 360 grasping trials.
AnyPart is capable of producing successful grasps 69.52 %, when
ignoring robot-based grasp failures, AnyPart predicts a grasp
location on the correct part 88.57% of the time.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2406.05951" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2406.05951" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2023</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography"><li>

<div id="rana2021BCF">
  
    
    <a href="https://arxiv.org/abs/2107.09822" class="title" target="_blank">Bayesian Controller Fusion: Leveraging Control Priors in Deep Reinforcement Learning for Robotics</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Vibhavari Dasagi,
            
          
        
      
        
          
            
              Jesse Haviland,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>The International Journal of Robotics Research (IJRR),</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2107.09822" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana21bcf.png"></a>
  
  
  <span class="abstract">
    We present Bayesian Controller Fusion (BCF): a hybrid control strategy that combines the strengths of traditional hand-crafted controllers and model-free deep reinforcement learning (RL). BCF thrives in the robotics domain, where reliable but suboptimal control priors exist for many tasks, but RL from scratch remains unsafe and data-inefficient. By fusing uncertainty-aware distributional outputs from each system, BCF arbitrates control between them, exploiting their respective strengths. 
    As exploration is naturally guided by the prior in the early stages of training, BCF accelerates learning, while substantially improving beyond the performance of the control prior, as the policy gains more experience. 
    More importantly, given the risk-aversity of the control prior, BCF ensures safe exploration <i>and</i> deployment, where the control prior naturally dominates the action distribution in states unknown to the policy. 
    We additionally show BCF’s applicability to the zero-shot sim-to-real setting and its ability to deal with out-of-distribution states in the real-world. 
    BCF is a promising approach for combining the complementary strengths of deep RL and traditional robotic control, surpassing what either can achieve independently.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2107.09822" target="_blank">arXiv</a>]
  
  
    [<a href="https://krishanrana.github.io/bcf" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2107.09822" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="rana2023sayplan">
  
    
    <a href="https://openreview.net/pdf?id=wMpOMO0Ss7a" class="title" target="_blank">Sayplan: Grounding Large Language Models Using 3d Scene Graphs for Scalable Robot Task Planning</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Jesse Haviland,
            
          
        
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL),</em>
    
    
      2023.
    
    </span>
  

  
  <span class="award">Oral Presentation</span>
  

  
  <a href="https://openreview.net/pdf?id=wMpOMO0Ss7a" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2023sayplan.png"></a>
  
  
  <span class="abstract">
    We introduce SayPlan, a scalable approach to LLM-based,
large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical
nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant
subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the
planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback
from a scene graph simulator, correcting infeasible actions and avoiding planning
failures. We evaluate our approach on two large-scale environments spanning up
to 3 floors and 36 rooms with 140 assets and objects and show that our approach is
capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2307.06135" target="_blank">arXiv</a>]
  
  
    [<a href="https://sayplan.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://openreview.net/pdf?id=wMpOMO0Ss7a" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="wilson2023safe">
  
    
    <a href="https://arxiv.org/abs/2208.13930" class="title" target="_blank">SAFE: Sensitivity-Aware Features for Out-of-Distribution Object Detection</a>
    
    <span class="author">
      
        
          
            
              Samuel Wilson,
            
          
        
      
        
          
            
              Tobias Fischer,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Dimity Miller,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision,</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2208.13930" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/wilson2023safe.png"></a>
  
  
  <span class="abstract">
     We address the problem of out-of-distribution (OOD)
detection for the task of object detection. We show that
residual convolutional layers with batch normalisation produce Sensitivity-Aware FEatures (SAFE) that are consistently powerful for distinguishing in-distribution from outof-distribution detections. We extract SAFE vectors for every detected object, and train a multilayer perceptron on
the surrogate task of distinguishing adversarially perturbed
from clean in-distribution examples. This circumvents the
need for realistic OOD training data, computationally expensive generative models, or retraining of the base object detector.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2208.13930" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2208.13930" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="suenderhauf2022nerf">
  
    
    <a href="https://arxiv.org/abs/2209.08718" class="title" target="_blank">Density-aware NeRF Ensembles: Quantifying Predictive Uncertainty in Neural Radiance Fields</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        

          
            
              Dimity Miller.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Conference on Robotics and Automation (ICRA),</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2209.08718" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf2022nerf.png"></a>
  
  
  <span class="abstract">
    
  We show that ensembling effectively quantifies
model uncertainty in Neural Radiance Fields (NeRFs) if a
density-aware epistemic uncertainty term is considered. The
naive ensembles investigated in prior work simply average
rendered RGB images to quantify the model uncertainty caused
by conflicting explanations of the observed scene. In contrast,
we additionally consider the termination probabilities along
individual rays to identify epistemic model uncertainty due to
a lack of knowledge about the parts of a scene unobserved
during training. We achieve new state-of-the-art performance
across established uncertainty quantification benchmarks for
NeRFs, outperforming methods that require complex changes
to the NeRF architecture and training regime. We furthermore
demonstrate that NeRF uncertainty can be utilised for next-best
view selection and model refinement.
  
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2209.08718" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2209.08718" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="wilson2021hyperdimensional">
  
    
    <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" class="title" target="_blank">Hyperdimensional Feature Fusion for Out-Of-Distribution Detection</a>
    
    <span class="author">
      
        
          
            
              Samuel Wilson,
            
          
        
      
        
          
            
              Tobias Fischer,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Winter Conference on Applications of Computer Vision (WACV),</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/wilson2021hyperdimensional.png"></a>
  
  
  <span class="abstract">
    We introduce powerful ideas from Hyperdimensional
Computing into the challenging field of Out-of-Distribution
(OOD) detection. In contrast to most existing works that perform OOD detection based on only a single layer of a neural
network, we use similarity-preserving semi-orthogonal projection matrices to project the feature maps from multiple
layers into a common vector space. By repeatedly applying the bundling operation ⊕, we create expressive classspecific descriptor vectors for all in-distribution classes.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2112.05341" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography">
<li>

<div id="abouchakra2023splatting">
  
    
    <a href="https://eprints.qut.edu.au/247354/1/Physically_Embodied_Gaussian_Splatting_Embedding_Physical_Priors_into_a_Visual_3D_World_Model_For_Robotics.pdf" class="title" target="_blank">Physically Embodied Gaussian Splatting: Embedding Physical Priors Into a Visual 3d World Model for Robotics</a>
    
    <span class="author">
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop for Neural Representation Learning for Robot Manipulation, Conference on Robot Learning (CoRL),</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://eprints.qut.edu.au/247354/1/Physically_Embodied_Gaussian_Splatting_Embedding_Physical_Priors_into_a_Visual_3D_World_Model_For_Robotics.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/abouchakra2023splatting.png"></a>
  
  
  <span class="abstract">
    Our dual Gaussian-Particle representation captures visual (Gaussians) and physical (particles) aspects of the world and enables forward prediction of robot interactions with the world.
     A photometric loss between rendered Gaussians and observed images is computed (Gaussian Splatting) and converted into visual forces. These and other physical phenomena such as gravity, collisions, and mechanical forces are resolved by the always-active physics system and applied to the particles, which in turn influence the position of their associated Gaussians.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
    [<a href="https://embodied-gaussians.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://eprints.qut.edu.au/247354/1/Physically_Embodied_Gaussian_Splatting_Embedding_Physical_Priors_into_a_Visual_3D_World_Model_For_Robotics.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="taras2023privacy">
  
    
    <a href="https://arxiv.org/pdf/2303.16408.pdf" class="title" target="_blank">The Need for Inherently Privacy-Preserving Vision in Trustworthy Autonomous Systems</a>
    
    <span class="author">
      
        
          
            
              Adam K Taras,
            
          
        
      
        
          
            Niko Suenderhauf,
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        

          
            
              Donald G Dansereau.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In ICRA Workshop on Multidisciplinary Approaches to Co-Creating Trustworthy Autonomous Systems,</em>
    
    
      2023.
    
    </span>
  

  
  <span class="award">Best Poster Award</span>
  

  
  <a href="https://arxiv.org/pdf/2303.16408.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/taras2023privacy.png"></a>
  
  
  <span class="abstract">
    This paper is a call to action to consider privacy
in the context of robotic vision. We propose a specific form
privacy preservation in which no images are captured or could
be reconstructed by an attacker even with full remote access.
We present a set of principles by which such systems can be
designed, and through a case study in localisation demonstrate
in simulation a specific implementation that delivers an important robotic capability in an inherently privacy-preserving
manner. This is a first step, and we hope to inspire future works
that expand the range of applications open to sighted robotic
systems.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2303.16408" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/pdf/2303.16408.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="rana2023contrastive">
  
    
    <a href="https://openreview.net/forum?id=sxKR6zhBDH" class="title" target="_blank">Contrastive Language, Action, and State Pre-training for Robot Learning</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Andrew Melnik,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In ICRA Workshop on Pretraining for Robotics (PT4R),</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://openreview.net/forum?id=sxKR6zhBDH" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2023contrastive.png"></a>
  
  
  <span class="abstract">
    We introduce a method for unifying language, action, and state information in a shared embedding space to facilitate a range of downstream tasks in robot learning.  Our method, Contrastive Language, Action, and State Pre-training (CLASP), extends the CLIP formulation by incorporating distributional learning, capturing the inherent complexities and one-to-many relationships in behaviour-text alignment. By employing distributional outputs for both text and behaviour encoders, our model effectively associates diverse textual commands with a single behaviour and vice-versa. We demonstrate the utility of our method for the following downstream tasks: zero-shot text-behaviour retrieval, captioning unseen robot behaviours, and learning a behaviour prior for language-conditioned reinforcement learning.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2304.10782" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://openreview.net/forum?id=sxKR6zhBDH" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>arXiv Preprints</strong></p>
<ol class="bibliography"><li>

<div id="ceola2023lhmanip">
  
    
    <a href="https://arxiv.org/abs/2312.12036" class="title" target="_blank">LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks in Cluttered Tabletop Environments</a>
    
    <span class="author">
      
        
          
            
              Federico Ceola,
            
          
        
      
        
          
            
              Lorenzo Natale,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Krishan Rana.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2312.12036,</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2312.12036" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/ceola2023manip.png"></a>
  
  
  <span class="abstract">
    We present the Long-Horizon Manipulation (LHManip) dataset comprising 200
episodes, demonstrating 20 different manipulation tasks via real robot teleoperation. The tasks entail multiple sub-tasks,
including grasping, pushing, stacking and throwing objects in highly cluttered environments. Each task is paired with a
natural language instruction and multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the dataset
comprises 176,278 observation-action pairs which form part of the Open X-Embodiment dataset.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2312.12036" target="_blank">arXiv</a>]
  
  
    [<a href="https://github.com/fedeceola/LHManip" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2312.12036" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2022</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography">
<li>

<div id="haviland2022holistic">
  
    
    <a href="https://ieeexplore.ieee.org/document/9695298/" class="title" target="_blank">A Holistic Approach to Reactive Mobile Manipulation</a>
    
    <span class="author">
      
        
          
            
              Jesse Haviland,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Corke.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters,</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/document/9695298/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/haviland2022holistic.png"></a>
  
  
  <span class="abstract">
    We present the design and implementation of a taskable reactive mobile manipulation system. In contrary to related work, we treat the arm and base degrees of freedom as a holistic structure which greatly improves the speed and fluidity of the resulting motion. At the core of this approach is a robust and reactive motion controller which can achieve a desired end-effector pose, while avoiding joint position and velocity limits, and ensuring the mobile manipulator is manoeuvrable throughout the trajectory. This can support sensor-based behaviours such as closed-loop visual grasping. As no planning is involved in our approach, the robot is never stationary thinking about what to do next. We show the versatility of our holistic motion controller by implementing a pick and place system using behaviour trees and demonstrate this task on a 9-degree-of-freedom mobile manipulator. 
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2109.04749" target="_blank">arXiv</a>]
  
  
    [<a href="http://jhavl.github.io/holistic" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/document/9695298/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="hall2022benchbot">
  
    
    <a href="https://journals.sagepub.com/doi/abs/10.1177/02783649211069404" class="title" target="_blank">BenchBot environments for active robotics (BEAR): Simulated data for active scene understanding research</a>
    
    <span class="author">
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Suman Raj Bista,
            
          
        
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              Rohan Smith,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>The International Journal of Robotics Research,</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="https://journals.sagepub.com/doi/abs/10.1177/02783649211069404" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/hall2022benchbot.png"></a>
  
  
  <span class="abstract">
    We present a platform to foster research in active scene understanding, consisting of high-fidelity simulated environments and a simple yet powerful API that controls a mobile robot in simulation and reality. In contrast to static, pre-recorded datasets that focus on the perception aspect of scene understanding, agency is a top priority in our work. We provide three levels of robot agency, allowing users to control a robot at varying levels of difficulty and realism. While the most basic level provides pre-defined trajectories and ground-truth localisation, the more realistic levels allow us to evaluate integrated behaviours comprising perception, navigation, exploration and SLAM. 
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
    [<a href="https://qcr.github.io/dataset/benchbot-bear-data/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://journals.sagepub.com/doi/abs/10.1177/02783649211069404" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="rahman2021fsnet">
  
    
    <a href="https://ieeexplore.ieee.org/document/9682519/" class="title" target="_blank">FSNet: A Failure Detection Framework for Semantic Segmentation</a>
    
    <span class="author">
      
        
          
            
              Quazi Marufur Rahman,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters,</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/document/9682519/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rahman2021fsnet.png"></a>
  
  
  <span class="abstract">
    Semantic segmentation is an important task that helps autonomous vehicles understand their surroundings and navigate safely. However, during deployment, even the most mature segmentation models are vulnerable to various external factors that can degrade the segmentation performance with potentially catastrophic consequences for the vehicle and its surroundings. To address this issue, we propose a failure detection framework to identify pixel-level misclassification. We do so by exploiting internal features of the segmentation model and training it simultaneously with a failure detection network. During deployment, the failure detector flags areas in the image where the segmentation model has failed to segment correctly.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2108.08748" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/document/9682519/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="garg2019semantic">
  
    
    <a href="https://doi.org/10.1177%2F0278364919839761" class="title" target="_blank">Semantic–geometric visual place recognition: a new perspective for reconciling opposing views</a>
    
    <span class="author">
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>The International Journal of Robotics Research (IJRR),</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="https://doi.org/10.1177%2F0278364919839761" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/garg18rss.png"></a>
  
  
  <span class="abstract">
    We propose a hybrid image descriptor that semantically aggregates salient visual information, complemented by appearance-based description, and augment a conventional coarse-to-fine recognition pipeline with keypoint correspondences extracted from within the convolutional feature maps of a pre-trained network. Finally, we introduce descriptor normalization and local score enhancement strategies for improving the robustness of the system. Using both existing benchmark datasets and extensive new datasets that for the first time combine the three challenges of opposing viewpoints, lateral viewpoint shifts, and extreme appearance change, we show that our system can achieve practical place recognition performance where existing state-of-the-art methods fail.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://doi.org/10.1177%2F0278364919839761" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="rana2022reskill">
  
    
    <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" class="title" target="_blank">Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Ming Xu,
            
          
        
      
        
          
            
              Brendan Tidd,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL),</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2022reskill.png"></a>
  
  
  <span class="abstract">
    Skill-based reinforcement learning (RL) has emerged as a promising strategy to
leverage prior knowledge for accelerated robot learning. We firstly
propose accelerating exploration in the skill space using state-conditioned generative models to directly bias the high-level agent towards only sampling skills
relevant to a given state based on prior experience. Next, we propose a low-level
residual policy for fine-grained skill adaptation enabling downstream RL agents
to adapt to unseen task variations. Finally, we validate our approach across four
challenging manipulation tasks that differ from those used to build the skill space,
demonstrating our ability to learn across task variations while significantly accelerating exploration, outperforming prior works.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2211.02231" target="_blank">arXiv</a>]
  
  
    [<a href="https://krishanrana.github.io/reskill" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="ruetz2022forest">
  
    
    <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9981401" class="title" target="_blank">Forest Traversability Mapping (FTM): Traversability estimation using 3D voxel-based Normal Distributed Transform to enable forest navigation</a>
    
    <span class="author">
      
        
          
            
              Fabio Ruetz,
            
          
        
      
        
          
            
              Paulo Borges,
            
          
        
      
        
          
            Niko Suenderhauf,
          
        
      
        
          
            
              Emili Hernández,
            
          
        
      
        

          
            
              Thierry Peynot.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9981401" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/ruetz2022forest.png"></a>
  
  
  <span class="abstract">
    Autonomous navigation in dense vegetation remains an open challenge and is an area of major interest for the research community. In this paper we propose a novel traversability estimation method, the Forest Traversability Map, that gives autonomous ground vehicles the ability to navigate in harsh forests or densely vegetated environments. The method estimates travers ability in unstructured environments dominated by vegetation, void of any dominant human structures, gravel or dirt roads, with higher accuracy than the state of the art.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9981401" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography"><li>

<div id="abouchakra2022implicit">
  
    
    <a href="https://arxiv.org/abs/2204.10516" class="title" target="_blank">Implicit Object Mapping With Noisy Data</a>
    
    <span class="author">
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In RSS Workshop on Implicit Representations for Robotic Manipulation,</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2204.10516" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/abouchakra2022implicit.png"></a>
  
  
  <span class="abstract">
    This paper uses the outputs of an object-based SLAM system to bound objects in the scene with coarse primitives and - in concert with instance masks - identify obstructions in the training images. Objects are therefore automatically bounded, and non-relevant geometry is excluded from the NeRF representation. The method’s performance is benchmarked under ideal conditions and tested against errors in the poses and instance masks. Our results show that object-based NeRFs are robust to pose variations but sensitive to the quality of the instance masks.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2204.10516" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2204.10516" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>arXiv Preprints</strong></p>
<ol class="bibliography"><li>

<div id="deitke2022retro">
  
    
    <a href="http://arxiv.org/abs/2210.06849" class="title" target="_blank">Retrospectives on the Embodied AI Workshop</a>
    
    <span class="author">
      
        
          
            
              Matt Deitke,
            
          
        
      
        
          
            
              Dhruv Batra,
            
          
        
      
        
          
            
              Yonatan Bisk,
            
          
        
      
        
          
            
              Tommaso Campari,
            
          
        
      
        
          
            
              Angel X Chang,
            
          
        
      
        
          
            
              Devendra Singh Chaplot,
            
          
        
      
        
          
            
              Changan Chen,
            
          
        
      
        
          
            
              Claudia Pérez D’Arpino,
            
          
        
      
        
          
            
              Kiana Ehsani,
            
          
        
      
        
          
            
              Ali Farhadi,
            
          
        
      
        

          
            
               others.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2210.06849,</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="http://arxiv.org/abs/2210.06849" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/deitke2022retro.png"></a>
  
  
  <span class="abstract">
    We present a retrospective on the state of Embodied AI
research. Our analysis focuses on 13 challenges presented
at the Embodied AI Workshop at CVPR. These challenges
are grouped into three themes: (1) visual navigation, (2) rearrangement, and (3) embodied vision-and-language. We
discuss the dominant datasets within each theme, evaluation metrics for the challenges, and the performance of stateof-the-art models. We highlight commonalities between top
approaches to the challenges and identify potential future
directions for Embodied AI research.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2210.06849" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="http://arxiv.org/abs/2210.06849" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2021</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography">
<li>

<div id="miller2021gmmdet">
  
    
    <a href="http://arxiv.org/abs/2104.01328" class="title" target="_blank">Uncertainty for Identifying Open-Set Errors in Visual Object Detection</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters (RA-L),</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="http://arxiv.org/abs/2104.01328" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller21GMMDet.png"></a>
  
  
  <span class="abstract">
    We propose GMM-Det, a real-time method for extracting
epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a
structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified
by their low log-probability under all Gaussian Mixture Models.
We test two common detector architectures, Faster R-CNN and
RetinaNet, across three varied datasets spanning robotics and
computer vision. Our results show that GMM-Det consistently
outperforms existing uncertainty techniques for identifying and
rejecting open-set detections, especially at the low-error-rate
operating point required for safety-critical applications. GMMDet maintains object detection performance, and introduces
only minimal computational overhead.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2104.01328" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="http://arxiv.org/abs/2104.01328" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="garg2021semantics">
  
    
    <a href="https://doi.org/10.1561/2300000059" class="title" target="_blank">Semantics for robotic mapping, perception and interaction: A survey</a>
    
    <span class="author">
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Douglas Morrison,
            
          
        
      
        
          
            
              Akansel Cosgun,
            
          
        
      
        
          
            
              Gustavo Carneiro,
            
          
        
      
        
          
            
              Qi Wu,
            
          
        
      
        
          
            
              Tat-Jun Chin,
            
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        
          
            
              Stephen Gould,
            
          
        
      
        

          
            
               others.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Foundations and Trends in Robotics,</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="https://doi.org/10.1561/2300000059" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/garg21semantics.png"></a>
  
  
  <span class="abstract">
    This survey provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity, in which semantics are extracted, used, or both. Within these broad categories we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics, including mapping, navigation and interaction with the world. The survey also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where semantics is or is likely to play a key role. In creating this survey, we hope to provide researchers across academia and industry with a comprehensive reference that helps facilitate future research in this exciting field.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2101.00443" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://doi.org/10.1561/2300000059" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="ming2021topometric">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/9484728" class="title" target="_blank">Probabilistic Appearance-Invariant Topometric Localization with New Place Awareness</a>
    
    <span class="author">
      
        
          
            
              Ming Xu,
            
          
        
      
        
          
            
              Tobias Fischer,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters (RA-L),</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/9484728" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/ming21topometric.png"></a>
  
  
  <span class="abstract">
    We present a new probabilistic topometric localization system which incorporates full 3-dof odometry into the motion model and furthermore, adds an “off-map” state within the state-estimation framework, allowing query traverses which feature significant route detours from the reference map to be successfully localized. We perform extensive evaluation on multiple query traverses from the Oxford RobotCar dataset exhibiting both significant appearance change and deviations from routes previously traversed.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2107.07707" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/9484728" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="ming2021probabilistic">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/9268070/" class="title" target="_blank">Probabilistic Visual Place Recognition for Hierarchical Localization</a>
    
    <span class="author">
      
        
          
            
              Ming Xu,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters (RA-L),</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/9268070/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/ming2021probabilistic.png"></a>
  
  
  <span class="abstract">
    We propose two methods which adapt image retrieval techniques used for visual place recognition to the Bayesian state estimation formulation for localization. We demonstrate significant improvements to the
  localization accuracy of the coarse localization stage using our methods, whilst retaining state-of-the-art performance under severe appearance change. Using extensive experimentation on the Oxford RobotCar dataset, results show that our approach
  outperforms comparable state-of-the-art methods in terms of precision-recall performance for localizing image sequences. In addition, our proposed methods provides the flexibility
  to contextually scale localization latency in order to achieve these improvements.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2105.03091" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/9268070/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="zhang2020varifocal">
  
    
    <a href="https://arxiv.org/abs/2008.13367" class="title" target="_blank">VarifocalNet: An IoU-aware Dense Object Detector</a>
    
    <span class="author">
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              Ying Wang,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2021.
    
    </span>
  

  
  <span class="award">Oral Presentation</span>
  

  
  <a href="https://arxiv.org/abs/2008.13367" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/zhang2020varifocal.png"></a>
  
  
  <span class="abstract">
    Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. Prior work uses the classification score or a combination of classification and predicted localization scores to rank candidates. However, neither option results in a reliable ranking, thus degrading detection performance. In this paper, we propose to learn an Iou-aware Classification Score (IACS) as a joint representation of object presence confidence and localization accuracy. We show that dense object detectors can achieve a more accurate ranking of candidate detections based on the IACS. We design a new loss function, named Varifocal Loss, to train a dense object detector to predict the IACS, and propose a new star-shaped bounding box feature representation for IACS prediction and bounding box refinement. Combining these two new components and a bounding box refinement branch, we build an IoU-aware dense object detector based on the FCOS+ATSS architecture, that we call VarifocalNet or VFNet for short.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2008.13367" target="_blank">arXiv</a>]
  
  
    [<a href="https://github.com/hyz-xmaster/VarifocalNet" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2008.13367" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="rahman2020monitoring">
  
    
    <a href="https://arxiv.org/abs/2011.07750" class="title" target="_blank">Online Monitoring of Object Detection Performance Post-Deployment</a>
    
    <span class="author">
      
        
          
            
              Quazi Marufur Rahman,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2011.07750" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rahman2020monitoring.png"></a>
  
  
  <span class="abstract">
    Post-deployment, an object detector is expected to operate at a similar level of performance that was reported on its testing dataset. However, when deployed onboard mobile robots that operate under varying and complex environmental conditions, the detector’s performance can fluctuate and occasionally degrade severely without warning. Undetected, this can lead the robot to take unsafe and risky actions based on low-quality and unreliable object detections. We address this problem and introduce a cascaded neural network that monitors the performance of the object detector by predicting the quality of its mean average precision (mAP) on a sliding window of the input frames.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2011.07750" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2011.07750" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="bista2021mapping">
  
    
    <span class="title">Evaluating the Impact of Semantic Segmentation and Pose Estimationon Dense Semantic SLAM</span>
    
    <span class="author">
      
        
          
            
              Suman Raj Bista,
            
          
        
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/bista21mapping.png"></a>
  
  
  <span class="abstract">
    Recent Semantic SLAM methods combine classical geometry-based estimation with deep learning-based object detection or semantic segmentation.
In this paper we evaluate the quality of semantic maps generated by state-of-the-art class- and instance-aware dense semantic SLAM algorithms whose codes are publicly available and explore the impacts both semantic segmentation and pose estimation have on the quality of semantic maps.
We obtain these results by providing algorithms with ground-truth pose and/or semantic segmentation data available from simulated environments. We establish that semantic segmentation is the largest source of error through our experiments, dropping mAP and OMQ performance by up to 74.3% and 71.3% respectively.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="miller2020cac">
  
    
    <a href="https://arxiv.org/abs/2004.02434" class="title" target="_blank">Class Anchor Clustering: A Loss for Distance-based Open Set Recognition</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Winter Conference on Applications of Computer Vision (WACV),</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2004.02434" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller2020cac.png"></a>
  
  
  <span class="abstract">
    Existing open set classifiers distinguish between known and unknown inputs by measuring distance in a network’s logit space, assuming that known inputs cluster closer to the training data than unknown inputs. However, this approach is typically applied post-hoc to networks trained with cross-entropy loss, which neither guarantees nor encourages the hoped-for clustering behaviour. To overcome this limitation, we introduce Class Anchor Clustering (CAC) loss. CAC is an entirely distance-based loss that explicitly encourages training data to form tight clusters techniques on the challenging TinyImageNet dataset, achieving a 2.4% performance increase in AUROC. 
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2004.02434" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2004.02434" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography"><li>

<div id="rana2021zero">
  
    
    <a href="https://arxiv.org/abs/2112.05299" class="title" target="_blank">Zero-Shot Uncertainty-Aware Deployment of Simulation Trained Policies on Real-World Robots</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Vibhavari Dasagi,
            
          
        
      
        
          
            
              Jesse Haviland,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In NeuIPS Workshop on Deployable Decision Makig in Embodied Systems,</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2112.05299" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2021zero.png"></a>
  
  
  <span class="abstract">
    While deep reinforcement learning (RL) agents have demonstrated incredible potential in attaining dexterous behaviours for robotics, they tend to make errors when deployed in the real world due to mismatches between the training and execution environments. In contrast, the classical robotics community have developed a range of controllers that can safely operate across most states in the real world given their explicit derivation. These controllers however lack the dexterity required for complex tasks given limitations in analytical modelling and approximations. In this paper, we propose Bayesian Controller Fusion (BCF), a novel uncertainty-aware deployment strategy that combines the strengths of deep RL policies and traditional handcrafted controllers.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2112.05299" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2112.05299" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>arXiv Preprints</strong></p>
<ol class="bibliography"></ol>

<h3 class="year">2020</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography"></ol>

<p><strong>Special Issues</strong></p>
<ol class="bibliography"><li>

<div id="Angelova20">
  
    
    <a href="https://link.springer.com/journal/11263/topicalCollection/AC_76e6f8b8fd2b0be6fd469722978b05de" class="title" target="_blank">Special Issue on Deep Learning for Robotic Vision</a>
    
    <span class="author">
      
        
          
            
              Anelia Angelova,
            
          
        
      
        
          
            
              Gustavo Carneiro,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Jürgen Leitner.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>The International Journal for Computer Vision (IJCV),</em>
    
    
      2020.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="https://link.springer.com/journal/11263/topicalCollection/AC_76e6f8b8fd2b0be6fd469722978b05de" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="rana2020multiplicative">
  
    
    <a href="https://arxiv.org/abs/2003.05117" class="title" target="_blank">Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Vibhavari Dasagi,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2003.05117" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2020multiplicative.png"></a>
  
  
  <span class="abstract">
    Learning long-horizon tasks on real robot hardware can be intractable, and transferring a learned policy from simulation to reality is still extremely challenging. We present a novel approach to model-free reinforcement learning that can leverage existing sub-optimal solutions as an algorithmic prior during training and deployment. During training, our gated fusion approach enables the prior to guide the initial stages of exploration, increasing sample-efficiency and enabling learning from sparse long-horizon reward signals. Importantly, the policy can learn to improve beyond the performance of the sub-optimal prior since the prior’s influence is annealed gradually. During deployment, the policy’s uncertainty provides a reliable strategy for transferring a simulation-trained policy to the real world by falling back to the prior controller in uncertain states. We show the efficacy of our Multiplicative Controller Fusion approach on the task of robot navigation and demonstrate safe transfer from simulation to the real world without any fine tuning. 
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2003.05117" target="_blank">arXiv</a>]
  
  
    [<a href="https://sites.google.com/view/mcf-nav/home" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2003.05117" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="rana19navigation">
  
    
    <a href="https://arxiv.org/abs/1909.10972" class="title" target="_blank">Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1909.10972" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana19navigation.png"></a>
  
  
  <span class="abstract">
    In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRNN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1909.10972" target="_blank">arXiv</a>]
  
  
    [<a href="https://github.com/krishanrana/2D_SRRN" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/1909.10972" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="hall2018probability">
  
    
    <a href="https://arxiv.org/abs/1811.10800" class="title" target="_blank">Probabilistic Object Detection: Definition and Evaluation</a>
    
    <span class="author">
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              John Skinner,
            
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        
          
            
              Gustavo Carneiro,
            
          
        
      
        
          
            
              Anelia Angelova,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Winter Conference on Applications of Computer Vision (WACV),</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1811.10800" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/hall2018probability.png"></a>
  
  
  <span class="abstract">
    We introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections. Given the lack of methods capable of assessing such probabilistic object detections, we present the new Probability-based Detection Quality measure (PDQ). Unlike AP-based measures, PDQ has no arbitrary thresholds and rewards spatial and label quality, and foreground/background separation quality while explicitly penalising false positive and false negative detections.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1811.10800" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1811.10800" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>arXiv Preprints</strong></p>
<ol class="bibliography">
<li>

<div id="zhang2020swa">
  
    
    <a href="https://arxiv.org/abs/2012.12645" class="title" target="_blank">Swa Object Detection</a>
    
    <span class="author">
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              Ying Wang,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2012.12645,</em>
    
    
      2020.
    
    </span>
  

  

  
  
  <span class="abstract">
    Do you want to improve 1.0 AP for your object detector without any inference cost and any change to your detector? It is surprisingly simple: train your detector for an extra 12 epochs using cyclical learning rates and then average these 12 checkpoints as your final detection model. This potent recipe is inspired by Stochastic Weights Averaging (SWA), which is proposed in arXiv:1803.05407 for improving generalization in deep neural networks. We found it also very effective in object detection. In this technique report, we systematically investigate the effects of applying SWA to object detection as well as instance segmentation. Through extensive experiments, we discover the aforementioned workable policy of performing SWA in object detection, and we consistently achieve ∼1.0 AP improvement over various popular detectors on the challenging COCO benchmark, including Mask RCNN, Faster RCNN, RetinaNet, FCOS, YOLOv3 and VFNet. We hope this work will make more researchers in object detection know this technique and help them train better object detectors.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2012.12645" target="_blank">arXiv</a>]
  
  
    [<a href="https://github.com/hyz-xmaster/swa_object_detection" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2012.12645" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="hall2020challenge">
  
    
    <a href="https://arxiv.org/abs/2009.05246" class="title" target="_blank">The Robotic Vision Scene Understanding Challenge</a>
    
    <span class="author">
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Suman Raj Bista,
            
          
        
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              Rohan Smith,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2009.05246,</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2009.05246" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/hall2020challenge.png"></a>
  
  
  <span class="abstract">
    Being able to explore an environment and understand the location and type of all objects therein is important for indoor robotic platforms that must interact closely with humans. However, it is difficult to evaluate progress in this area due to a lack of standardized testing which is limited due to the need for active robot agency and perfect object ground-truth. To help provide a standard for testing scene understanding systems, we present a new robot vision scene understanding challenge using simulation to enable repeatable experiments with active robot agency. We provide two challenging task types, three difficulty levels, five simulated environments and a new evaluation measure for evaluating 3D cuboid object maps.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2009.05246" target="_blank">arXiv</a>]
  
  
    [<a href="https://nikosuenderhauf.github.io/roboticvisionchallenges/scene-understanding" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2009.05246" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="talbot2020benchbot">
  
    
    <a href="https://arxiv.org/abs/2008.00635" class="title" target="_blank">Benchbot: Evaluating robotics research in photorealistic 3d simulation and on real robots</a>
    
    <span class="author">
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              Suman Raj Bista,
            
          
        
      
        
          
            
              Rohan Smith,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2008.00635,</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2008.00635" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/talbot2020benchbot.png"></a>
  
  
  <span class="abstract">
    We introduce BenchBot, a novel software suite for benchmarking the performance of robotics research across both photorealistic 3D simulations and real robot platforms. BenchBot provides a simple interface to the sensorimotor capabilities of a robot when solving robotics research problems; an interface that is consistent regardless of whether the target platform is simulated or a real robot. In this paper we outline the BenchBot system architecture, and explore the parallels between its user-centric design and an ideal research development process devoid of tangential robot engineering challenges.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2008.00635" target="_blank">arXiv</a>]
  
  
    [<a href="http://www.benchbot.org" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2008.00635" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="rahman2020performance">
  
    
    <a href="https://arxiv.org/abs/2009.08650" class="title" target="_blank">Performance Monitoring of Object Detection During Deployment</a>
    
    <span class="author">
      
        
          
            
              Quazi Marufur Rahman,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2009.08650,</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2009.08650" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rahman2020performance.png"></a>
  
  
  <span class="abstract">
    Performance monitoring of object detection is crucial for safety-critical applications such as autonomous vehicles that operate under varying and complex environmental conditions. Currently, object detectors are evaluated using summary metrics based on a single dataset that is assumed to be representative of all future deployment conditions. In practice, this assumption does not hold, and the performance fluctuates as a function of the deployment conditions. To address this issue, we propose an introspection approach to performance monitoring during deployment without the need for ground truth data. We do so by predicting when the per-frame mean average precision drops below a critical threshold using the detector’s internal features.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2009.08650" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2009.08650" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="corke2020can">
  
    
    <a href="https://arxiv.org/abs/2001.02366" class="title" target="_blank">What can robotics research learn from computer vision research?</a>
    
    <span class="author">
      
        
          
            
              Peter Corke,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              John Skinner,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2001.02366,</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2001.02366" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/corke2020research.png"></a>
  
  
  <span class="abstract">
    The computer vision and robotics research communities are each strong. However progress in computer vision has become turbo-charged in recent years due to big data, GPU computing, novel learning algorithms and a very effective research methodology. By comparison, progress in robotics seems slower. It is true that robotics came later to exploring the potential of learning – the advantages over the well-established body of knowledge in dynamics, kinematics, planning and control is still being debated, although reinforcement learning seems to offer real potential. However, the rapid development of computer vision compared to robotics cannot be only attributed to the former’s adoption of deep learning. In this paper, we argue that the gains in computer vision are due to research methodology – evaluation under strict constraints versus experiments; bold numbers versus videos.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2001.02366" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2001.02366" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2019</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography"><li>

<div id="suenderhauf2019probabilistic">
  
    
    <a href="https://rdcu.be/bQR84" class="title" target="_blank">A Probabilistic Challenge for Object Detection</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              John Skinner,
            
          
        
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              Gustavo Carneiro,
            
          
        
      
        

          
            
              Peter Corke.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Nature Machine Intelligence,</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://rdcu.be/bQR84" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf19probabilistic.png"></a>
  
  
  <span class="abstract">
    To safely operate in the real world, robots need to evaluate how confident they are about what they see.
  A new competition challenges computer vision algorithms to not just detect and localize objects, but also report how certain they are.
  To this end, we introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://rdcu.be/bQR84" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Miller19sampling">
  
    
    <a href="https://arxiv.org/abs/1809.06006" class="title" target="_blank">Evaluating Merging Strategies for Sampling-based Uncertainty Techniques in Object Detection</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1809.06006" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller18sampling.png"></a>
  
  
  <span class="abstract">
    There has been a recent emergence of sampling-based techniques for estimating epistemic uncertainty in deep neural networks. While these methods can be applied to classification or semantic segmentation tasks by simply averaging samples, this is not the case for object detection, where detection sample bounding boxes must be accurately associated and merged. A weak merging strategy can significantly degrade the performance of the detector and yield an unreliable uncertainty measure. This paper provides the first in-depth investigation of the effect of different association and merging strategies. We compare different combinations of three spatial and two semantic affinity measures with four clustering methods for MC Dropout with a Single Shot Multi-Box Detector. Our results show that the correct choice of affinity-clustering combinations can greatly improve the effectiveness of the classification and spatial uncertainty estimation and the resulting object detection performance. We base our evaluation on a new mix of datasets that emulate near open-set conditions (semantically similar unknown classes), distant open-set conditions (semantically dissimilar unknown classes) and the common closed-set conditions (only known classes).
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1809.06006" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1809.06006" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Dasagi19transfer">
  
    
    <a href="https://arxiv.org/abs/1809.07480" class="title" target="_blank">Sim-to-Real Transfer of Robot Learning with Variable Length Inputs</a>
    
    <span class="author">
      
        
          
            
              Vibhavari Dasagi,
            
          
        
      
        
          
            
              Robert Lee,
            
          
        
      
        
          
            
              Serena Mou,
            
          
        
      
        
          
            
              Jake Bruce,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Jürgen Leitner.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Australasian Conf. for Robotics and Automation (ACRA),</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1809.07480" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/lee18zeroshot.png"></a>
  
  
  <span class="abstract">
    Current end-to-end deep Reinforcement Learning (RL) approaches require jointly learning perception, decision-making and low-level control from very sparse reward signals and high-dimensional inputs, with little capability of incorporating prior knowledge. In this work, we propose a framework that combines deep sets encoding, which allows for variable-length abstract representations, with modular RL that utilizes these representations, decoupling high-level decision making from low-level control. We successfully demonstrate our approach on the robot manipulation task of object sorting, showing that this method can learn effective policies within mere minutes of highly simplified simulation. The learned policies can be directly deployed on a robot without further training, and generalize to variations of the task unseen during training.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1809.07480" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1809.07480" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="stanislas2019airborne">
  
    
    <a href="https://eprints.qut.edu.au/133596/1/FSR_2019_paper_47.pdf" class="title" target="_blank">Airborne Particle Classification in LiDAR Point Clouds Using Deep Learning</a>
    
    <span class="author">
      
        
          
            
              Leo Stanislas,
            
          
        
      
        
          
            
              Julian Nubert,
            
          
        
      
        
          
            
              Daniel Dugas,
            
          
        
      
        
          
            
              Julia Nitsch,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Roland Siegwart,
            
          
        
      
        
          
            
              Cesar Cadena,
            
          
        
      
        

          
            
              Thierry Peynot.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Conference on Field and Service Robotics (FSR),</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://eprints.qut.edu.au/133596/1/FSR_2019_paper_47.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/stanislas19lidar.png"></a>
  
  
  <span class="abstract">
    In this work, we propose and compare two deep learning approaches to classify airborne particles in LiDAR data. The first is based on voxel-wise classification, while the second is based on point-wise classification. We also study the impact of different combinations of input features extracted from LiDAR data, including the use of multi-echo returns as a classification feature. We evaluate the performance of the proposed methods on a realistic dataset with the presence of fog and dust particles in outdoor scenes. We achieve an F1 score of 94% for the classification of airborne particles in LiDAR point clouds, thereby significantly outperforming the state-of-the-art.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://eprints.qut.edu.au/133596/1/FSR_2019_paper_47.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="rahman2019traffic">
  
    
    <a href="https://arxiv.org/abs/1903.06391" class="title" target="_blank">Did You Miss the Sign? A False Negative Alarm System for Traffic Sign Detectors</a>
    
    <span class="author">
      
        
          
            
              Quazi Marufur Rahman,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1903.06391" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rahman2019traffic.png"></a>
  
  
  <span class="abstract">
    In this paper, we propose an approach to identify traffic signs that have been mistakenly discarded by the object detector. The proposed method raises an alarm when it discovers a failure by the object detector to detect a traffic sign. This approach can be useful to evaluate the performance of the detector during the deployment phase. We trained a single shot multi-box object detector to detect traffic signs and used its internal features to train a separate false negative detector (FND). During deployment, FND decides whether the traffic sign detector has missed a sign or not.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1903.06391" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1903.06391" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="garg2019look">
  
    
    <a href="https://arxiv.org/abs/1902.07381" class="title" target="_blank">Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation</a>
    
    <span class="author">
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            
              V Babu,
            
          
        
      
        
          
            
              Thanuja Dharmasiri,
            
          
        
      
        
          
            
              Stephen Hausler,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Swagat Kumar,
            
          
        
      
        
          
            
              Tom Drummond,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1902.07381" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/garg2019look.png"></a>
  
  
  <span class="abstract">
    We present a new depth-and temporal-aware visual place recognition system that solves the opposing viewpoint, extreme appearance-change visual place recognition problem. Our system performs sequence-to-single matching by extracting depth-filtered keypoints using a state-of-the-art depth estimation pipeline, constructing a keypoint sequence over multiple frames from the reference dataset, and comparing those keypoints to those in a single query image. We evaluate the system on a challenging benchmark dataset and show that it consistently outperforms state-of-the-art techniques. We also develop a range of diagnostic simulation experiments that characterize the contribution of depth-filtered keypoint sequences with respect to key domain parameters including degree of appearance change and camera motion.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1902.07381" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1902.07381" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography"><li>

<div id="miller2019benchmarking">
  
    
    <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.pdf" class="title" target="_blank">Benchmarking Sampling-based Probabilistic Object Detectors</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Haoyang Zhang,
            
          
        
      
        
          
            
              David Hall,
            
          
        
      
        

          
            
              Feras Dayoub.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller2019benchmarking.png"></a>
  
  
  <span class="abstract">
    This paper provides the first benchmark for sampling-based probabilistic object detectors. A probabilistic object
  detector expresses uncertainty for all detections that reliably indicates object localisation and classification performance. We compare performance for two sampling-based
  uncertainty techniques, namely Monte Carlo Dropout and Deep Ensembles, when implemented into one-stage and
  two-stage object detectors, Single Shot MultiBox Detector and Faster R-CNN.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>arXiv Preprints</strong></p>
<ol class="bibliography"><li>

<div id="suenderhauf19keys">
  
    
    <a href="https://arxiv.org/abs/1909.07376" class="title" target="_blank">Where are the Keys? – Learning Object-Centric Navigation Policies on Semantic Maps with Graph Convolutional Networks</a>
    
    <span class="author">
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:1909.07376,</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1909.07376" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf19keys.png"></a>
  
  
  <span class="abstract">
    Emerging object-based SLAM algorithms can build a graph representation of an environment comprising nodes for robot poses and object landmarks. However, while this map will contain static objects such as furniture or appliances, many moveable objects (e.g. the car keys, the glasses, or a magazine), are not suitable as landmarks and will not be part of the map due to their non-static nature. We show that Graph Convolutional Networks can learn navigation policies to find such unmapped objects by learning to exploit the hidden probabilistic model that governs where these objects appear in the environment. The learned policies can generalise to object classes unseen during training by using word vectors that express semantic similarity as representations for object nodes in the graph. Furthermore, we show that the policies generalise to unseen environments with only minimal loss of performance. We demonstrate that pre-training the policy network with a proxy task can significantly speed up learning, improving sample efficiency.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1909.07376" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1909.07376" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2018</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography">
<li>

<div id="nicholson18quadricslam">
  
    
    <a href="https://ieeexplore.ieee.org/document/8440105/" class="title" target="_blank">QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Object-oriented SLAM</a>
    
    <span class="author">
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters (RA-L),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/document/8440105/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/nicholson18quadricslam.png"></a>
  
  
  <span class="abstract">
    In this paper, we use 2D object detections from multiple views to simultaneously estimate a 3D quadric surface for each object and localize the camera position. We derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D object detections can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for object detectors that addresses the challenge of partially visible objects, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1804.04011" target="_blank">arXiv</a>]
  
  
    [<a href="http://semanticslam.ai/quadricslam.html" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/document/8440105/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="sunderhauf2018limits">
  
    
    <a href="http://journals.sagepub.com/doi/abs/10.1177/0278364918770733" class="title" target="_blank">The Limits and Potentials of Deep Learning for Robotics</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Oliver Brock,
            
          
        
      
        
          
            
              Walter Scheirer,
            
          
        
      
        
          
            
              Raia Hadsell,
            
          
        
      
        
          
            
              Dieter Fox,
            
          
        
      
        
          
            
              Jürgen Leitner,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        
          
            
              Pieter Abbeel,
            
          
        
      
        
          
            
              Wolfram Burgard,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
               others.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>The International Journal of Robotics Research,</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="http://journals.sagepub.com/doi/abs/10.1177/0278364918770733" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf18limitsandpotentials.png"></a>
  
  
  <span class="abstract">
    The application of deep learning in robotics leads
to very specific problems and research questions that are
typically not addressed by the computer vision and machine
learning communities. In this paper we discuss a number
of robotics-specific learning, reasoning, and embodiment challenges
for deep learning. We explain the need for better evaluation
metrics, highlight the importance and unique challenges for
deep robotic learning in simulation, and explore the spectrum
between purely data-driven and model-driven approaches. We
hope this paper provides a motivating overview of important
research directions to overcome the current limitations, and
help fulfill the promising potentials of deep learning in robotics.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1804.06557" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="http://journals.sagepub.com/doi/abs/10.1177/0278364918770733" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Special Issues</strong></p>
<ol class="bibliography"><li>

<div id="Suenderhauf17">
  
    
    <a href="https://journals.sagepub.com/toc/ijr/37/4-5" class="title" target="_blank">Special Issue on Deep Learning in Robotics</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Jürgen Leitner,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Jose Neira.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>The International Journal of Robotics Research (IJRR),</em>
    
    
      2018.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="https://journals.sagepub.com/toc/ijr/37/4-5" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="hosseinzadeh2018structure">
  
    
    <span class="title">Structure Aware SLAM using Quadrics and Planes</span>
    
    <span class="author">
      
        
          
            
              M Hosseinzadeh,
            
          
        
      
        
          
            
              Y Latif,
            
          
        
      
        
          
            
              T Pham,
            
          
        
      
        
          
            
              N Sünderhauf,
            
          
        
      
        

          
            
              I Reid.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of Asian Conference on Computer Vision (ACCV),</em>
    
    
      2018.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
    [<a href="http://arxiv.org/abs/1804.09111" target="_blank">arXiv</a>]
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="stanislas2018lidar">
  
    
    <span class="title">Lidar-based Detection of Airborne Particles for Robust Robot Perception</span>
    
    <span class="author">
      
        
          
            
              Leo Stanislas,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Thierry Peynot.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2018.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Bruce18navigation">
  
    
    <a href="https://arxiv.org/abs/1807.05211" class="title" target="_blank">Learning Deployable Navigation Policies at Kilometer Scale from a Single Traversal</a>
    
    <span class="author">
      
        
          
            
              Jake Bruce,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Piotr Mirowski,
            
          
        
      
        
          
            
              Raia Hadsell,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Conference on Robot Learning (CoRL),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1807.05211" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/bruce18navigation.png"></a>
  
  
  <span class="abstract">
    We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multiple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1807.05211" target="_blank">arXiv</a>]
  
  
    [<a href="http://rl-navigation.github.io/deployable" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/1807.05211" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="garg2018lost">
  
    
    <a href="https://arxiv.org/pdf/1804.05526" class="title" target="_blank">LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics</a>
    
    <span class="author">
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Robotics: Science and Systems (RSS),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="https://arxiv.org/pdf/1804.05526" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/garg18rss.png"></a>
  
  
  <span class="abstract">
    In this paper we develop a suite of novel semantic- and appearance-based techniques to enable for the first time high performance place recognition in the challenging scenario of recognizing places when returning from the opposite direction. We first propose a novel Local Semantic Tensor (LoST) descriptor of images using the convolutional feature maps from a state-of-the-art dense semantic segmentation network. Then, to verify the spatial semantic arrangement of the top matching candidates, we develop a novel approach for mining semantically-salient keypoint correspondences. 
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://arxiv.org/pdf/1804.05526" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Miller18">
  
    
    <a href="http://arxiv.org/abs/1710.06677" class="title" target="_blank">Dropout Sampling for Robust Object Detection in Open-Set Conditions</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="http://arxiv.org/abs/1710.06677" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller18dropout.png"></a>
  
  
  <span class="abstract">
    Dropout Variational Inference, or Dropout Sampling, has been recently proposed as an
approximation technique for Bayesian Deep Learning and evaluated for image classification
and regression tasks. This paper investigates the utility of Dropout Sampling for object
detection for the first time. We demonstrate how label uncertainty can be extracted from a
state-of-the-art object detection system via Dropout Sampling. We show that this uncertainty
can be utilized to increase object detection performance under the open-set conditions that
are typically encountered in robotic vision. We evaluate this approach on a large synthetic
dataset with 30,000 images, and a real-world dataset captured by a mobile robot in a
versatile campus environment.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://arxiv.org/abs/1710.06677" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="anderson2017vision">
  
    
    <a href="http://arxiv.org/abs/1711.07280" class="title" target="_blank">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</a>
    
    <span class="author">
      
        
          
            
              Peter Anderson,
            
          
        
      
        
          
            
              Qi Wu,
            
          
        
      
        
          
            
              Damien Teney,
            
          
        
      
        
          
            
              Jake Bruce,
            
          
        
      
        
          
            
              Mark Johnson,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        
          
            
              Stephen Gould,
            
          
        
      
        

          
            
              Anton van den Hengel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="http://arxiv.org/abs/1711.07280" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/anderson18navigation.png"></a>
  
  
  <span class="abstract">
     To enable and encourage the application of vision and
language methods to the problem of interpreting visually grounded
navigation instructions, we present the Matterport3D
Simulator – a large-scale reinforcement learning
environment based on real imagery. Using this simulator,
which can in future support a range of embodied vision
and language tasks, we provide the first benchmark dataset
for visually-grounded natural language navigation in real
buildings – the Room-to-Room (R2R) dataset.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://arxiv.org/abs/1711.07280" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Garg18">
  
    
    <a href="https://arxiv.org/pdf/1801.05078" class="title" target="_blank">Don’t Look Back: Robustifying Place Categorization for Viewpoint- and Condition-Invariant Place Recognition</a>
    
    <span class="author">
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="https://arxiv.org/pdf/1801.05078" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/garg18lookBack.png"></a>
  
  
  <span class="abstract">
    In this work, we develop a novel methodology for using the semantics-aware
higher-order layers of deep neural networks for recognizing
specific places from within a reference database. To further
improve the robustness to appearance change, we develop a
descriptor normalization scheme that builds on the success of
normalization schemes for pure appearance-based techniques.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://arxiv.org/pdf/1801.05078" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Trung18">
  
    
    <a href="http://arxiv.org/abs/1709.07158" class="title" target="_blank">SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes</a>
    
    <span class="author">
      
        
          
            
              Trung T. Pham,
            
          
        
      
        
          
            
              Thanh-Toan Do,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="http://arxiv.org/abs/1709.07158" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/pham18scenecut.png"></a>
  
  
  <span class="abstract">
    This paper presents SceneCut, a novel approach to jointly discover previously unseen
objects and non-object surfaces using a single RGB-D image. SceneCut’s joint reasoning
over scene semantics and geometry allows a robot to detect and segment object instances
in complex scenes where modern deep learning-based methods either fail to separate
object instances, or fail to detect objects that were not seen during training. SceneCut
automatically decomposes a scene into meaningful regions which either represent objects
or scene surfaces. The decomposition is qualified by an unified energy function over
objectness and geometric fitting. We show how this energy function can be optimized
efficiently by utilizing hierarchical segmentation trees.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://arxiv.org/abs/1709.07158" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography"><li>

<div id="nicholson18quadricslam">
  
    
    <a href="https://arxiv.org/abs/1804.04011v1" class="title" target="_blank">QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Semantic SLAM</a>
    
    <span class="author">
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Representing a Complex World, International Conference on Robotics and Automation (ICRA),</em>
    
    
      2018.
    
    </span>
  

  
  <span class="award">Best Workshop Paper Award</span>
  

  
  <a href="https://arxiv.org/abs/1804.04011v1" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/nicholson18quadricslamworkshop.png"></a>
  
  
  <span class="abstract">
    We derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D bounding boxes (such as those typically obtained from visual object detection systems) can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for deep-learned object detectors that addresses the challenge of partial object detections often encountered in robotics applications, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1804.04011" target="_blank">arXiv</a>]
  
  
    [<a href="http://semanticslam.ai/quadricslam.html" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/1804.04011v1" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>arXiv Preprints</strong></p>
<ol class="bibliography"><li>

<div id="Jablonsky18geometric">
  
    
    <a href="https://arxiv.org/abs/1809.06977" class="title" target="_blank">An Orientation Factor for Object-Oriented SLAM</a>
    
    <span class="author">
      
        
          
            
              Natalie Jablonsky,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint,</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1809.06977" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/jablonsky18geometric.png"></a>
  
  
  <span class="abstract">
    Current approaches to object-oriented SLAM lack the ability to incorporate prior knowledge of the scene geometry, such as the expected global orientation of objects. We overcome this limitation by proposing a geometric factor that constrains the global orientation of objects in the map, depending on the objects’ semantics. This new geometric factor is a first example of how semantics can inform and improve geometry in object-oriented SLAM. We implement the geometric factor for the recently proposed QuadricSLAM that represents landmarks as dual quadrics. The factor probabilistically models the quadrics’ major axes to be either perpendicular to or aligned with the direction of gravity, depending on their semantic class. Our experiments on simulated and real-world datasets show that using the proposed factors to incorporate prior knowledge improves both the trajectory and landmark quality.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1809.06977" target="_blank">arXiv</a>]
  
  
    [<a href="http://semanticslam.ai/geometricfactors.html" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/1809.06977" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2017</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography"><li>

<div id="McMahon17">
  
    
    <a href="https://ieeexplore.ieee.org/document/7959072/" class="title" target="_blank">Multi-Modal Trip Hazard Affordance Detection On Construction Sites</a>
    
    <span class="author">
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael J Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters (RA-L),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/document/7959072/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/mcmahon17hazards.png"></a>
  
  
  <span class="abstract">
    Trip hazards are a significant contributor to accidents on construction and manufacturing sites. We conduct a comprehensive investigation into the performance characteristics of 11 different colors and depth fusion approaches, including four fusion and one nonfusion approach, using color and two types of depth images. Trained and tested on more than 600 labeled trip hazards over four floors and 2000 m2 in an active construction site, this approach was able to differentiate between identical objects in different physical configurations. Outperforming a color-only detector, our multimodal trip detector fuses color and depth information to achieve a 4% absolute improvement in F1-score. These investigative results and the extensive publicly available dataset move us one step closer to assistive or fully automated safety inspection systems on construction sites.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/document/7959072/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf17a">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/8206392/" class="title" target="_blank">Meaningful Maps With Object-Oriented Semantic Mapping</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Trung T. Pham Pham,
            
          
        
      
        
          
            
              Yasir Latif,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/8206392/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf17maps.png"></a>
  
  
  <span class="abstract">
    For intelligent robots to interact in meaningful ways with their environment, they must understand both the geometric and semantic properties of the scene surrounding them. The majority of research to date has addressed these mapping challenges separately, focusing on either geometric or semantic mapping. In this paper we address the problem of building environmental maps that include both semantically meaningful, object-level entities and point- or mesh-based geometrical representations. We simultaneously build geometric point cloud models of previously unseen instances of known object classes and create a map that contains these object models as central entities. Our system leverages sparse, feature-based RGB-D SLAM, image-based deep-learning object detection and 3D unsupervised segmentation.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/8206392/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Leitner17">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/7989545/" class="title" target="_blank">The ACRV Picking Benchmark: A Robotic Shelf Picking Benchmark to Foster Reproducible Research</a>
    
    <span class="author">
      
        
          
            
              Jürgen Leitner,
            
          
        
      
        
          
            
              Adam W Tow,
            
          
        
      
        
          
            
              Jake E Dean,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Joseph W Durham,
            
          
        
      
        
          
            
              Matthew Cooper,
            
          
        
      
        
          
            
              Markus Eich,
            
          
        
      
        
          
            
              Christopher Lehnert,
            
          
        
      
        
          
            
              Ruben Mangels,
            
          
        
      
        
          
            
              Christopher McCool,
            
          
        
      
        
          
            
              Peter Kujala,
            
          
        
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Trung Pham,
            
          
        
      
        
          
            
              James Sergeant,
            
          
        
      
        
          
            
              Fangyi Zhang,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Peter Corke.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/7989545/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/leitner17picking.png"></a>
  
  
  <span class="abstract">
    Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress. They make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking: the ACRV Picking Benchmark. Designed to be reproducible, it consists of a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparison of complete robotic systems - including perception and manipulation - instead of sub-systems only. Our paper also describes and reports results achieved by an open baseline system based on a Baxter robot.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/7989545/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Chen17">
  
    
    <a href="http://ieeexplore.ieee.org/abstract/document/7989366/" class="title" target="_blank">Deep Learning Features at Scale for Visual Place Recognition</a>
    
    <span class="author">
      
        
          
            
              Zetao Chen,
            
          
        
      
        
          
            
              Adam Jacobson,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        
          
            
              Lingqiao Liu,
            
          
        
      
        
          
            
              Chunhua Shen,
            
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="http://ieeexplore.ieee.org/abstract/document/7989366/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/chen17placerec.png"></a>
  
  
  <span class="abstract">
    In this paper, we train, at large scale, two CNN architectures for the specific place recognition task and employ a multi-scale feature encoding method to generate condition- and viewpoint-invariant features. To enable this training to occur, we have developed a massive Specific PlacEs Dataset (SPED) with hundreds of examples of place appearance change at thousands of different places, as opposed to the semantic place type datasets currently available. This new dataset enables us to set up a training regime that interprets place recognition as a classification problem. We comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10% increase in performance over other place recognition algorithms and pre-trained CNNs. 
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://ieeexplore.ieee.org/abstract/document/7989366/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="McMahon17a">
  
    
    <a href="http://www.araa.asn.au/acra/acra2017/papers/pap104s1-file1.pdf" class="title" target="_blank">Auxiliary Tasks to Improve Trip Hazard Affordance Detection</a>
    
    <span class="author">
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            
              Tong Shen,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        
          
            
              Chunhua Shen,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="http://www.araa.asn.au/acra/acra2017/papers/pap104s1-file1.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/mcmahon17a.png"></a>
  
  
  <span class="abstract">
    We propose to train a CNN performing pixel-wise trip detection with three auxiliary tasks to help the CNN better infer scene geometric properties of trip hazards. Of the three approaches investigated pixel-wise ground plane estimation, pixel depth estimation and pixel height above ground plane estimation, the first approach allowed the trip detector to achieve a 11.1% increase in Trip IOU over earlier work. These new approaches make it plausible to deploy a robotic platform to perform trip hazard detection, and so potentially reduce the number of injuries on construction sites.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://www.araa.asn.au/acra/acra2017/papers/pap104s1-file1.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Miller17a">
  
    
    <a href="http://bayesiandeeplearning.org/2017/papers/20.pdf" class="title" target="_blank">Dropout Variational Inference Improves Object Detection in Open-Set Conditions</a>
    
    <span class="author">
      
        
          
            
              Dimity Miller,
            
          
        
      
        
          
            
              Lachlan Nicholson,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of NIPS Workshop on Bayesian Deep Learning,</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="http://bayesiandeeplearning.org/2017/papers/20.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/miller17dropout.png"></a>
  
  
  <span class="abstract">
    One of the biggest current challenges of visual object detection is reliable operation in open-set
conditions. One way to handle the open-set problem is to utilize the uncertainty of the model to reject predictions
with low probability. Bayesian Neural Networks (BNNs), with variational inference commonly
used as an approximation, is an established approach to estimate model uncertainty. Here we extend the concept of Dropout sampling to object detection for the first time. We evaluate
Bayesian object detection on a large synthetic and a real-world dataset and show how the estimated
label uncertainty can be utilized to increase object detection performance under open-set conditions.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://bayesiandeeplearning.org/2017/papers/20.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Dayoub17">
  
    
    <a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/papers/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.pdf" class="title" target="_blank">Episode-Based Active Learning with Bayesian Neural Networks</a>
    
    <span class="author">
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Corke.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Deep Learning for Robotic Vision, Conference on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/papers/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/dayoub17active.png"></a>
  
  
  <span class="abstract">
    We investigate different strategies for active learning
with Bayesian deep neural networks. We focus our analysis
on scenarios where new, unlabeled data is obtained episodically,
such as commonly encountered in mobile robotics
applications. An evaluation of different strategies for acquisition,
updating, and final training on the CIFAR-10 dataset
shows that incremental network updates with final training
on the accumulated acquisition set are essential for best
performance, while limiting the amount of required human
labeling labor.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/papers/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Bruce17">
  
    
    <a href="https://arxiv.org/abs/1711.10137" class="title" target="_blank">One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay</a>
    
    <span class="author">
      
        
          
            
              Jacob Bruce,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Piotr Mirowski,
            
          
        
      
        
          
            
              Raia Hadsell,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of NIPS Workshop on Acting and Interacting in the Real World: Challenges in Robot Learning,</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1711.10137" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/bruce17navigation.png"></a>
  
  
  <span class="abstract">
    Recently, model-free reinforcement learning algorithms have been shown to solve challenging problems by learning from extensive interaction with the environment. A significant issue with transferring this success to the robotics domain is that interaction with the real world is costly, but training on limited experience is prone to overfitting. We present a method for learning to navigate, to a fixed goal and in a known environment, on a mobile robot. The robot leverages an interactive world model built from a single traversal of the environment, a pre-trained visual feature encoder, and stochastic environmental augmentation, to demonstrate successful zero-shot transfer under real-world environmental variations without fine-tuning.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1711.10137" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2016</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Tow16">
  
    
    <a href="http://eprints.qut.edu.au/104307/" class="title" target="_blank">A Robustness Analysis of Deep Q Networks</a>
    
    <span class="author">
      
        
          
            
              Adam W Tow,
            
          
        
      
        
          
            
              Sareh Shirazi,
            
          
        
      
        
          
            
              Jürgen Leitner,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Ben Upcroft.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2016.
    
    </span>
  

  

  
  <a href="http://eprints.qut.edu.au/104307/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/tow16dqn.png"></a>
  
  
  <span class="abstract">
    In this paper, we present an analysis of the robustness of Deep Q Networks to various types of perceptual noise (changing brightness, Gaussian blur, salt and pepper, distractors). We present a benchmark example that involves playing the game Breakout though a webcam and screen environment, like humans do. We present a simple training approach to improve the performance maintained when transferring a DQN agent trained in simulation to the real world. We also evaluate DQN agents trained under a variety of simulation environments to report for the first time how DQNs cope with perceptual noise, common to real world robotic applications.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://eprints.qut.edu.au/104307/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Skinner16">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/7759425/" class="title" target="_blank">High-Fidelity Simulation for Evaluating Robotic Vision Performance</a>
    
    <span class="author">
      
        
          
            
              John Robert Skinner,
            
          
        
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael J Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2016.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/7759425/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/skinner16sim.png"></a>
  
  
  <span class="abstract">
    For machine learning applications a critical bottleneck is the limited amount of real world image data that can be captured and labelled for both training and testing purposes. In this paper we investigate the use of a photo-realistic simulation tool to address these challenges, in three specific domains: robust place recognition, visual SLAM and object recognition. For the first two problems we generate images from a complex 3D environment with systematically varying camera paths, camera viewpoints and lighting conditions. For the first time we are able to systematically characterise the performance of these algorithms as paths and lighting conditions change. In particular, we are able to systematically generate varying camera viewpoint datasets that would be difficult or impossible to generate in the real world. We also compare algorithm results for a camera in a real environment and a simulated camera in a simulation model of that real environment. Finally, for the object recognition domain, we generate labelled image data and characterise the viewpoint dependency of a current convolution neural network in performing object recognition. Together these results provide a multi-domain demonstration of the beneficial properties of using simulation to characterise and analyse a wide range of robotic vision algorithms.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/7759425/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Leitner16">
  
    
    <a href="http://ieeexplore.ieee.org/abstract/document/7500760/" class="title" target="_blank">LunaRoo: Designing a Hopping Lunar Science Payload.</a>
    
    <span class="author">
      
        
          
            
              Jürgen Leitner,
            
          
        
      
        
          
            
              Will Chamberlain,
            
          
        
      
        
          
            
              Donald G. Dansereau,
            
          
        
      
        
          
            
              Matthew Dunbabin,
            
          
        
      
        
          
            
              Markus Eich,
            
          
        
      
        
          
            
              Thierry Peynot,
            
          
        
      
        
          
            
              Jon Roberts,
            
          
        
      
        
          
            
              Raymond Russell,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE Aerospace Conference,</em>
    
    
      2016.
    
    </span>
  

  

  
  <a href="http://ieeexplore.ieee.org/abstract/document/7500760/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/leitner16luna.png"></a>
  
  
  <span class="abstract">
    We describe a hopping science payload solution designed to exploit the Moon’s lower gravity to leap up to 20m above the surface. The entire solar-powered robot is compact enough to fit within a 10cm cube, whilst providing unique observation and mission capabilities by creating imagery during the hop. The LunaRoo concept is a proposed payload to fly onboard a Google Lunar XPrize entry. Its compact form is specifically designed for lunar exploration and science mission within the constraints given by PTScientists. The core features of LunaRoo are its method of locomotion - hopping like a kangaroo - and its imaging system capable of unique over-the-horizon perception. The payload will serve as a proof of concept, highlighting the benefits of alternative mobility solutions, in particular enabling observation and exploration of terrain not traversable by wheeled robots. in addition providing data for beyond line-of-sight planning and communications for surface assets, extending overall mission capabilities.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://ieeexplore.ieee.org/abstract/document/7500760/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf16">
  
    
    <a href="http://ieeexplore.ieee.org/abstract/document/7487796/" class="title" target="_blank">Place Categorization and Semantic Mapping on a Mobile Robot</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Ruth Schulz,
            
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        
          
            
              Gordon Wyeth,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2016.
    
    </span>
  

  

  
  <a href="http://ieeexplore.ieee.org/abstract/document/7487796/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf16mapping.png"></a>
  
  
  <span class="abstract">
    In this paper we focus on the challenging problem of place categorization and semantic mapping on a robot without environment-specific training. Motivated by their ongoing success in various visual recognition tasks, we build our system upon a state-of-the-art convolutional network. We overcome its closed-set limitations by complementing the network with a series of one-vs-all classifiers that can learn to recognize new semantic classes online. Prior domain knowledge is incorporated by embedding the classification system into a Bayesian filter framework that also ensures temporal coherence. We evaluate the classification accuracy of the system on a robot that maps a variety of places on our campus in real-time. We show how semantic information can boost robotic object detection performance and how the semantic map can be used to modulate the robot’s behaviour during navigation tasks. The system is made available to the community as a ROS module.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://ieeexplore.ieee.org/abstract/document/7487796/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2015</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography">
<li>

<div id="Lowry15">
  
    
    <a href="/assets/papers/visual_place_recognition_a_survey.pdf" class="title" target="_blank">Visual Place Recognition: A Survey</a>
    
    <span class="author">
      
        
          
            
              Stephanie Lowry,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Paul Newman,
            
          
        
      
        
          
            
              John J Leonard,
            
          
        
      
        
          
            
              David Cox,
            
          
        
      
        
          
            
              Peter Corke,
            
          
        
      
        

          
            
              Michael J Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Transactions on Robotics (TRO),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="/assets/papers/visual_place_recognition_a_survey.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/lowry15survey.png"></a>
  
  
  <span class="abstract">
    This paper presents a survey of the visual place
recognition research landscape. We start by introducing the
concepts behind place recognition – the role of place recognition
in the animal kingdom, how a “place” is defined in a robotics
context, and the major components of a place recognition system.
We then survey visual place recognition solutions for
environments where appearance change is assumed to be
negligible. Long term robot operations have revealed that
environments continually change; consequently we survey place
recognition solutions that implicitly or explicitly account for
appearance change within the environment. Finally we close with
a discussion of the future of visual place recognition, in particular
with respect to the rapid advances being made in the related
fields of deep learning, semantic scene understanding and video
description.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/visual_place_recognition_a_survey.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="neubert2015superpixel">
  
    
    <a href="/assets/papers/ACP_RAS.pdf" class="title" target="_blank">Superpixel-based appearance change prediction for long-term navigation across seasons</a>
    
    <span class="author">
      
        
          
            
              Peer Neubert,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Robotics and Autonomous Systems,</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="/assets/papers/ACP_RAS.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/neubert15change.png"></a>
  
  
  <span class="abstract">
    The goal of our work is to support
existing approaches to place recognition by learning how the
visual appearance of an environment changes over time and by
using this learned knowledge to predict its appearance under
different environmental conditions. We describe the general
idea of appearance change prediction (ACP) and investigate
properties of our novel implementation based on vocabularies
of superpixels (SP-ACP). This paper deepens the
understanding of the proposed SP-ACP system and evaluates
the influence of its parameters. We present the results of a largescale
experiment on the complete 10 hour Nordland dataset and
appearance change predictions between different combinations
of seasons.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/ACP_RAS.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Hall15">
  
    
    <a href="http://ieeexplore.ieee.org/abstract/document/7045965/" class="title" target="_blank">Evaluation of features for leaf classification in challenging conditions</a>
    
    <span class="author">
      
        
          
            
              David Hall,
            
          
        
      
        
          
            
              Chris McCool,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Ben Upcroft.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on,</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="http://ieeexplore.ieee.org/abstract/document/7045965/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/hall15wacv.png"></a>
  
  
  <span class="abstract">
    Fine-grained leaf classification has concentrated on the use of traditional shape and statistical features to classify ideal images. In this paper we evaluate the effectiveness of traditional hand-crafted features and propose the use of deep convolutional neural network (Conv Net) features. We introduce a range of condition variations to explore the robustness of these features, including: translation, scaling, rotation, shading and occlusion. Evaluations on the Flavia dataset demonstrate that in ideal imaging conditions, combining traditional and Conv Net features yields state-of-the art performance with an average accuracy of 97.3%±0:6% compared to traditional features which obtain an average accuracy of 91.2%±1:6%. Further experiments show that this combined classification approach consistently outperforms the best set of traditional features by an average of 5.7% for all of the evaluated condition variations.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://ieeexplore.ieee.org/abstract/document/7045965/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Sergeant15">
  
    
    <a href="http://www.araa.asn.au/acra/acra2015/papers/pap149.pdf" class="title" target="_blank">Multimodal Deep Autoencoders for Control of a Mobile Robot</a>
    
    <span class="author">
      
        
          
            
              James Sergeant,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Ben Upcroft.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="http://www.araa.asn.au/acra/acra2015/papers/pap149.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/sergeant15.png"></a>
  
  
  <span class="abstract">
    Robot navigation systems are typically engineered
to suit certain platforms, sensing suites
and environment types. In order to deploy a
robot in an environment where its existing navigation
system is insufficient, the system must
be modified manually, often at significant cost.
In this paper we address this problem, proposing
a system based on multimodal deep autoencoders
that enables a robot to learn how
to navigate by observing a dataset of sensor input
and motor commands collected while being
teleoperated by a human. Low-level features
and cross modal correlations are learned and
used in initialising two different architectures
with three operating modes. During operation,
these systems exploit the learned correlations
in generating suitable control signals based only
on the sensor information.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://www.araa.asn.au/acra/acra2015/papers/pap149.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="McMahon15b">
  
    
    <a href="http://www.araa.asn.au/acra/acra2015/papers/pap158.pdf" class="title" target="_blank">TripNet: Detecting Trip Hazards on Construction Sites</a>
    
    <span class="author">
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Ben Upcroft.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="http://www.araa.asn.au/acra/acra2015/papers/pap158.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/mcmahon15.png"></a>
  
  
  <span class="abstract">
    This paper introduces TripNet, a robotic vision system that detects trip hazards using raw construction site images.
  TripNet performs trip hazard identification using only camera imagery and minimal training with a pre-trained Convolutional Neural Network (CNN) rapidly fine-tuned on a small corpus of labelled image regions from construction sites. There is no reliance on prior scene segmentation methods during deployment. Trip-Net achieves comparable performance to a human on a dataset recorded in two distinct real world construction sites. TripNet exhibits spatial and temporal generalization by functioning in previously unseen parts of a construction site and over time periods of several weeks.
  
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://www.araa.asn.au/acra/acra2015/papers/pap158.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Rezazadegan15">
  
    
    <span class="title">Enhancing Human Action Recognition with Region Proposals</span>
    
    <span class="author">
      
        
          
            
              Fahimeh Rezazadegan,
            
          
        
      
        
          
            
              Sareh Shirazi,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Ben Upcroft.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2015.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf15a">
  
    
    <a href="/assets/papers/rss15_placeRec.pdf" class="title" target="_blank">Place Recognition with ConvNet Landmarks: Viewpoint-Robust, Condition-Robust, Training-Free</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Sareh Shirazi,
            
          
        
      
        
          
            
              Adam Jacobson,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Edward Pepperell,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Robotics: Science and Systems (RSS),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="/assets/papers/rss15_placeRec.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf15rss.png"></a>
  
  
  <span class="abstract">
    Here
we present an approach that adapts state-of-the-art object
proposal techniques to identify potential landmarks within an
image for place recognition. We use the astonishing power
of convolutional neural network features to identify matching
landmark proposals between images to perform place recognition
over extreme appearance and viewpoint variations. Our system
does not require any form of training, all components are generic
enough to be used off-the-shelf. We present a range of challenging
experiments in varied viewpoint and environmental conditions.
We demonstrate superior performance to current state-of-the-art
techniques.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/rss15_placeRec.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
    [<a href="https://nikosuenderhauf.github.io/assets/papers/RSS-15-poster.pdf" target="_blank">Poster</a>]
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf15">
  
    
    <a href="/assets/papers/IROS15-placeRecognition.pdf" class="title" target="_blank">On the Performance of ConvNet Features for Place Recognition</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Sareh Shirazi,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="/assets/papers/IROS15-placeRecognition.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf15iros.png"></a>
  
  
  <span class="abstract">
    This paper comprehensively evaluates
and compares the utility of three state-of-the-art ConvNets on
the problems of particular relevance to navigation for robots;
viewpoint-invariance and condition-invariance, and for the first
time enables real-time place recognition performance using
ConvNets with large maps by integrating a variety of existing
(locality-sensitive hashing) and novel (semantic search space
partitioning) optimization techniques. We present extensive
experiments on four real world datasets cultivated to evaluate
each of the specific challenges in place recognition. The results
demonstrate that speed-ups of two orders of magnitude can
be achieved with minimal accuracy degradation, enabling
real-time performance. We confirm that networks trained for
semantic place categorization also perform better at (specific)
place recognition when faced with severe appearance changes
and provide a reference for which networks and layers are
optimal for different aspects of the place recognition problem.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/IROS15-placeRecognition.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Milford15">
  
    
    <a href="https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W11/papers/Milford_Sequence_Searching_With_2015_CVPR_paper.pdf" class="title" target="_blank">Sequence Searching with Deep-learnt Depth for Condition-and Viewpoint-invariant Route-based Place Recognition</a>
    
    <span class="author">
      
        
          
            
              Michael Milford,
            
          
        
      
        
          
            
              Stephanie Lowry,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Sareh Shirazi,
            
          
        
      
        
          
            
              Edward Pepperell,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        
          
            
              Chunhua Shen,
            
          
        
      
        
          
            
              Guosheng Lin,
            
          
        
      
        
          
            
              Fayao Liu,
            
          
        
      
        
          
            
              Cesar Cadena,
            
          
        
      
        

          
            
              Ian Reid.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Computer Vision in Vehicle Technology (CVVT), Conference on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W11/papers/Milford_Sequence_Searching_With_2015_CVPR_paper.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/milford15.png"></a>
  
  
  <span class="abstract">
    Vision-based localization on robots and vehicles remains unsolved when extreme appearance change and viewpoint change are present simultaneously. In this paper we significantly improve the viewpoint invariance of the SeqSLAM algorithm by using state-of-the-art deep learning techniques to generate synthetic viewpoints. Our approach is different to other deep learning approaches in that it does not rely on the ability of the CNN network to learn invariant features, but only to produce“good enough” depth images from day-time imagery only. We evaluate the system on a new multi-lane day-night car dataset specifically gathered to simultaneously test both appearance and viewpoint change.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W11/papers/Milford_Sequence_Searching_With_2015_CVPR_paper.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf15b">
  
    
    <a href="https://eprints.qut.edu.au/109683/1/109683.pdf" class="title" target="_blank">Continuous Factor Graphs For Holistic Scene Understanding</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Scene Understanding (SUNw), Intl. Conf. on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="https://eprints.qut.edu.au/109683/1/109683.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf15b.png"></a>
  
  
  <span class="abstract">
    We propose a novel mathematical formulation for the
holistic scene understanding problem and transform it from
the discrete into the continuous domain. The problem can
then be modeled with a nonlinear continuous factor graph,
and the MAP solution is found via least squares optimization.
We evaluate our method on the realistic NYU2 dataset.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://eprints.qut.edu.au/109683/1/109683.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="McMahon15">
  
    
    <span class="title">How Good Are EdgeBoxes, Really?</span>
    
    <span class="author">
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Scene Understanding (SUNw), Intl. Conf. on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2015.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf15d">
  
    
    <a href="https://eprints.qut.edu.au/109668/1/109668.pdf" class="title" target="_blank">SLAM – Quo Vadis? In Support of Object Oriented and Semantic SLAM</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        
          
            
              Sean McMahon,
            
          
        
      
        
          
            
              Markus Eich,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on The Problem of Moving Sensors, Robotics: Science and Systems (RSS),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="https://eprints.qut.edu.au/109668/1/109668.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf15d.png"></a>
  
  
  <span class="abstract">
    Most current SLAM systems are still based on
primitive geometric features such as points, lines, or planes.
The created maps therefore carry geometric information, but
no immediate semantic information. With the recent significant
advances in object detection and scene classification we think the
time is right for the SLAM community to ask where the SLAM
research should be going during the next years. As a possible
answer to this question, we advocate developing SLAM systems
that are more object oriented and more semantically enriched
than the current state of the art. This paper provides an overview
of our ongoing work in this direction.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://eprints.qut.edu.au/109668/1/109668.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2014</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography"><li>

<div id="Suenderhauf14">
  
    
    <a href="/assets/papers/suenderhauf14spacebot.pdf" class="title" target="_blank">Phobos and Deimos on Mars – Two Autonomous Robots for the DLR SpaceBot
    Cup</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peer Neubert,
            
          
        
      
        
          
            
              Martina Truschzinski,
            
          
        
      
        
          
            
              Daniel Wunschel,
            
          
        
      
        
          
            
              Johannes Pöschmann,
            
          
        
      
        
          
            
              Sven Lanve,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of International Symposium on Artificial Intelligence,
    Robotics and Automation in Space (iSAIRAS),</em>
    
    
      2014.
    
    </span>
  

  

  
  <a href="/assets/papers/suenderhauf14spacebot.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf14spacebot.png"></a>
  
  
  <span class="abstract">
    In 2013, ten teams from German universities and research
institutes participated in a national robot competition
called SpaceBot Cup organized by the DLR Space
Administration. The robots had one hour to autonomously
explore and map a challenging Mars-like environment,
find, transport, and manipulate two objects, and navigate
back to the landing site. Localization without GPS in an
unstructured environment was a major issue as was mobile
manipulation and very restricted communication. This paper describes our system of two rovers operating on the
ground plus a quadrotor UAV simulating an observing orbiting
satellite. We relied on ROS (robot operating system)
as the software infrastructure and describe the main
ROS components utilized in performing the tasks. Despite
(or because of) faults, communication loss and breakdowns,
it was a valuable experience with many lessons
learned.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/suenderhauf14spacebot.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography"><li>

<div id="sunderhauf2014fine">
  
    
    <span class="title">Fine-Grained Plant Classification Using Convolutional Neural Networks for Feature Extraction.</span>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Chris McCool,
            
          
        
      
        
          
            
              Ben Upcroft,
            
          
        
      
        

          
            
              Tristan Perez.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In CLEF (Working Notes),</em>
    
    
      2014.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2013</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf13">
  
    
    <a href="/assets/papers/suenderhauf13delays.pdf" class="title" target="_blank">Incremental Sensor Fusion in Factor Graphs with Unknown Delays</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Sven Lange,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of ESA Symposium on Advanced Space Technologies in Robotics
    and Automation (ASTRA),</em>
    
    
      2013.
    
    </span>
  

  

  
  <a href="/assets/papers/suenderhauf13delays.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf13delays.png"></a>
  
  
  <span class="abstract">
     Our paper addresses the problem of performing
incremental sensor fusion in factor graphs when
some of the sensor information arrive with a significant
unknown delay. We develop and compare two techniques
to handle such delayed measurements under mild conditions
on the characteristics of that delay: We consider
the unknown delay to be bounded and quantizable into
multiples of the state transition cycle time. The proposed
methods are evaluated using a simulation of a dynamic
3-DoF system that fuses odometry and GPS measurements.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/suenderhauf13delays.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf13c">
  
    
    <a href="/assets/papers/IV13-NLOS-iSAM2.pdf" class="title" target="_blank">Switchable Constraints and Incremental Smoothing for Online Mitigation
    of Non-Line-of-Sight and Multipath Effects</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Marcus Obst,
            
          
        
      
        
          
            
              Sven Lange,
            
          
        
      
        
          
            
              Gerd Wanielik,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE Intelligent Vehicles Symposium (IV),</em>
    
    
      2013.
    
    </span>
  

  

  
  <a href="/assets/papers/IV13-NLOS-iSAM2.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf13iv.png"></a>
  
  
  <span class="abstract">
    Reliable vehicle positioning is a crucial requirement
for many applications of advanced driver assistance
systems. While satellite navigation provides a reasonable performance
in general, it often suffers from multipath and non-line-of-sight
errors when it is applied in urban areas and therefore
does not guarantee consistent results anymore. Our paper
proposes a novel online method that identifies and excludes
the affected pseudorange measurements. Our approach does
not depend on additional sensors, maps, or environmental
models. We rather formulate the positioning problem as a
Bayesian inference problem in a factor graph and combine
the recently developed concept of switchable constraints with
an algorithm for efficient incremental inference in such graphs.
We furthermore introduce the concepts of auxiliary updates and
factor graph pruning in order to accelerate convergence while
keeping the graph size and required runtime bounded. A realworld
experiment demonstrates that the resulting algorithm is
able to successfully localize despite a large number of satellite
observations are influenced by NLOS or multipath effects.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/IV13-NLOS-iSAM2.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf13a">
  
    
    <a href="/assets/papers/ICRA13-comparisonRobustSLAM.pdf" class="title" target="_blank">Switchable Constraints vs. Max-Mixture Models vs. RRR – A Comparison
    of three Approaches to Robust Pose Graph SLAM</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Intl. Conf. on Robotics and Automation (ICRA),</em>
    
    
      2013.
    
    </span>
  

  

  
  <a href="/assets/papers/ICRA13-comparisonRobustSLAM.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf13icra.png"></a>
  
  
  <span class="abstract">
    SLAM algorithms that can infer a correct map despite
the presence of outliers have recently attracted increasing
attention. In the context of SLAM, outlier constraints are typically
caused by a failed place recognition due to perceptional
aliasing. If not handled correctly, they can have catastrophic
effects on the inferred map. Since robust robotic mapping and
SLAM are among the key requirements for autonomous longterm
operation, inference methods that can cope with such data
association failures are a hot topic in current research. Our
paper compares three very recently published approaches to
robust pose graph SLAM, namely switchable constraints, maxmixture
models and the RRR algorithm. All three methods were
developed as extensions to existing factor graph-based SLAM
back-ends and aim at improving the overall system’s robustness
to false positive loop closure constraints. Due to the novelty of
the three proposed algorithms, no direct comparison has been
conducted so far.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/ICRA13-comparisonRobustSLAM.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Lange13">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/6630810/" class="title" target="_blank">Incremental Smoothing vs. Filtering for Sensor Fusion on an Indoor
    UAV</a>
    
    <span class="author">
      
        
          
            
              Sven Lange,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Intl. Conf. on Robotics and Automation (ICRA),</em>
    
    
      2013.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/6630810/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/lange13smoothing.png"></a>
  
  
  <span class="abstract">
    Our paper explores the performance of a recently proposed incremental smoother in the context of nonlinear sensor fusion for a real-world UAV. This efficient factor graph based smoothing approach has a number of advantages compared to conventional filtering techniques like the EKF or its variants. It can more easily incorporate asynchronous and delayed measurements from sensors operating at different rates and is supposed to be less error-prone in highly nonlinear settings. We compare the novel incremental smoothing approach based on iSAM2 against our conventional EKF based sensor fusion framework. Unlike previously presented work, the experiments are not only performed in simulation, but also on a real-world quadrotor UAV system using IMU, optical flow and altitude measurements.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/6630810/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Neubert13b">
  
    
    <a href="/assets/papers/ECMR13_ACP.pdf" class="title" target="_blank">Appearance Change Prediction for Long-Term Navigation Across Seasons</a>
    
    <span class="author">
      
        
          
            
              Peer Neubert,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of European Conference on Mobile Robotics (ECMR),</em>
    
    
      2013.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="/assets/papers/ECMR13_ACP.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf13d">
  
    
    <a href="/assets/papers/rss13Workshop.pdf" class="title" target="_blank">Predicting the Change – A Step Towards Life-Long Operation in Everyday Environments</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peer Neubert,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of Robotics: Science and Systems (RSS) Robotics Challenges
    and Vision Workshop,</em>
    
    
      2013.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="/assets/papers/rss13Workshop.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf13b">
  
    
    <a href="/assets/papers/openseqslam.pdf" class="title" target="_blank">Are We There Yet? Challenging SeqSLAM on a 3000 km Journey Across All
    Four Seasons.</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peer Neubert,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of Workshop on Long-Term Autonomy, IEEE International
    Conference on Robotics and Automation (ICRA),</em>
    
    
      2013.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="/assets/papers/openseqslam.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2012</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf12c">
  
    
    <a href="/assets/papers/IV12-multipathMitigation.pdf" class="title" target="_blank">Multipath Mitigation in GNSS-Based Localization using Robust Optimization</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Marcus Obst,
            
          
        
      
        
          
            
              Gerd Wanielik,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE Intelligent Vehicles Symposium (IV),</em>
    
    
      2012.
    
    </span>
  

  

  
  <a href="/assets/papers/IV12-multipathMitigation.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf12iv.png"></a>
  
  
  <span class="abstract">
    Our paper adapts recent advances in the SLAM
(Simultaneous Localization and Mapping) literature to the
problem of multipath mitigation and proposes a novel approach
to successfully localize a vehicle despite a significant
number of multipath observations. We show that GNSS-based
localization problems can be modelled as factor graphs and
solved using efficient nonlinear least squares methods that
exploit the sparsity inherent in the problem formulation. Using
a recently developed novel approach for robust optimization,
satellite observations that are subject to multipath errors can be
successfully identified and rejected during the optimization process.
We demonstrate the feasibility of the proposed approach
on a real-world urban dataset and compare it to an existing
method of multipath detection.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/IV12-multipathMitigation.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf12">
  
    
    <a href="/assets/papers/ICRA12-robustSLAM.pdf" class="title" target="_blank">Towards a Robust Back-End for Pose Graph SLAM</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE Intl. Conf. on Robotics and Automation (ICRA),</em>
    
    
      2012.
    
    </span>
  

  

  
  <a href="/assets/papers/ICRA12-robustSLAM.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf12icra.png"></a>
  
  
  <span class="abstract">
    We propose a novel formulation that allows the back-end
to change parts of the topological structure of the graph
during the optimization process. The back-end can thereby
discard loop closures and converge towards correct solutions
even in the presence of false positive loop closures. This largely
increases the overall robustness of the SLAM system and closes
a gap between the sensor-driven front-end and the back-end
optimizers. We demonstrate the approach and present results
both on large scale synthetic and real-world dataset
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/ICRA12-robustSLAM.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf12e">
  
    
    <a href="/assets/papers/IROS12-switchableConstraints.pdf" class="title" target="_blank">Switchable Constraints for Robust Pose Graph SLAM</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and
    Systems (IROS),</em>
    
    
      2012.
    
    </span>
  

  

  
  <a href="/assets/papers/IROS12-switchableConstraints.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf12iros.png"></a>
  
  
  <span class="abstract">
    Current SLAM back-ends are based on least
squares optimization and thus are not robust against outliers
like data association errors and false positive loop closure
detections. Our paper presents and evaluates a robust back-end
formulation for SLAM using switchable constraints. Instead
of proposing yet another appearance-based data association
technique, our system is able to recognize and reject outliers
during the optimization. This is achieved by making
the topology of the underlying factor graph representation
subject to the optimization instead of keeping it fixed. The
evaluation shows that the approach can deal with up to 1000
false positive loop closure constraints on various datasets. This
largely increases the robustness of the overall SLAM system
and closes a gap between the sensor-driven front-end and the
back-end optimizers.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/IROS12-switchableConstraints.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf12a">
  
    
    <span class="title">Towards Robust Graphical Models for GNSS-Based Localization in Urban
    Environments</span>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Multi-Conference on Systems, Signals
    and Devices (SSD),</em>
    
    
      2012.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography"><li>

<div id="Suenderhauf12d">
  
    
    <span class="title">A Generic Approach for Robust Probabilistic Estimation with Graphical
    Models</span>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of RSS Workshop on Long-term Operation of Autonomous Robotic
    Systems in Changing Environments,</em>
    
    
      2012.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Misc</strong></p>
<ol class="bibliography"><li>

<div id="Suenderhauf12b">
  
    
    <a href="http://nbn-resolving.de/urn:nbn:de:bsz:ch1-qucosa-86443" class="title" target="_blank">Robust Optimization for Simultaneous Localization and Mapping</a>
    
    <span class="author">
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>PhD Thesis. Chemnitz University of Technology</em>
    
    
      2012.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="http://nbn-resolving.de/urn:nbn:de:bsz:ch1-qucosa-86443" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2011</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf11">
  
    
    <a href="/assets/papers/briefGist.pdf" class="title" target="_blank">BRIEF-Gist – Closing the Loop by Simple Means</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE Intl. Conf. on Intelligent Robots and Systems (IROS),</em>
    
    
      2011.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="/assets/papers/briefGist.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Lange11">
  
    
    <a href="/assets/papers/lange11corridorAMiRE.pdf" class="title" target="_blank">Autonomous Corridor Flight of a UAV Using a Low-Cost and Light-Weight
    RGB-D Camera</a>
    
    <span class="author">
      
        
          
            
              Sven Lange,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peer Neubert,
            
          
        
      
        
          
            
              Sebastian Drews,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Intl. Symposium on Autonomous Mini Robots for Research and
    Edutainment (AMiRE),</em>
    
    
      2011.
    
    </span>
  

  

  
  <a href="/assets/papers/lange11corridorAMiRE.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/lange11corridorAMiRE.png"></a>
  
  
  <span class="abstract">
    We describe the first application of the novel Kinect RGB-D sensor on a fully autonomous quadrotor UAV. In contrast to the established RGB-D devices that are both expensive and comparably heavy, the Kinect is light-weight and especially low-cost. It provides dense color and depth information and can be readily applied to a variety of tasks in the robotics domain. We apply the Kinect on a UAV in an indoor corridor scenario. The sensor extracts a 3D point cloud of the environment that is further processed on-board to identify walls, obstacles, and the position and orientation of the UAV inside the corridor. Subsequent controllers for altitude, position, velocity, and heading enable the UAV to autonomously operate in this indoor environment.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/lange11corridorAMiRE.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Lange11a">
  
    
    <a href="/assets/papers/lange11corridor.pdf" class="title" target="_blank">Autonomous Corridor Flight of a UAV Using an RGB-D Camera</a>
    
    <span class="author">
      
        
          
            
              Sven Lange,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peer Neubert,
            
          
        
      
        
          
            
              Sebastian Drews,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of EuRobotics RGB-D Workshop on 3D Perception in Robotics,</em>
    
    
      2011.
    
    </span>
  

  

  
  <a href="/assets/papers/lange11corridor.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/lange11corridor.png"></a>
  
  
  <span class="abstract">
    We describe the first application of the novel Kinect RGB-D sensor on a fully autonomous quadrotor UAV. We apply
the UAV in an indoor corridor scenario. The position and
orientation of the UAV inside the corridor is extracted from
the RGB-D data. Subsequent controllers for altitude, posi-
tion, velocity, and heading enable the UAV to autonomously
operate in this indoor environment.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/lange11corridor.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2010</h3>

<p><strong>Journal Articles</strong></p>
<ol class="bibliography"><li>

<div id="Suenderhauf10">
  
    
    <a href="/assets/papers/suenderhauf10review.pdf" class="title" target="_blank">Learning from Nature: Biologically Inspired Robot Navigation and
    SLAM – A Review</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Künstliche Intelligenz (German Journal on Artificial Intelligence), Special Issue on SLAM,</em>
    
    
      2010.
    
    </span>
  

  

  
  <a href="/assets/papers/suenderhauf10review.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf10review.png"></a>
  
  
  <span class="abstract">
    In this paper we summarize the most important
neuronal fundamentals of navigation in rodents, primates
and humans. We review a number of brain cells that are
involved in spatial navigation and their properties. Furthermore,
we review RatSLAM, a working SLAM system that is
partially inspired by neuronal mechanisms underlying mammalian
spatial navigation.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/suenderhauf10review.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf10a">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/5653221/" class="title" target="_blank">The Causal Update Filter – A Novel Biologically Inspired Filter
    Paradigm for Appearance Based SLAM</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Peer Neubert,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the IEEE International Conference on Intelligent Robots
    and Systems (IROS),</em>
    
    
      2010.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/5653221/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf10causal.png"></a>
  
  
  <span class="abstract">
    Recently a SLAM algorithm based on biological principles (RatSLAM) has been proposed. It was proven to perform well in large and demanding scenarios. In this paper we establish a comparison of the principles underlying this algorithm with standard probabilistic SLAM approaches and identify the key difference to be an additive update step. Using this insight, we derive the novel, non-Bayesian Causal Update filter that is suitable for application in appearance-based SLAM. We successfully apply this new filter to two demanding vision-only urban SLAM problems of 5 and 66 km length. We show that it can functionally replace the core of RatSLAM, gaining a massive speed-up.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/5653221/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf10b">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/5641280/" class="title" target="_blank">Beyond RatSLAM: Improvements to a Biologically Inspired SLAM System</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc of Intel. Conf. on Emerging Technologies and Factory Automation
    (ETFA),</em>
    
    
      2010.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/5641280/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf10beyond.png"></a>
  
  
  <span class="abstract">
    A SLAM algorithm inspired by biological principles
has been recently proposed and shown to perform well
in a large and demanding scenario. We analyse and compare
this system (RatSLAM) and the established Bayesian
SLAM methods and identify the key difference to be an additive
update step. Using this insight, we derive a novel
filter scheme and successfully show that it can entirely replace
the core of the RatSLAM system while maintaining
its desirable robustness. This leads to a massive speedup,
as the novel filter can be calculated very efficiently.
We successfully applied the new algorithm to the same 66
km long dataset that was used with the original algorithm.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/5641280/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf10c">
  
    
    <a href="/assets/papers/suenderhauf10efficient.pdf" class="title" target="_blank">From Neurons to Robots: Towards Efficient Biologically Inspired
    Filtering and SLAM</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of KI 2010: Advances in Artificial Intelligence,</em>
    
    
      2010.
    
    </span>
  

  

  
  <a href="/assets/papers/suenderhauf10efficient.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf10efficient.png"></a>
  
  
  <span class="abstract">
    We discuss recently published models of neural information process-
ing under uncertainty and a SLAM system that was inspired by the neural struc-
tures underlying mammalian spatial navigation. We summarize the derivation of
a novel filter scheme that captures the important ideas of the biologically inspired
SLAM approach, but implements them on a higher level of abstraction. This leads
to a new and more efficient approach to biologically inspired filtering which we
successfully applied to real world urban SLAM challenge of 66 km length.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/suenderhauf10efficient.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2009</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Lange09">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/5174709/" class="title" target="_blank">A Vision Based Onboard Approach for Landing and Position Control
    of an Autonomous Multirotor UAV in GPS-Denied Environments</a>
    
    <span class="author">
      
        
          
            
              Sven Lange,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Intl. Conf. on Advanced Robotics (ICAR),</em>
    
    
      2009.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/5174709/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/lange09landing.png"></a>
  
  
  <span class="abstract">
    We describe our work on multirotor UAVs and
focus on our method for autonomous landing and position
control. The paper describes the design of our landing pad
and the vision based detection algorithm that estimates the 3Dposition
of the UAV relative to the landing pad. A cascaded
controller structure stabilizes velocity and position in the
absence of GPS signals by using a dedicated optical flow sensor.
Practical experiments prove the quality of our approach.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/5174709/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf09">
  
    
    <span class="title">Using Image Profiles and Integral Images for Efficient Calculation
    of Sparse Optical Flow Fields.</span>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Advanced Robotics,</em>
    
    
      2009.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<h3 class="year">2008</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography"><li>

<div id="Lange08">
  
    
    <a href="/assets/papers/lange08landing.pdf" class="title" target="_blank">Autonomous Landing for a Multirotor UAV Using Vision</a>
    
    <span class="author">
      
        
          
            
              Sven Lange,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop Proc. of SIMPAR 2008 Intl. Conf. on Simulation, Modeling
    and Programming for Autonomous Robots,</em>
    
    
      2008.
    
    </span>
  

  

  
  <a href="/assets/papers/lange08landing.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/lange08landing.png"></a>
  
  
  <span class="abstract">
    We describe our work on multirotor UAVs and focus on our
method for autonomous landing. The paper describes the design of our
landing pad and its advantages. We explain how the landing pad detection
algorithm works and how the 3D-position of the UAV relative to
the landing pad is calculated. Practical experiments prove the quality of
these estimations.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="/assets/papers/lange08landing.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2007</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf07">
  
    
    <a href="https://ieeexplore.ieee.org/abstract/document/4381265/" class="title" target="_blank">Using the Unscented Kalman Filter in Mono-SLAM with Inverse Depth
    Parametrization for Autonomous Airship Control</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Sven Lange,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Workshop on Safety Security and Rescue
    Robotics (SSRR),</em>
    
    
      2007.
    
    </span>
  

  

  
  <a href="https://ieeexplore.ieee.org/abstract/document/4381265/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf07.png"></a>
  
  
  <span class="abstract">
    In this paper, we present an approach for aiding control of an autonomous airship by the means of SLAM. We show how the Unscented Kalman Filter can be applied in a SLAM context with monocular vision. The recently published Inverse Depth Parametrization is used for undelayed single-hypothesis landmark initialization and modelling. The novelty of the presented approach lies in the combination of UKF, Inverse Depth Parametrization and bearing-only SLAM and its application for autonomous airship control and UAV control in general.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://ieeexplore.ieee.org/abstract/document/4381265/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Neubert07">
  
    
    <span class="title">FastSLAM using SURF Features: An Efficient Implementation and Practical
    Experiences</span>
    
    <span class="author">
      
        
          
            
              Peer Neubert,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Intelligent and Autonomous
    Vehicles, IAV07,</em>
    
    
      2007.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Bauer07">
  
    
    <span class="title">Comparing several implementations of two recently published feature
    detectors</span>
    
    <span class="author">
      
        
          
            
              Johannes Bauer,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Intelligent and Autonomous
    Vehicles, IAV07,</em>
    
    
      2007.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Misc</strong></p>
<ol class="bibliography"><li>

<div id="Suenderhauf07a">
  
    
    <span class="title">Stereo Odometry – A Review of Approaches</span>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
    
      2007.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2006</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf06a">
  
    
    <a href="/assets/papers/icra06-roboking.pdf" class="title" target="_blank">Bringing Robotics Closer to Students – A Threefold Approach</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              T. Krause,
            
          
        
      
        

          
            
              P. Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Intl. Conf. on Robotics and Automation (ICRA),</em>
    
    
      2006.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="/assets/papers/icra06-roboking.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Krueger06">
  
    
    <span class="title">Using and Extending the Miro Middleware for Autonomous Mobile Robots</span>
    
    <span class="author">
      
        
          
            
              Daniel Krüger,
            
          
        
      
        
          
            
              Ingo Lil,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Robert Baumgartl,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of Towards Autonomous Robotic Systems (TAROS06),</em>
    
    
      2006.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf06">
  
    
    <span class="title">Towards Using Bundle Adjustment for Robust Stereo Odometry in Outdoor
    Terrain</span>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of Towards Autonomous Robotic Systems (TAROS06),</em>
    
    
      2006.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Misc</strong></p>
<ol class="bibliography"><li>

<div id="Suenderhauf06b">
  
    
    <span class="title">Stereo Odometry on an Autonomous Mobile Robot in Outdoor Terrain
    (Diplomarbeit)</span>
    
    <span class="author">
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
    
      2006.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<h3 class="year">2005</h3>

<p><strong>Conference Publications</strong></p>
<ol class="bibliography">
<li>

<div id="Suenderhauf05a">
  
    
    <a href="/assets/papers/icra05-roboking.pdf" class="title" target="_blank">RoboKing - Bringing Robotics Closer to Pupils</a>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Thomas Krause,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Intl. Conf. on Robotics and Automation (ICRA),</em>
    
    
      2005.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!-- 
    [<a href="/assets/papers/icra05-roboking.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
<li>

<div id="Suenderhauf05">
  
    
    <span class="title">Visual Odometry using Sparse Bundle Adjustment on an Autonomous
    Outdoor Vehicle</span>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Kurt Konolige,
            
          
        
      
        
          
            
              Simon Lacroix,
            
          
        
      
        

          
            
              Peter Protzel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Tagungsband Autonome Mobile Systeme 2005,</em>
    
    
      2005.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li>
</ol>

<p><strong>Workshop Publications</strong></p>
<ol class="bibliography"><li>

<div id="Suenderhauf05b">
  
    
    <span class="title">Comparison of Stereovision Odometry Approaches.</span>
    
    <span class="author">
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Kurt Konolige,
            
          
        
      
        
          
            
              Thomas Lemaire,
            
          
        
      
        

          
            
              Simon Lacroix.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of IEEE International Conference on Robotics and Automation,
    Planetary Rover Workshop,</em>
    
    
      2005.
    
    </span>
  

  

  
  

  <span class="links">
  <!--  -->
  
  
  <!--  -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>


  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    © Copyright 2025 Niko Sünderhauf.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://nikosuenderhauf.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://nikosuenderhauf.github.io/assets/js/katex.js"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>






<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-135749210-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
