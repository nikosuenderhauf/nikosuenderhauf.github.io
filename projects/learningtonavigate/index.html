<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niko Sünderhauf | Robot Learning</title>
  <meta name="description" content="">

  <link rel="shortcut icon" href="https://nikosuenderhauf.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/main.css">
  <link rel="canonical" href="https://nikosuenderhauf.github.io/projects/learningtonavigate/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Niko</strong> Sünderhauf
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://nikosuenderhauf.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/cv/">bio</a>
          
        
          
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/projects/">research</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/jobs/">recruiting</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/workshops/">workshops</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Robot Learning</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Robot Learning clearfix">
    <p><img class="col three" src="/assets/img/projects/robot_learning_showcase.png" /></p>

<p>How can robots learn everyday tasks such as tidying up an apartment or assisting humans in their everyday domestic chores?</p>

<p>With a fantastic team, I am interested in robotic learning for Manipulation, Complex Task Planning, and Navigation.</p>

<p>We combine modern techniques such as imitation learning, LLMs, Vision-Language Models, and reinforcement learning with classical control and navigation, but also with a fresh take on <a href="../sceneunderstanding">representing the environment with NeRFs or Gaussian Splatting</a>.</p>

<h3 id="join-us-for-your-phd-or-postdoc">Join us for your PhD or Postdoc!</h3>
<p>I am hiring a Postdoc in Robot Learning starting immediately (from September 2025). Please <a href="../../jobs/postdoc_2025">check here</a> if you are interested.</p>

<p>If you want to do a PhD with us in this area, read more information <a href="../../jobs">here</a>.</p>

<h3 id="publications">Publications</h3>

<ol class="bibliography"><li>

<div id="rana2025policy">
  
    
    <a href="https://arxiv.org/abs/2410.12124" class="title" target="_blank">Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            
              Robert Lee,
            
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL),</em>
    
    
      2025.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2410.12124" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2024policy.png" /></a>
  
  
  <span class="abstract">
    We introduce oriented affordance frames, a structured representation for state and action spaces that improves spatial and intra-category generalisation and enables policies to be learned efficiently from only 10 demonstrations. More importantly, we show how this abstraction allows for compositional generalisation of independently trained sub-policies to solve long-horizon, multi-object tasks. To seamlessly transition between sub-policies, we introduce the notion of self-progress prediction, which we directly derive from the duration of the training demonstrations. We validate our method across three real-world tasks, each requiring multi-step, multi-object interactions. Despite the small dataset, our policies generalise robustly to unseen object appearances, geometries, and spatial arrangements, achieving high success rates without reliance on exhaustive training data.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2410.12124" target="_blank">arXiv</a>]
  
  
    [<a href="https://affordance-policy.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2410.12124" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="abou-chakra2025real">
  
    
    <a href="https://arxiv.org/abs/2504.03597" class="title" target="_blank">Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation</a>
    
    <span class="author">
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Lingfeng Sun,
            
          
        
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Brandon May,
            
          
        
      
        
          
            
              Karl Schmeckpeper,
            
          
        
      
        
          
            Niko Suenderhauf,
          
        
      
        
          
            
              Maria Vittoria Minniti,
            
          
        
      
        

          
            
              Laura Herlant.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2504.03597,</em>
    
    
      2025.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2504.03597" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/abou-chakra2025real.png" /></a>
  
  
  <span class="abstract">
    We introduce real-is-sim, a new approach to integrating simulation into behavior cloning pipelines. In contrast to real-only methods, which lack the ability to safely test policies before deployment, and sim-to-real methods, which require complex adaptation to cross the sim-to-real gap, our framework allows policies to seamlessly switch between running on real hardware and running in parallelized virtual environments. At the center of real-is-sim is a dynamic digital twin, powered by the Embodied Gaussian simulator, that synchronizes with the real world at 60Hz. This twin acts as a mediator between the behavior cloning policy and the real robot. Policies are trained using representations derived from simulator states and always act on the simulated robot, never the real one. During deployment, the real robot simply follows the simulated robot’s joint states, and the simulation is continuously corrected with real world measurements. This setup, where the simulator drives all policy execution and maintains real-time synchronization with the physical world, shifts the responsibility of crossing the sim-to-real gap to the digital twin’s synchronization mechanisms, instead of the policy itself.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2504.03597" target="_blank">arXiv</a>]
  
  
    [<a href="https://real-is-sim.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2504.03597" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rana2025imle">
  
    
    <a href="https://arxiv.org/abs/2502.12371" class="title" target="_blank">IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Robert Lee,
            
          
        
      
        
          
            
              David Pershouse,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Robotics: Science and Systems (RSS),</em>
    
    
      2025.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2502.12371" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2025imle.png" /></a>
  
  
  <span class="abstract">
    We introduce IMLE Policy, a novel behaviour cloning approach based on Implicit Maximum Likelihood Estimation (IMLE). IMLE Policy excels in low-data regimes, effectively learning from minimal demonstrations and requiring 38% less data on average to match the performance of baseline methods in learning complex multi-modal behaviours. Its simple generator-based architecture enables single-step action generation, improving inference speed by 97.3% compared to Diffusion Policy, while outperforming single-step Flow Matching.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2502.12371" target="_blank">arXiv</a>]
  
  
    [<a href="https://imle-policy.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2502.12371" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="abou-chakra2024physically">
  
    
    <a href="https://openreview.net/forum?id=AEq0onGrN2" class="title" target="_blank">Physically Embodied Gaussian Splatting: A Realtime Correctable World Model for Robotics</a>
    
    <span class="author">
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Feras Dayoub,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL),</em>
    
    
      2024.
    
    </span>
  

  
  <span class="award">Oral Presentation</span>
  

  
  <a href="https://openreview.net/forum?id=AEq0onGrN2" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/abou-chakra2024physically.png" /></a>
  
  
  <span class="abstract">
    We propose a novel dual "Gaussian-Particle" representation that models the physical world while (i) enabling predictive simulation of future states and (ii) allowing online correction from visual observations in a dynamic world. Our representation comprises particles that capture the geometrical aspect of objects in the world and can be used alongside a particle-based physics system to anticipate physically plausible future states. Attached to these particles are 3D Gaussians that render images from any viewpoint through a splatting process thus capturing the visual state. By comparing the predicted and observed images, our approach generates "visual forces" that correct the particle positions while respecting known physical constraints. By integrating predictive physical modeling with continuous visually-derived corrections, our unified representation reasons about the present and future while synchronizing with reality.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2406.10788" target="_blank">arXiv</a>]
  
  
    [<a href="https://embodied-gaussians.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://openreview.net/forum?id=AEq0onGrN2" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="padalkar2023open">
  
    
    <a href="https://robotics-transformer-x.github.io/" class="title" target="_blank">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</a>
    
    <span class="author">
      
        

          
            
              Open X-Embodiment Collaboration.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Conference on Robotics and Automation (ICRA),</em>
    
    
      2024.
    
    </span>
  

  
  <span class="award">ICRA Best Conference Paper. ICRA Best Paper Award in Robot Manipulation.</span>
  

  
  <a href="https://robotics-transformer-x.github.io/" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/openx2024.png" /></a>
  
  
  <span class="abstract">
    Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2310.08864" target="_blank">arXiv</a>]
  
  
    [<a href="https://robotics-transformer-x.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://robotics-transformer-x.github.io/" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rana2023sayplan">
  
    
    <a href="https://openreview.net/pdf?id=wMpOMO0Ss7a" class="title" target="_blank">Sayplan: Grounding Large Language Models Using 3d Scene Graphs for Scalable Robot Task Planning</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Jesse Haviland,
            
          
        
      
        
          
            
              Sourav Garg,
            
          
        
      
        
          
            
              Jad Abou-Chakra,
            
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL),</em>
    
    
      2023.
    
    </span>
  

  
  <span class="award">Oral Presentation</span>
  

  
  <a href="https://openreview.net/pdf?id=wMpOMO0Ss7a" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2023sayplan.png" /></a>
  
  
  <span class="abstract">
    We introduce SayPlan, a scalable approach to LLM-based,
large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical
nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant
subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the
planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback
from a scene graph simulator, correcting infeasible actions and avoiding planning
failures. We evaluate our approach on two large-scale environments spanning up
to 3 floors and 36 rooms with 140 assets and objects and show that our approach is
capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2307.06135" target="_blank">arXiv</a>]
  
  
    [<a href="https://sayplan.github.io/" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://openreview.net/pdf?id=wMpOMO0Ss7a" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="ceola2023lhmanip">
  
    
    <a href="https://arxiv.org/abs/2312.12036" class="title" target="_blank">LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks in Cluttered Tabletop Environments</a>
    
    <span class="author">
      
        
          
            
              Federico Ceola,
            
          
        
      
        
          
            
              Lorenzo Natale,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Krishan Rana.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2312.12036,</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2312.12036" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/ceola2023manip.png" /></a>
  
  
  <span class="abstract">
    We present the Long-Horizon Manipulation (LHManip) dataset comprising 200
episodes, demonstrating 20 different manipulation tasks via real robot teleoperation. The tasks entail multiple sub-tasks,
including grasping, pushing, stacking and throwing objects in highly cluttered environments. Each task is paired with a
natural language instruction and multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the dataset
comprises 176,278 observation-action pairs which form part of the Open X-Embodiment dataset.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2312.12036" target="_blank">arXiv</a>]
  
  
    [<a href="https://github.com/fedeceola/LHManip" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2312.12036" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rana2023contrastive">
  
    
    <a href="https://openreview.net/forum?id=sxKR6zhBDH" class="title" target="_blank">Contrastive Language, Action, and State Pre-training for Robot Learning</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Andrew Melnik,
            
          
        
      
        

          
            
              Niko Suenderhauf.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In ICRA Workshop on Pretraining for Robotics (PT4R),</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://openreview.net/forum?id=sxKR6zhBDH" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2023contrastive.png" /></a>
  
  
  <span class="abstract">
    We introduce a method for unifying language, action, and state information in a shared embedding space to facilitate a range of downstream tasks in robot learning.  Our method, Contrastive Language, Action, and State Pre-training (CLASP), extends the CLIP formulation by incorporating distributional learning, capturing the inherent complexities and one-to-many relationships in behaviour-text alignment. By employing distributional outputs for both text and behaviour encoders, our model effectively associates diverse textual commands with a single behaviour and vice-versa. We demonstrate the utility of our method for the following downstream tasks: zero-shot text-behaviour retrieval, captioning unseen robot behaviours, and learning a behaviour prior for language-conditioned reinforcement learning.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2304.10782" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://openreview.net/forum?id=sxKR6zhBDH" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rana2021BCF">
  
    
    <a href="https://arxiv.org/abs/2107.09822" class="title" target="_blank">Bayesian Controller Fusion: Leveraging Control Priors in Deep Reinforcement Learning for Robotics</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Vibhavari Dasagi,
            
          
        
      
        
          
            
              Jesse Haviland,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>The International Journal of Robotics Research (IJRR),</em>
    
    
      2023.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2107.09822" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana21bcf.png" /></a>
  
  
  <span class="abstract">
    We present Bayesian Controller Fusion (BCF): a hybrid control strategy that combines the strengths of traditional hand-crafted controllers and model-free deep reinforcement learning (RL). BCF thrives in the robotics domain, where reliable but suboptimal control priors exist for many tasks, but RL from scratch remains unsafe and data-inefficient. By fusing uncertainty-aware distributional outputs from each system, BCF arbitrates control between them, exploiting their respective strengths. 
    As exploration is naturally guided by the prior in the early stages of training, BCF accelerates learning, while substantially improving beyond the performance of the control prior, as the policy gains more experience. 
    More importantly, given the risk-aversity of the control prior, BCF ensures safe exploration <i>and</i> deployment, where the control prior naturally dominates the action distribution in states unknown to the policy. 
    We additionally show BCF’s applicability to the zero-shot sim-to-real setting and its ability to deal with out-of-distribution states in the real-world. 
    BCF is a promising approach for combining the complementary strengths of deep RL and traditional robotic control, surpassing what either can achieve independently.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2107.09822" target="_blank">arXiv</a>]
  
  
    [<a href="https://krishanrana.github.io/bcf" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2107.09822" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rana2022reskill">
  
    
    <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" class="title" target="_blank">Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Ming Xu,
            
          
        
      
        
          
            
              Brendan Tidd,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL),</em>
    
    
      2022.
    
    </span>
  

  

  
  <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2022reskill.png" /></a>
  
  
  <span class="abstract">
    Skill-based reinforcement learning (RL) has emerged as a promising strategy to
leverage prior knowledge for accelerated robot learning. We firstly
propose accelerating exploration in the skill space using state-conditioned generative models to directly bias the high-level agent towards only sampling skills
relevant to a given state based on prior experience. Next, we propose a low-level
residual policy for fine-grained skill adaptation enabling downstream RL agents
to adapt to unseen task variations. Finally, we validate our approach across four
challenging manipulation tasks that differ from those used to build the skill space,
demonstrating our ability to learn across task variations while significantly accelerating exploration, outperforming prior works.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2211.02231" target="_blank">arXiv</a>]
  
  
    [<a href="https://krishanrana.github.io/reskill" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rana2021zero">
  
    
    <a href="https://arxiv.org/abs/2112.05299" class="title" target="_blank">Zero-Shot Uncertainty-Aware Deployment of Simulation Trained Policies on Real-World Robots</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Vibhavari Dasagi,
            
          
        
      
        
          
            
              Jesse Haviland,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In NeuIPS Workshop on Deployable Decision Makig in Embodied Systems,</em>
    
    
      2021.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2112.05299" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2021zero.png" /></a>
  
  
  <span class="abstract">
    While deep reinforcement learning (RL) agents have demonstrated incredible potential in attaining dexterous behaviours for robotics, they tend to make errors when deployed in the real world due to mismatches between the training and execution environments. In contrast, the classical robotics community have developed a range of controllers that can safely operate across most states in the real world given their explicit derivation. These controllers however lack the dexterity required for complex tasks given limitations in analytical modelling and approximations. In this paper, we propose Bayesian Controller Fusion (BCF), a novel uncertainty-aware deployment strategy that combines the strengths of deep RL policies and traditional handcrafted controllers.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2112.05299" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/2112.05299" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="rana2020multiplicative">
  
    
    <a href="https://arxiv.org/abs/2003.05117" class="title" target="_blank">Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Vibhavari Dasagi,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS),</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/2003.05117" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana2020multiplicative.png" /></a>
  
  
  <span class="abstract">
    Learning long-horizon tasks on real robot hardware can be intractable, and transferring a learned policy from simulation to reality is still extremely challenging. We present a novel approach to model-free reinforcement learning that can leverage existing sub-optimal solutions as an algorithmic prior during training and deployment. During training, our gated fusion approach enables the prior to guide the initial stages of exploration, increasing sample-efficiency and enabling learning from sparse long-horizon reward signals. Importantly, the policy can learn to improve beyond the performance of the sub-optimal prior since the prior’s influence is annealed gradually. During deployment, the policy’s uncertainty provides a reliable strategy for transferring a simulation-trained policy to the real world by falling back to the prior controller in uncertain states. We show the efficacy of our Multiplicative Controller Fusion approach on the task of robot navigation and demonstrate safe transfer from simulation to the real world without any fine tuning. 
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2003.05117" target="_blank">arXiv</a>]
  
  
    [<a href="https://sites.google.com/view/mcf-nav/home" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/2003.05117" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/X99cFN2UyTI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</center>
<ol class="bibliography"><li>

<div id="rana19navigation">
  
    
    <a href="https://arxiv.org/abs/1909.10972" class="title" target="_blank">Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments</a>
    
    <span class="author">
      
        
          
            
              Krishan Rana,
            
          
        
      
        
          
            
              Ben Talbot,
            
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of IEEE International Conference on Robotics and Automation (ICRA),</em>
    
    
      2020.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1909.10972" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/rana19navigation.png" /></a>
  
  
  <span class="abstract">
    In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRNN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1909.10972" target="_blank">arXiv</a>]
  
  
    [<a href="https://github.com/krishanrana/2D_SRRN" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/1909.10972" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/uhHzbGVPYj4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></center>
<ol class="bibliography"><li>

<div id="suenderhauf19keys">
  
    
    <a href="https://arxiv.org/abs/1909.07376" class="title" target="_blank">Where are the Keys? – Learning Object-Centric Navigation Policies on Semantic Maps with Graph Convolutional Networks</a>
    
    <span class="author">
      
        

          
            Niko Sünderhauf.
        
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:1909.07376,</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1909.07376" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/suenderhauf19keys.png" /></a>
  
  
  <span class="abstract">
    Emerging object-based SLAM algorithms can build a graph representation of an environment comprising nodes for robot poses and object landmarks. However, while this map will contain static objects such as furniture or appliances, many moveable objects (e.g. the car keys, the glasses, or a magazine), are not suitable as landmarks and will not be part of the map due to their non-static nature. We show that Graph Convolutional Networks can learn navigation policies to find such unmapped objects by learning to exploit the hidden probabilistic model that governs where these objects appear in the environment. The learned policies can generalise to object classes unseen during training by using word vectors that express semantic similarity as representations for object nodes in the graph. Furthermore, we show that the policies generalise to unseen environments with only minimal loss of performance. We demonstrate that pre-training the policy network with a proxy task can significantly speed up learning, improving sample efficiency.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1909.07376" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1909.07376" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Dasagi19transfer">
  
    
    <a href="https://arxiv.org/abs/1809.07480" class="title" target="_blank">Sim-to-Real Transfer of Robot Learning with Variable Length Inputs</a>
    
    <span class="author">
      
        
          
            
              Vibhavari Dasagi,
            
          
        
      
        
          
            
              Robert Lee,
            
          
        
      
        
          
            
              Serena Mou,
            
          
        
      
        
          
            
              Jake Bruce,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        

          
            
              Jürgen Leitner.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Australasian Conf. for Robotics and Automation (ACRA),</em>
    
    
      2019.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1809.07480" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/lee18zeroshot.png" /></a>
  
  
  <span class="abstract">
    Current end-to-end deep Reinforcement Learning (RL) approaches require jointly learning perception, decision-making and low-level control from very sparse reward signals and high-dimensional inputs, with little capability of incorporating prior knowledge. In this work, we propose a framework that combines deep sets encoding, which allows for variable-length abstract representations, with modular RL that utilizes these representations, decoupling high-level decision making from low-level control. We successfully demonstrate our approach on the robot manipulation task of object sorting, showing that this method can learn effective policies within mere minutes of highly simplified simulation. The learned policies can be directly deployed on a robot without further training, and generalize to variations of the task unseen during training.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1809.07480" target="_blank">arXiv</a>]
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1809.07480" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Bruce18navigation">
  
    
    <a href="https://arxiv.org/abs/1807.05211" class="title" target="_blank">Learning Deployable Navigation Policies at Kilometer Scale from a Single Traversal</a>
    
    <span class="author">
      
        
          
            
              Jake Bruce,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Piotr Mirowski,
            
          
        
      
        
          
            
              Raia Hadsell,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of Conference on Robot Learning (CoRL),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1807.05211" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/bruce18navigation.png" /></a>
  
  
  <span class="abstract">
    We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multiple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1807.05211" target="_blank">arXiv</a>]
  
  
    [<a href="http://rl-navigation.github.io/deployable" target="_blank">website</a>]
  
  <!-- 
    [<a href="https://arxiv.org/abs/1807.05211" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Bruce17">
  
    
    <a href="https://arxiv.org/abs/1711.10137" class="title" target="_blank">One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay</a>
    
    <span class="author">
      
        
          
            
              Jacob Bruce,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Piotr Mirowski,
            
          
        
      
        
          
            
              Raia Hadsell,
            
          
        
      
        

          
            
              Michael Milford.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. of NIPS Workshop on Acting and Interacting in the Real World: Challenges in Robot Learning,</em>
    
    
      2017.
    
    </span>
  

  

  
  <a href="https://arxiv.org/abs/1711.10137" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/bruce17navigation.png" /></a>
  
  
  <span class="abstract">
    Recently, model-free reinforcement learning algorithms have been shown to solve challenging problems by learning from extensive interaction with the environment. A significant issue with transferring this success to the robotics domain is that interaction with the real world is costly, but training on limited experience is prone to overfitting. We present a method for learning to navigate, to a fixed goal and in a known environment, on a mobile robot. The robot leverages an interactive world model built from a single traversal of the environment, a pre-trained visual feature encoder, and stochastic environmental augmentation, to demonstrate successful zero-shot transfer under real-world environmental variations without fine-tuning.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="https://arxiv.org/abs/1711.10137" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="anderson2017vision">
  
    
    <a href="http://arxiv.org/abs/1711.07280" class="title" target="_blank">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</a>
    
    <span class="author">
      
        
          
            
              Peter Anderson,
            
          
        
      
        
          
            
              Qi Wu,
            
          
        
      
        
          
            
              Damien Teney,
            
          
        
      
        
          
            
              Jake Bruce,
            
          
        
      
        
          
            
              Mark Johnson,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Ian Reid,
            
          
        
      
        
          
            
              Stephen Gould,
            
          
        
      
        

          
            
              Anton van den Hengel.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Computer Vision and Pattern Recognition (CVPR),</em>
    
    
      2018.
    
    </span>
  

  

  
  <a href="http://arxiv.org/abs/1711.07280" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/anderson18navigation.png" /></a>
  
  
  <span class="abstract">
     To enable and encourage the application of vision and
language methods to the problem of interpreting visually grounded
navigation instructions, we present the Matterport3D
Simulator – a large-scale reinforcement learning
environment based on real imagery. Using this simulator,
which can in future support a range of embodied vision
and language tasks, we provide the first benchmark dataset
for visually-grounded natural language navigation in real
buildings – the Room-to-Room (R2R) dataset.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://arxiv.org/abs/1711.07280" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>
<ol class="bibliography"><li>

<div id="Sergeant15">
  
    
    <a href="http://www.araa.asn.au/acra/acra2015/papers/pap149.pdf" class="title" target="_blank">Multimodal Deep Autoencoders for Control of a Mobile Robot</a>
    
    <span class="author">
      
        
          
            
              James Sergeant,
            
          
        
      
        
          
            Niko Sünderhauf,
          
        
      
        
          
            
              Michael Milford,
            
          
        
      
        

          
            
              Ben Upcroft.
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Australasian Conference on Robotics and Automation (ACRA),</em>
    
    
      2015.
    
    </span>
  

  

  
  <a href="http://www.araa.asn.au/acra/acra2015/papers/pap149.pdf" target="_blank"><img class="paper_thumb" src="/assets/paper-thumbnails/sergeant15.png" /></a>
  
  
  <span class="abstract">
    Robot navigation systems are typically engineered
to suit certain platforms, sensing suites
and environment types. In order to deploy a
robot in an environment where its existing navigation
system is insufficient, the system must
be modified manually, often at significant cost.
In this paper we address this problem, proposing
a system based on multimodal deep autoencoders
that enables a robot to learn how
to navigate by observing a dataset of sensor input
and motor commands collected while being
teleoperated by a human. Low-level features
and cross modal correlations are learned and
used in initialising two different architectures
with three operating modes. During operation,
these systems exploit the learned correlations
in generating suitable control signals based only
on the sensor information.
  </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  <!-- 
    [<a href="http://www.araa.asn.au/acra/acra2015/papers/pap149.pdf" target="_blank">PDF</a>]
   -->
  <!--  -->
  
  
  
  
  </span>



  <!-- Hidden abstract block -->

</div>
</li></ol>

<!-- ### Sim-to-Real Transfer -->

  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2025 Niko Sünderhauf.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://nikosuenderhauf.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://nikosuenderhauf.github.io/assets/js/katex.js"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>






<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-135749210-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
