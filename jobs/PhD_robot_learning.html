<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niko Sünderhauf | PhD Topic – Robotic Learning for Complex Everyday Tasks</title>
  <meta name="description" content="">

  <link rel="shortcut icon" href="https://nikosuenderhauf.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/main.css">
  <link rel="canonical" href="https://nikosuenderhauf.github.io/jobs/PhD_robot_learning">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Niko</strong> Sünderhauf
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://nikosuenderhauf.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/cv/">bio</a>
          
        
          
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/projects/">research</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/jobs/">recruiting</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/workshops/">workshops</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">PhD Topic – Robotic Learning for Complex Everyday Tasks</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content PhD Topic – Robotic Learning for Complex Everyday Tasks clearfix">
    <p><strong>How can robots learn everyday tasks such as tidying up an apartment or assisting humans in their everyday domestic chores?</strong></p>

<p><img class="col three" src="/assets/img/jobs/robot_learning_showcase.png" /></p>

<p>I have multiple PhD projects available in the area of robotic learning, combining modern techniques such as imitation learning, LLMs, Vision-Language Models, and reinforcement learning with classical control and navigation, but also with a fresh take on representing the environment with NeRFs or Gaussian Splatting.</p>

<p>These PhD projects build on our previous work such as <a href="https://sayplan.github.io/">SayPlan</a> (oral presentation at CoRL 2023) or <a href="https://sites.google.com/view/particlenerf">ParticleNeRF</a> (best paper honourable mention at WACV 2024).</p>

<h4 id="apply-now-for-your-phd-with-me-in-2025">Apply now for your PhD with me in 2025!</h4>
<p>To work with me on one of the topics below, you have to apply for a scholarship through <a href="https://www.qut.edu.au/research/study-with-us/how-to-apply">QUT’s scholarship round</a>. 
If you are successful in obtaining a scholarship, you can start your PhD between January and June 2025.</p>

<p><strong>Application deadlines</strong></p>
<ul>
  <li>for international (i.e. non-Australian) students: 31 July 2024</li>
  <li>for domestic (i.e. Australian) students: 31 August 2024</li>
</ul>

<h3 id="research-questions-and-topics">Research Questions and Topics</h3>
<p>Research questions for the available topics include:</p>

<ul>
  <li>How can robots learn tasks by observing humans, for example from a single demonstration or from the youtube videos? How can the kinematic chain of a human be mapped to the kinematics of a robot in the process?</li>
  <li>How can Large Language Models or Vision Language Models be used effectively to guide robot navigation and interaction with the world?</li>
  <li>What is the best way for robots to learn a library of skills that generalise and transfer to unseen objects?</li>
  <li>How can LLMs be used to compose such base skills into more and more complex tasks?</li>
  <li>How can an environment be represented to ease robot learning? Are modern mapping techniques based on NeRFs or Gaussian Splatting helpful?</li>
  <li>Can ad-hoc simulations of the environment be used as thought-experiments to guide interaction and manipulation of complex scenes, e.g. in clutter or manipulating mechanisms?</li>
  <li>How can robots improvise when they don’t succeed at first? How can they recover from failures and mistakes, and learn from it to do better next time?</li>
</ul>

<p>To answer these questions, you will work at the forefront of robotic learning and AI and become an expert in exciting research areas such as LLMs, VLMs, imitation learning, reinforcement learning, NeRFs, Gaussian Splatting, and physics simulations.</p>

<p>You would work with our amazing fleet of robots—both stationary and mobile—and our instrumented experimentation setup, which lets us train and evaluate robots for various chores. You’ll be part of an amazing team of researchers, postdocs and other PhD students at the Centre for Robotics. You will also become fluent in PyTorch, one of the most versatile and widely-used deep learning frameworks in industry.</p>

<!-- In this PhD project, you investigate different strategies for robots to learn everyday tasks. 

You apply techniques such as reinforcement learning and imitation learning in combination with large language models or vision-language models and investigate how to incorporate common sense knowledge, written task descriptions, human demonstrations and 3D scene understanding into the robotic learning process. -->

<p><!-- You get the chance to work with a mobile robot that is equipped with a versatile arm, and set up impressive demos and experiments. --></p>

<!-- This project is supported by funding from the Australian Research Council and is done in collaboration between QUT and the University of Adelaide. You will be embedded in a team that spans both universities and will have the opportunity to travel and be co-supervised by experts from both unis. -->

<p>Have a look at <a href="../projects/learningtonavigate">similar research</a> I have been doing with my students, postdocs and colleagues.</p>

<h3 id="join-the-team">Join the Team!</h3>
<p>As a PhD student, you would work closely with my wonderful current postdocs and students, e.g. <a href="https://krishanrana.github.io/">Dr Krishan Rana</a>, <a href="https://jhavl.com/">Dr Jesse Haviland</a> or <a href="https://jadchakra.github.io/">Jad Abou-Chakra</a>. You would of course also be embedded in the vibrant <a href="http://qcr.ai">QUT Centre for Robotics</a> and get a chance to collaborate with any of the 100+ researchers in our group.</p>

<h3 id="about-the-position">About the Position</h3>
<p><img class="col one" src="/assets/img/jobs/campus-1.jpg" />
I am looking for creative and enthusiastic PhD researchers to contribute to the Visual Learning and Understanding research program. You will be a member of the QUT Centre for Robotics and will work closely with academics, research fellows, engineers, and other PhD students.</p>

<p><strong>You should be excited to do pioneering research in Robotic Learning and Robotic Scene Understanding</strong></p>

<ul>
  <li>Scholarship: $33,637 AUD per year, tax free</li>
  <li>$5,000 top-up scholarship available</li>
  <li>multiple positions available</li>
  <li>start date: immediately and ongoing</li>
</ul>

<h3 id="how-to-apply">How to Apply</h3>

<p>First, you should <a href="https://www.qut.edu.au/research/study-with-us/how-to-apply">check that you are eligible</a> to do a PhD at QUT. Then, <a href="https://www.qut.edu.au/about/our-people/academic-profiles/niko.suenderhauf">contact me via email</a> and include:</p>
<ul>
  <li>your CV, including academic transcript</li>
  <li>a description of your research interests, describing the topic you are interested in, and why</li>
  <li>a short summary about a previous research experience you are most proud of</li>
</ul>

<p>I will be in touch if I think you would be a good fit for this project and for our lab.</p>

<h3 id="about-us">About Us</h3>
<p><img class="col one" src="/assets/img/jobs/S11-4.jpg" />
The QUT Centre for Robotics (QCR) conducts at-scale world-leading research in intelligent robotics; translates fundamental research into commercial and societal outcomes; is a leader in education, training and development of talent to meet growing demands for expertise in robotics and autonomous systems; and provides leadership in technological policy development and societal debate. Established in 2020, the Centre has been built on the momentum of a decade’s investment in robotic research and translation at QUT which has been funded by QUT, ARC, Queensland Government, CRCs and Industry. QCR comprises over 100 researchers and engineers.</p>

<p>QCR researchers collaborate with industry and universities around the world, including MIT, Harvard and Oxford universities, Boeing, Thales, DST, Airservices Australia, CASA, JARUS, TRAFI, Google Deepmind, Google AI, Amazon Robotics, Caterpillar, Rheinmetall, US Air Force, and NASA’s Jet Propulsion Laboratory.</p>

<p>We are proud of our beautiful and big modern lab space and research environment. We have a fantastic collection of equipment to support your research, including many mobile robot platforms and robotic arms.</p>

<p>The Centre supports a flexible working environment. We support a diverse and inclusive atmosphere and encourage applications from women, Aboriginal Australians and Torres Strait Islander people.</p>

<div class="img_row">
<img class="col one" src="/assets/img/jobs/S11-1.jpg" />
<img class="col one" src="/assets/img/jobs/S11-3.jpg" />
<img class="col one" src="/assets/img/jobs/S11-2.jpg" />
</div>

<h3 id="location">Location</h3>
<p>Queensland University of Technology (QUT), Gardens Point campus, Brisbane, Australia.
We are located on a beautiful campus next to the Brisbane City Botanic Gardens, just a few minutes on foot from the Brisbane CBD and the bustling Southbank cultural precinct with many fantastic restaurants and bars. QUT has excellent connections to public transport, including our CityCat river ferry, trains, and bus lines. A bike path along the river connects QUT with the nearby suburbs.</p>

<p>Brisbane is a very liveable sub-tropical city of 2.3M people and offers great opportunities for recreational activities ranging from hiking in the many nearby national parks, rock climbing (the Kangaroo Point crag is just across the river, and there are many of well-maintained sport crags in a 1-2 hour radius around Brisbane, as well as a selection of climbing and bouldering gyms in the city), surfing (the famous Gold Coast is just over one hour away), and all things beach and ocean related.</p>

<iframe src="https://www.google.com/maps/embed?pb=!1m14!1m12!1m3!1d2906.049913465745!2d153.02894556592284!3d-27.47748600138352!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!5e1!3m2!1sen!2sau!4v1583472988458!5m2!1sen!2sau" width="600" height="450" frameborder="0" style="border:0;" allowfullscreen=""></iframe>

  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2025 Niko Sünderhauf.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://nikosuenderhauf.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://nikosuenderhauf.github.io/assets/js/katex.js"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>






<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-135749210-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
