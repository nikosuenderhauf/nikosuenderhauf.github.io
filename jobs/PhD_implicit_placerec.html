<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niko Sünderhauf | PhD Topic – Implicit Representations for Place Recognition and Localisation</title>
  <meta name="description" content="">

  <link rel="shortcut icon" href="https://nikosuenderhauf.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/main.css">
  <link rel="canonical" href="https://nikosuenderhauf.github.io/jobs/PhD_implicit_placerec">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Niko</strong> Sünderhauf
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://nikosuenderhauf.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/cv/">bio</a>
          
        
          
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/projects/">research</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/jobs/">recruiting</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/workshops/">workshops</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">PhD Topic – Implicit Representations for Place Recognition and Localisation</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content PhD Topic – Implicit Representations for Place Recognition and Localisation clearfix">
    <p>This project will develop a novel localization pipeline based on implicit map representations.</p>

<p>Unlike traditional approaches that use explicit representations like point clouds or voxel grids, the map in our project is represented implicitly in the weights of neural networks such as Neural Radiance Fields (NeRF).</p>

<p>You will get a chance to develop a new class of localization algorithms that work directly on the implicit representation, bypassing the costly rendering step from implicit to explicit representation. The designed algorithms will address the still difficult problem of accurate large-scale localization under challenging environmental conditions, and in situations where GPS is not reliably available, which is crucial for autonomous and assisted driving.</p>

<p>I am advertising this position together with my colleague <a href="http://www.tobiasfischer.info">Dr Tobias Fischer</a>, who will be a co-supervisor on this project.</p>

<p>Have a look at <a href="../projects/sceneunderstanding">similar research</a> I have been doing with my students, postdocs and colleagues.</p>

<h3 id="about-the-position">About the Position</h3>
<p><img class="col one" src="/assets/img/jobs/campus-1.jpg" />
We are looking for a creative and enthusiastic PhD researcher to contribute to the Visual Learning and Understanding research program. You will be a member of the QUT Centre for Robotics and will work closely with academics, research fellows, engineers, and other PhD students.</p>

<p><strong>You should be excited to do pioneering research in Robotic Learning and Robotic Scene Understanding</strong></p>

<ul>
  <li>Scholarship: $32,500 AUD per year, tax free (via a competitive application process)</li>
  <li>$5,000 top-up scholarship available</li>
  <li>multiple positions available</li>
  <li>start date: immediately and ongoing</li>
</ul>

<h3 id="about-us">About Us</h3>
<p><img class="col one" src="/assets/img/jobs/S11-4.jpg" />
The QUT Centre for Robotics (QCR) conducts at-scale world-leading research in intelligent robotics; translates fundamental research into commercial and societal outcomes; is a leader in education, training and development of talent to meet growing demands for expertise in robotics and autonomous systems; and provides leadership in technological policy development and societal debate. Established in 2020, the Centre has been built on the momentum of a decade’s investment in robotic research and translation at QUT which has been funded by QUT, ARC, Queensland Government, CRCs and Industry. QCR comprises over 100 researchers and engineers.</p>

<p>QCR researchers collaborate with industry and universities around the world, including MIT, Harvard and Oxford universities, Boeing, Thales, DST, Airservices Australia, CASA, JARUS, TRAFI, Google Deepmind, Google AI, Amazon Robotics, Caterpillar, Rheinmetall, US Air Force, and NASA’s Jet Propulsion Laboratory.</p>

<p>We are proud of our beautiful and big modern lab space and research environment. We have a fantastic collection of equipment to support your research, including many mobile robot platforms and robotic arms.</p>

<p>The Centre supports a flexible working environment. We support a diverse and inclusive atmosphere and encourage applications from women, Aboriginal Australians and Torres Strait Islander people.</p>

<div class="img_row">
<img class="col one" src="/assets/img/jobs/S11-1.jpg" />
<img class="col one" src="/assets/img/jobs/S11-3.jpg" />
<img class="col one" src="/assets/img/jobs/S11-2.jpg" />
</div>

<h3 id="location">Location</h3>
<p>Queensland University of Technology (QUT), Gardens Point campus, Brisbane, Australia.
We are located on a beautiful campus next to the Brisbane City Botanic Gardens, just a few minutes on foot from the Brisbane CBD and the bustling Southbank cultural precinct with many fantastic restaurants and bars. QUT has excellent connections to public transport, including our CityCat river ferry, trains, and bus lines. A bike path along the river connects QUT with the nearby suburbs.</p>

<p>Brisbane is a very liveable sub-tropical city of 2.3M people and offers great opportunities for recreational activities ranging from hiking in the many nearby national parks, rock climbing (the Kangaroo Point crag is just across the river, and there are many of well-maintained sport crags in a 1-2 hour radius around Brisbane, as well as a selection of climbing and bouldering gyms in the city), surfing (the famous Gold Coast is just over one hour away), and all things beach and ocean related.</p>

<iframe src="https://www.google.com/maps/embed?pb=!1m14!1m12!1m3!1d2906.049913465745!2d153.02894556592284!3d-27.47748600138352!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!5e1!3m2!1sen!2sau!4v1583472988458!5m2!1sen!2sau" width="600" height="450" frameborder="0" style="border:0;" allowfullscreen=""></iframe>

  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2025 Niko Sünderhauf.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://nikosuenderhauf.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://nikosuenderhauf.github.io/assets/js/katex.js"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>






<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-135749210-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
