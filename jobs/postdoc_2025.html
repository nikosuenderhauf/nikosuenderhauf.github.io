<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niko SÃ¼nderhauf | Postdoctoral Research Fellow</title>
  <meta name="description" content="">

  <link rel="shortcut icon" href="https://nikosuenderhauf.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/main.css">
  <link rel="canonical" href="https://nikosuenderhauf.github.io/jobs/postdoc_2025">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Niko</strong> SÃ¼nderhauf
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://nikosuenderhauf.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/cv/">bio</a>
          
        
          
        
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/projects/">research</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/jobs/">recruiting</a>
          
        
          
            <a class="page-link" href="https://nikosuenderhauf.github.io/workshops/">workshops</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://nikosuenderhauf.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Postdoctoral Research Fellow</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Postdoctoral Research Fellow clearfix">
    <p><img class="col three" src="/assets/img/jobs/Postdoc_in_Robot_Learning.png" /></p>

<h3 id="about-the-position">About the Position</h3>
<p>I am seeking an enthusiastic Postdoctoral Research Fellow to make a decisive contribution to our research in robot learning at <a href="http://www.qcr.ai">QUTâ€™s Centre for Robotics (QCR)</a>. This is a unique opportunity to join one of Australiaâ€™s leading robotics groups and play a key role in a nationally significant research collaboration.</p>

<p>As part of the <a href="http://www.ariamhub.com">ARIAM Research Hub</a>, youâ€™ll work closely with me, our vibrant QCR team of academics, engineers, and PhD students, and expert colleagues from USyd (control) and ANU (planning). Together, we are pushing the boundaries of learning from demonstration, aiming to tackle key challenges in sample efficiency and generalisation.</p>

<p>I am looking to fill this position <strong>as soon as possible</strong> with a start date before or around June 2026. Please check the instructions for applicants and <a href="#how-to-apply">contact me</a> if you are interested.</p>

<h3 id="position-details">Position Details</h3>
<ul>
  <li>Type: Full-time, fixed-term (24 months)</li>
  <li>Level: Academic Level B</li>
  <li>Salary: Starting from $114,000 AUD  with annual increments, + 17% superannuation</li>
  <li>Leave: 20 days annual leave + paid personal/sick leave</li>
  <li>Location: QUT Gardens Point campus, Brisbane, Australia</li>
</ul>

<h3 id="the-research-vision">The Research Vision</h3>
<p>This project is about rethinking how robots learn from humans. Weâ€™re especially interested in:</p>
<ul>
  <li>Learning from very few demonstrations</li>
  <li>Generalising to new tasks, objects, and environments</li>
  <li>Combining imitation learning with insights from control, planning, scene understanding, and LLM-driven reasoning</li>
</ul>

<p>Youâ€™ll have the chance to do pioneering work with many real robots (stationary single and dual-arm setups, as well as mobile manipulators) and shape the future of robot learning in the context of infrastructure robotics and beyond.</p>

<p>Youâ€™ll also gain insights into research group management, including milestone tracking, budget planning, and broader research leadership â€“ ideal preparation for an independent research career.</p>

<p>For a sample of our work in this area, check out our recent papers:
Youâ€™ll continue our recent work in this area, such as:</p>
<ul>
  <li><a href="https://imle-policy.github.io/">IMLE Policy:
Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation</a> â€“ RSS 2025</li>
  <li><a href="https://affordance-policy.github.io/">Learning from 10 Demos:
Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames</a> â€“ CoRL 2025</li>
  <li><a href="https://embodied-gaussians.github.io/">Physically Embodied Gaussian Splatting: A Realtime Correctable World Model for Robotics</a> â€“ CoRL 2024 Oral Presentation</li>
  <li><a href="https://sayplan.github.io/">Sayplan: Grounding Large Language Models Using 3d Scene Graphs for Scalable Robot Task Planning</a> â€“ CoRL 2023 Oral Presentation</li>
</ul>

<h3 id="who-should-apply">Who Should Apply?</h3>
<p>If youâ€™re driven by curiosity and creativity, and excited about building the next generation of robot learning systems, this is for you.
You should have:</p>
<ul>
  <li>A strong background in robot learning â€“ ideally with publications at CoRL, RSS, or similar venues</li>
  <li>Excellent communication skills, both written and verbal</li>
  <li>A clear vision for your research over the next two years</li>
  <li>A collaborative mindset and the ability to work effectively across diverse teams at QUT, ANU, and Uni Sydney</li>
  <li>Solid understanding of the mathematical and statistical foundations of deep learning</li>
  <li>A passion for real-world robotics and a desire to stay at the cutting edge of a fast-moving field</li>
</ul>

<h3 id="key-responsibilities">Key Responsibilities</h3>
<ul>
  <li>Lead and contribute to cutting-edge research in robot learning at QCR and the ARIAM Hub</li>
  <li>Publish at top-tier venues in robotics and machine learning</li>
  <li>Lead or co-organise workshops at major international conferences</li>
  <li>Collaborate with QCRâ€™s researchers, engineers, and students to build reusable robot learning systems</li>
  <li>Translate research into practical demonstrations (e.g. asset management, infrastructure)</li>
  <li>Actively participate in the QCR and ARIAM communities, including events in Brisbane, Sydney, and Canberra</li>
  <li>Co-supervise PhD and undergraduate students</li>
  <li>Contribute to QCRâ€™s Visual Learning and Understanding research program</li>
  <li>Support a collaborative and inclusive research culture in Australia through visits and joint publications</li>
</ul>

<h3 id="selection-criteria">Selection Criteria</h3>
<ul>
  <li>Completion of a PhD in robotics or a related field</li>
  <li>A strong and clear research agenda to advance robot learning</li>
  <li>Demonstrated ability to conduct independent, high-quality research</li>
  <li>Track record of publication of robot learning work in top venues in robotics (especially CoRL and RSS)</li>
  <li>Deep expertise in robotic learning, general robotics, machine learning, and computer vision</li>
  <li>Proficiency in Python, PyTorch, and general software development on Linux</li>
  <li>Experience working with robot hardware: arms, mobile bases, cameras, other sensors</li>
  <li>Excellent written and verbal communication skills</li>
</ul>

<h3 id="how-to-apply">How to Apply</h3>
<p>To apply, please email the following materials to ðŸ“§ niko.suenderhauf@qut.edu.au</p>
<ul>
  <li>A short video introducing yourself, your previous work, and your research vision for the next two years</li>
  <li>Your CV, including a full list of publications</li>
  <li>A motivation letter and research plan (max 3 pages)</li>
  <li>A statement on how you meet the selection criteria above (max 2 pages)</li>
  <li>A statement on how you plan to address the key responsibilities of the role (max 2 pages)</li>
  <li>Contact details for 1â€“2 referees, or attach reference letters</li>
</ul>

<!-- ## Postdoctoral Research Fellow -->
<!-- This position is with the QUT Centre for Robotics (QCR) and will work closely with me in the [Visual Learning and Understanding research program](https://research.qut.edu.au/qcr/visual-learning-understanding-2/). -->

<!-- **Position advertised: 6 March 2020**. -->

<div class="img_row">
<img class="col one" src="/assets/img/jobs/S11-1.jpg" />
<img class="col one" src="/assets/img/jobs/S11-3.jpg" />
<img class="col one" src="/assets/img/jobs/S11-2.jpg" />
</div>

<h3 id="about-us">About Us</h3>
<p><img class="col one" src="/assets/img/jobs/S11-4.jpg" />
The QUT Centre for Robotics (QCR) conducts at-scale world-leading research in intelligent robotics; translates fundamental research into commercial and societal outcomes; is a leader in education, training and development of talent to meet growing demands for expertise in robotics and autonomous systems; and provides leadership in technological policy development and societal debate. Established in 2020, the Centre has been built on the momentum of a decadeâ€™s investment in robotic research and translation at QUT which has been funded by QUT, ARC, Queensland Government, CRCs and Industry. QCR comprises over 100 researchers and engineers.</p>

<p>QCR researchers collaborate with industry and universities around the world, including MIT, Harvard and Oxford universities, Boeing, Thales, DST, Airservices Australia, CASA, JARUS, TRAFI, Google Deepmind, Google AI, Amazon Robotics, Caterpillar, Rheinmetall, US Air Force, and NASAâ€™s Jet Propulsion Laboratory.</p>

<p>We are proud of our beautiful and big modern lab space and research environment. We have a fantastic collection of equipment to support your research, including many mobile robot platforms and robotic arms.</p>

<p>We support a diverse and inclusive atmosphere and encourage applications from women, Aboriginal Australians and Torres Strait Islander people.</p>

<h3 id="why-brisbane-why-qut">Why Brisbane? Why QUT?</h3>
<p>Weâ€™re based in the heart of Brisbane on QUTâ€™s stunning Gardens Point campus, overlooking the Brisbane River and nestled beside the Botanic Gardens. Youâ€™ll enjoy easy access to the CBD, a vibrant cafÃ© and cultural scene, and a sub-tropical climate perfect for year-round outdoor activity.
Brisbane is a highly liveable city of 2.7 million people. Whether you love:</p>
<ul>
  <li>Climbing (Kangaroo Point is across the river!)</li>
  <li>Surfing (Gold Coast is 1hr away)</li>
  <li>Hiking and national parks</li>
  <li>Scuba diving (Moreton Bay or Great Barrier Reef)</li>
  <li>or live music, theatre, and sport â€“ Brisbane has something for you.</li>
</ul>

<iframe src="https://www.google.com/maps/embed?pb=!1m14!1m12!1m3!1d2906.049913465745!2d153.02894556592284!3d-27.47748600138352!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!5e1!3m2!1sen!2sau!4v1583472988458!5m2!1sen!2sau" width="600" height="450" frameborder="0" style="border:0;" allowfullscreen=""></iframe>

<!-- <div class="col three caption">
      We are proud of our modern lab space and equipment.
</div> -->

<!-- 

### About the Visual Learning and Understanding Program
The Research Fellow will be a key member of QCRâ€™s Visual Learning and Understanding Program.

This program investigates the fundamental problem of how a robot can learn to reliably interpret its environment, and build an internal representation of its surroundings in order to decide on its actions. The program addresses research questions such as how machine learning for visual perception can be made safe, trustworthy, and reliable; how robots can understand and represent the geometry, semantics, and functionality of their surroundings and the task-relevant objects therein; and how robots can use this internal representation and learn to decide or plan their next actions in order to accomplish a useful task in a safe way.


### Type of Appointment
This appointment will be offered on a fixed term, full-time basis for **12 months** with a potential option for extension.

### Remuneration and Benefits
This Research Fellow position is classified as Academic Level B (LEVB) which has an annual salary of **$104,290 AUD to $123,858 AUD, plus 17% superannuation**.

QUT is a high quality and flexible organisation that is proud of its excellent employment conditions which include but are not limited to:
 * Reduced working year scheme
 * Parental leave provisions
 * Study support encompassing leave and financial assistance
 * Comprehensive professional development
 * Salary Packaging

### Location
Queensland University of Technology (QUT), Gardens Point campus, Brisbane, Australia.
We are located on a beautiful campus next to the Brisbane City Botanic Gardens, just a few minutes on foot from the Brisbane CBD and the bustling Southbank cultural precinct with many fantastic restaurants and bars. QUT has excellent connections to public transport, including our CityCat river ferry, trains, and bus lines. A bike path along the river connects QUT with the nearby suburbs.

Brisbane is a very liveable sub-tropical city of 2.3M people and offers great opportunities for recreational activities ranging from hiking in the many nearby national parks, rock climbing (the Kangaroo Point crag is just across the river, and there are many of well-maintained sport crags in a 1-2 hour radius around Brisbane, as well as a selection of climbing and bouldering gyms in the city), surfing (the famous Gold Coast is just over one hour away), and all things beach and ocean related. -->


  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2025 Niko SÃ¼nderhauf.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://nikosuenderhauf.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://nikosuenderhauf.github.io/assets/js/katex.js"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>






<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://nikosuenderhauf.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-135749210-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
