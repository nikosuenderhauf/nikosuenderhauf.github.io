---
---

@inproceedings{zhang2020varifocal,
  title={{VarifocalNet: An IoU-aware Dense Object Detector}},
  author={Zhang, Haoyang and Wang, Ying and Dayoub, Feras and S\"underhauf, Niko},
  booktitle={{Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year={2021},
  arxiv={2008.13367},
  link={https://arxiv.org/abs/2008.13367},
  thumb={zhang2020varifocal.png},
  abstract={Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. Prior work uses the classification score or a combination of classification and predicted localization scores to rank candidates. However, neither option results in a reliable ranking, thus degrading detection performance. In this paper, we propose to learn an Iou-aware Classification Score (IACS) as a joint representation of object presence confidence and localization accuracy. We show that dense object detectors can achieve a more accurate ranking of candidate detections based on the IACS. We design a new loss function, named Varifocal Loss, to train a dense object detector to predict the IACS, and propose a new star-shaped bounding box feature representation for IACS prediction and bounding box refinement. Combining these two new components and a bounding box refinement branch, we build an IoU-aware dense object detector based on the FCOS+ATSS architecture, that we call VarifocalNet or VFNet for short.},
  html={https://github.com/hyz-xmaster/VarifocalNet},
  award={Oral Presentation},
}


@inproceedings{rahman2020monitoring,
  title={{Online Monitoring of Object Detection Performance Post-Deployment}},
  author={Rahman, Quazi Marufur and S\"underhauf, Niko and Dayoub, Feras},
  booktitle={{Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS)}},
  year={2021},
  arxiv={2011.07750},
  link={https://arxiv.org/abs/2011.07750},
  thumb={rahman2020monitoring.png},
  abstract={Post-deployment, an object detector is expected to operate at a similar level of performance that was reported on its testing dataset. However, when deployed onboard mobile robots that operate under varying and complex environmental conditions, the detector's performance can fluctuate and occasionally degrade severely without warning. Undetected, this can lead the robot to take unsafe and risky actions based on low-quality and unreliable object detections. We address this problem and introduce a cascaded neural network that monitors the performance of the object detector by predicting the quality of its mean average precision (mAP) on a sliding window of the input frames.},
}

@inproceedings{bista2021mapping,
  title={{Evaluating the Impact of Semantic Segmentation and Pose Estimationon Dense Semantic SLAM}},
  author={Suman Raj Bista and David Hall and Ben Talbot and Haoyang Zhang and Feras Dayoub and Niko S\"{u}nderhauf},
  booktitle={{Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS)}},
  year={2021},
  thumb={bista21mapping.png},
  abstract={Recent Semantic SLAM methods combine classical geometry-based estimation with deep learning-based object detection or semantic segmentation.
In this paper we evaluate the quality of semantic maps generated by state-of-the-art class- and instance-aware dense semantic SLAM algorithms whose codes are publicly available and explore the impacts both semantic segmentation and pose estimation have on the quality of semantic maps.
We obtain these results by providing algorithms with ground-truth pose and/or semantic segmentation data available from simulated environments. We establish that semantic segmentation is the largest source of error through our experiments, dropping mAP and OMQ performance by up to 74.3\% and 71.3\% respectively.},
}



@inproceedings{miller2020cac,
  title={{Class Anchor Clustering: A Loss for Distance-based Open Set Recognition}},
  author={Dimity Miller and Niko S\"underhauf and Michael Milford and Feras Dayoub},
  booktitle={{IEEE Winter Conference on Applications of Computer Vision (WACV)}},
  year={2021},
  abstract={Existing open set classifiers distinguish between known and unknown inputs by measuring distance in a network's logit space, assuming that known inputs cluster closer to the training data than unknown inputs. However, this approach is typically applied post-hoc to networks trained with cross-entropy loss, which neither guarantees nor encourages the hoped-for clustering behaviour. To overcome this limitation, we introduce Class Anchor Clustering (CAC) loss. CAC is an entirely distance-based loss that explicitly encourages training data to form tight clusters techniques on the challenging TinyImageNet dataset, achieving a 2.4% performance increase in AUROC. },
  link={https://arxiv.org/abs/2004.02434},
  arxiv={2004.02434},
  thumb={miller2020cac.png},
}


@inproceedings{rana2020multiplicative,
  title={{Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer}},
  author={Rana, Krishan and Dasagi, Vibhavari and Talbot, Ben and Milford, Michael and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2003.05117},
  year={2020},
  booktitle={{Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS)}},
  abstract={Learning long-horizon tasks on real robot hardware can be intractable, and transferring a learned policy from simulation to reality is still extremely challenging. We present a novel approach to model-free reinforcement learning that can leverage existing sub-optimal solutions as an algorithmic prior during training and deployment. During training, our gated fusion approach enables the prior to guide the initial stages of exploration, increasing sample-efficiency and enabling learning from sparse long-horizon reward signals. Importantly, the policy can learn to improve beyond the performance of the sub-optimal prior since the prior's influence is annealed gradually. During deployment, the policy's uncertainty provides a reliable strategy for transferring a simulation-trained policy to the real world by falling back to the prior controller in uncertain states. We show the efficacy of our Multiplicative Controller Fusion approach on the task of robot navigation and demonstrate safe transfer from simulation to the real world without any fine tuning. },
  arxiv={2003.05117},
  link={https://arxiv.org/abs/2003.05117},
  thumb={rana2020multiplicative.png},
  html={https://sites.google.com/view/mcf-nav/home}
}

@inproceedings{rana19navigation,
  title={{Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments}},
  author={Krishan Rana and Ben Talbot and Michael Milford and Niko S{\"u}nderhauf},
  booktitle = {{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  year={2020},
  abstract={In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRNN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world.},
  link={https://arxiv.org/abs/1909.10972},
  arxiv={1909.10972},
  thumb={rana19navigation.png},
  html={https://github.com/krishanrana/2D_SRRN}
}


@inproceedings{hall2018probability,
  title={{Probabilistic Object Detection: Definition and Evaluation}},
  author={Hall, David and Dayoub, Feras and Skinner, John and Corke, Peter and Carneiro, Gustavo and Angelova, Anelia and S{\"u}nderhauf, Niko},
  booktitle={{IEEE Winter Conference on Applications of Computer Vision (WACV)}},
  year={2020},
  abstract={We introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections. Given the lack of methods capable of assessing such probabilistic object detections, we present the new Probability-based Detection Quality measure (PDQ). Unlike AP-based measures, PDQ has no arbitrary thresholds and rewards spatial and label quality, and foreground/background separation quality while explicitly penalising false positive and false negative detections.},
  link={https://arxiv.org/abs/1811.10800},
  arxiv={1811.10800},
  thumb={hall2018probability.png}
}


@inproceedings{Miller19sampling,
  title={{Evaluating Merging Strategies for Sampling-based Uncertainty Techniques in Object Detection}},
  author={Dimity Miller and Feras Dayoub and Michael Milford and Niko S\"underhauf},
  year={2019},
  booktitle = {{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  abstract = {There has been a recent emergence of sampling-based techniques for estimating epistemic uncertainty in deep neural networks. While these methods can be applied to classification or semantic segmentation tasks by simply averaging samples, this is not the case for object detection, where detection sample bounding boxes must be accurately associated and merged. A weak merging strategy can significantly degrade the performance of the detector and yield an unreliable uncertainty measure. This paper provides the first in-depth investigation of the effect of different association and merging strategies. We compare different combinations of three spatial and two semantic affinity measures with four clustering methods for MC Dropout with a Single Shot Multi-Box Detector. Our results show that the correct choice of affinity-clustering combinations can greatly improve the effectiveness of the classification and spatial uncertainty estimation and the resulting object detection performance. We base our evaluation on a new mix of datasets that emulate near open-set conditions (semantically similar unknown classes), distant open-set conditions (semantically dissimilar unknown classes) and the common closed-set conditions (only known classes).},
  link={https://arxiv.org/abs/1809.06006},
  thumb={miller18sampling.png},
  arxiv={1809.06006}
}

@inproceedings{Dasagi19transfer,
  title={{Sim-to-Real Transfer of Robot Learning with Variable Length Inputs}},
  author={Vibhavari Dasagi and Robert Lee and Serena Mou and Jake Bruce and Niko S\"underhauf and Jürgen Leitner},
  year={2019},
  booktitle = {{Australasian Conf. for Robotics and Automation (ACRA)}},
  link={https://arxiv.org/abs/1809.07480},
  thumb={lee18zeroshot.png},
  arxiv={1809.07480},
  abstract={Current end-to-end deep Reinforcement Learning (RL) approaches require jointly learning perception, decision-making and low-level control from very sparse reward signals and high-dimensional inputs, with little capability of incorporating prior knowledge. In this work, we propose a framework that combines deep sets encoding, which allows for variable-length abstract representations, with modular RL that utilizes these representations, decoupling high-level decision making from low-level control. We successfully demonstrate our approach on the robot manipulation task of object sorting, showing that this method can learn effective policies within mere minutes of highly simplified simulation. The learned policies can be directly deployed on a robot without further training, and generalize to variations of the task unseen during training.}
}

@inproceedings{stanislas2019airborne,
  title={Airborne Particle Classification in LiDAR Point Clouds Using Deep Learning},
  author={Stanislas, Leo and Nubert, Julian and Dugas, Daniel and Nitsch, Julia and S{\"u}nderhauf, Niko and Siegwart, Roland and Cadena, Cesar and Peynot, Thierry},
  year={2019},
  booktitle={Proceedings of the Conference on Field and Service Robotics (FSR)},
  link={https://eprints.qut.edu.au/133596/1/FSR_2019_paper_47.pdf},
  abstract={In this work, we propose and compare two deep learning approaches to classify airborne particles in LiDAR data. The first is based on voxel-wise classification, while the second is based on point-wise classification. We also study the impact of different combinations of input features extracted from LiDAR data, including the use of multi-echo returns as a classification feature. We evaluate the performance of the proposed methods on a realistic dataset with the presence of fog and dust particles in outdoor scenes. We achieve an F1 score of 94% for the classification of airborne particles in LiDAR point clouds, thereby significantly outperforming the state-of-the-art.},
  thumb={stanislas19lidar.png}
}

@inproceedings{rahman2019traffic,
  title={Did You Miss the Sign? A False Negative Alarm System for Traffic Sign Detectors},
  author={Rahman, Quazi Marufur and S{\"u}nderhauf, Niko and Dayoub, Feras},
  booktitle={{Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS)}},
  arxiv={1903.06391},
  year={2019},
  thumb={rahman2019traffic.png},
  abstract={In this paper, we propose an approach to identify traffic signs that have been mistakenly discarded by the object detector. The proposed method raises an alarm when it discovers a failure by the object detector to detect a traffic sign. This approach can be useful to evaluate the performance of the detector during the deployment phase. We trained a single shot multi-box object detector to detect traffic signs and used its internal features to train a separate false negative detector (FND). During deployment, FND decides whether the traffic sign detector has missed a sign or not.},
  link={https://arxiv.org/abs/1903.06391}
}

@inproceedings{garg2019look,
  title={Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation},
  author={Garg, Sourav and Babu, V and Dharmasiri, Thanuja and Hausler, Stephen and S\"underhauf, Niko and Kumar, Swagat and Drummond, Tom and Milford, Michael},
  booktitle={{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  abstract={We present a new depth-and temporal-aware visual place recognition system that solves the opposing viewpoint, extreme appearance-change visual place recognition problem. Our system performs sequence-to-single matching by extracting depth-filtered keypoints using a state-of-the-art depth estimation pipeline, constructing a keypoint sequence over multiple frames from the reference dataset, and comparing those keypoints to those in a single query image. We evaluate the system on a challenging benchmark dataset and show that it consistently outperforms state-of-the-art techniques. We also develop a range of diagnostic simulation experiments that characterize the contribution of depth-filtered keypoint sequences with respect to key domain parameters including degree of appearance change and camera motion.},
  arxiv={1902.07381},
  year={2019},
  thumb={garg2019look.png},
  link={https://arxiv.org/abs/1902.07381}
}

@inproceedings{hosseinzadeh2018structure,
  title={{Structure Aware SLAM using Quadrics and Planes}},
  author={Hosseinzadeh, M and Latif, Y and Pham, T and S{\"u}nderhauf, N and Reid, I},
  booktitle={Proceedings of Asian Conference on Computer Vision (ACCV)},
  year={2018},
  arxiv={1804.09111}
}


@inproceedings{stanislas2018lidar,
  title={{Lidar-based Detection of Airborne Particles for Robust Robot Perception}},
  author={Leo Stanislas and Niko S{\"u}nderhauf and Thierry Peynot },
  booktitle={Proceedings of Australasian Conference on Robotics and Automation (ACRA)},
  year={2018}
}

@inproceedings{Bruce18navigation,
  title={Learning Deployable Navigation Policies at Kilometer Scale from a Single Traversal},
  author={Bruce, Jake and S{\"u}nderhauf, Niko and Mirowski, Piotr and Hadsell, Raia and Milford, Michael},
  booktitle={{Proc. of Conference on Robot Learning (CoRL)}},
  year={2018},
  abstract={We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multiple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time.},
  thumb={bruce18navigation.png},
  link={https://arxiv.org/abs/1807.05211},
  arxiv={1807.05211},
  html={http://rl-navigation.github.io/deployable}
}


@inproceedings{garg2018lost,
  title={LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics},
  author={Garg, Sourav and S\"underhauf, Niko and Milford, Michael},
  booktitle={{Proc. of Robotics: Science and Systems (RSS)}},
  year={2018},
  link={https://arxiv.org/pdf/1804.05526},
  abstract={In this paper we develop a suite of novel semantic- and appearance-based techniques to enable for the first time high performance place recognition in the challenging scenario of recognizing places when returning from the opposite direction. We first propose a novel Local Semantic Tensor (LoST) descriptor of images using the convolutional feature maps from a state-of-the-art dense semantic segmentation network. Then, to verify the spatial semantic arrangement of the top matching candidates, we develop a novel approach for mining semantically-salient keypoint correspondences. },
  thumb={garg18rss.png}
}

@inproceedings{Miller18,
  title={{Dropout Sampling for Robust Object Detection in Open-Set Conditions}},
  author={Dimity Miller and Lachlan Nicholson and Feras Dayoub and Niko S\"underhauf},
  year={2018},
  booktitle = {{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  abstract = {Dropout Variational Inference, or Dropout Sampling, has been recently proposed as an
approximation technique for Bayesian Deep Learning and evaluated for image classification
and regression tasks. This paper investigates the utility of Dropout Sampling for object
detection for the first time. We demonstrate how label uncertainty can be extracted from a
state-of-the-art object detection system via Dropout Sampling. We show that this uncertainty
can be utilized to increase object detection performance under the open-set conditions that
are typically encountered in robotic vision. We evaluate this approach on a large synthetic
dataset with 30,000 images, and a real-world dataset captured by a mobile robot in a
versatile campus environment.},
link={http://arxiv.org/abs/1710.06677},
thumb={miller18dropout.png}
}

@inproceedings{anderson2017vision,
  title={Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Hengel, Anton van den},
  booktitle = {{Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year={2018},
  thumb={anderson18navigation.png},
  abstract={ To enable and encourage the application of vision and
language methods to the problem of interpreting visually grounded
navigation instructions, we present the Matterport3D
Simulator – a large-scale reinforcement learning
environment based on real imagery. Using this simulator,
which can in future support a range of embodied vision
and language tasks, we provide the first benchmark dataset
for visually-grounded natural language navigation in real
buildings – the Room-to-Room (R2R) dataset.},
link={http://arxiv.org/abs/1711.07280}
}


@inproceedings{Garg18,
  title={{Don't Look Back: Robustifying Place Categorization for Viewpoint- and Condition-Invariant Place Recognition}},
  author={Sourav Garg and Niko S\"underhauf and Michael Milford},
  year={2018},
  booktitle = {{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  link = {https://arxiv.org/pdf/1801.05078},
  thumb = {garg18lookBack.png},
  abstract= {In this work, we develop a novel methodology for using the semantics-aware
higher-order layers of deep neural networks for recognizing
specific places from within a reference database. To further
improve the robustness to appearance change, we develop a
descriptor normalization scheme that builds on the success of
normalization schemes for pure appearance-based techniques.}
}


@inproceedings{Trung18,
  title={{SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes}},
  author={Trung T. Pham and Thanh-Toan Do and Niko S\"underhauf and Ian Reid and Michael Milford},
  year={2018},
  booktitle = {{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  abstract = {This paper presents SceneCut, a novel approach to jointly discover previously unseen
objects and non-object surfaces using a single RGB-D image. SceneCut's joint reasoning
over scene semantics and geometry allows a robot to detect and segment object instances
in complex scenes where modern deep learning-based methods either fail to separate
object instances, or fail to detect objects that were not seen during training. SceneCut
automatically decomposes a scene into meaningful regions which either represent objects
or scene surfaces. The decomposition is qualified by an unified energy function over
objectness and geometric fitting. We show how this energy function can be optimized
efficiently by utilizing hierarchical segmentation trees.},
thumb={pham18scenecut.png},
link={http://arxiv.org/abs/1709.07158}
}




@inproceedings{Suenderhauf17a,
  title={{Meaningful Maps With Object-Oriented Semantic Mapping}},
  author={S\"underhauf, Niko and Pham, Trung T. Pham and Latif, Yasir and Milford, Michael},
  booktitle={Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year={2017},
  link={https://ieeexplore.ieee.org/abstract/document/8206392/},
  abstract={For intelligent robots to interact in meaningful ways with their environment, they must understand both the geometric and semantic properties of the scene surrounding them. The majority of research to date has addressed these mapping challenges separately, focusing on either geometric or semantic mapping. In this paper we address the problem of building environmental maps that include both semantically meaningful, object-level entities and point- or mesh-based geometrical representations. We simultaneously build geometric point cloud models of previously unseen instances of known object classes and create a map that contains these object models as central entities. Our system leverages sparse, feature-based RGB-D SLAM, image-based deep-learning object detection and 3D unsupervised segmentation.},
  thumb={suenderhauf17maps.png}
}


@inproceedings{Leitner17,
  title={{The ACRV Picking Benchmark: A Robotic Shelf Picking Benchmark to Foster Reproducible Research}},
  author={J\"urgen Leitner and Adam W Tow and Jake E Dean and Niko S\"underhauf and Joseph W Durham and Matthew Cooper and Markus Eich and Christopher Lehnert and Ruben Mangels and Christopher McCool and Peter Kujala and Lachlan Nicholson and Trung Pham and James Sergeant and Fangyi Zhang and Ben Upcroft and Peter Corke},
  year={2017},
  booktitle = {{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  abstract={Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress. They make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking: the ACRV Picking Benchmark. Designed to be reproducible, it consists of a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparison of complete robotic systems - including perception and manipulation - instead of sub-systems only. Our paper also describes and reports results achieved by an open baseline system based on a Baxter robot.},
  link={https://ieeexplore.ieee.org/abstract/document/7989545/},
  thumb={leitner17picking.png}
}

@inproceedings{Chen17,
  title={{Deep Learning Features at Scale for Visual Place Recognition}},
  author={Zetao Chen and Adam Jacobson and Niko S\"underhauf and Ben Upcroft and Lingqiao Liu and Chunhua Shen and Ian Reid and Michael Milford},
  year={2017},
  booktitle = {{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  abstract={In this paper, we train, at large scale, two CNN architectures for the specific place recognition task and employ a multi-scale feature encoding method to generate condition- and viewpoint-invariant features. To enable this training to occur, we have developed a massive Specific PlacEs Dataset (SPED) with hundreds of examples of place appearance change at thousands of different places, as opposed to the semantic place type datasets currently available. This new dataset enables us to set up a training regime that interprets place recognition as a classification problem. We comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10% increase in performance over other place recognition algorithms and pre-trained CNNs. },
  link={http://ieeexplore.ieee.org/abstract/document/7989366/},
  thumb={chen17placerec.png}

}

@inproceedings{McMahon17a,
  title={Auxiliary Tasks to Improve Trip Hazard Affordance Detection},
  author={McMahon, Sean and Shen, Tong and S{\"u}nderhauf, Niko and Reid, Ian and Shen, Chunhua and Milford, Michael},
  booktitle={Proceedings of the Australasian Conference on Robotics and Automation (ACRA)},
  year={2017},
  link={http://www.araa.asn.au/acra/acra2017/papers/pap104s1-file1.pdf},
  abstract={We propose to train a CNN performing pixel-wise trip detection with three auxiliary tasks to help the CNN better infer scene geometric properties of trip hazards. Of the three approaches investigated pixel-wise ground plane estimation, pixel depth estimation and pixel height above ground plane estimation, the first approach allowed the trip detector to achieve a 11.1% increase in Trip IOU over earlier work. These new approaches make it plausible to deploy a robotic platform to perform trip hazard detection, and so potentially reduce the number of injuries on construction sites.},
  thumb={mcmahon17a.png}
}

@inproceedings{Tow16,
  title={{A Robustness Analysis of Deep Q Networks}},
  author={Adam W Tow and Sareh Shirazi and J\"urgen Leitner and Niko S\"underhauf and Michael Milford and Ben Upcroft},
  year={2016},
  booktitle = {{Proc. of Australasian Conference on Robotics and Automation (ACRA)}},
  link={http://eprints.qut.edu.au/104307/},
  abstract={In this paper, we present an analysis of the robustness of Deep Q Networks to various types of perceptual noise (changing brightness, Gaussian blur, salt and pepper, distractors). We present a benchmark example that involves playing the game Breakout though a webcam and screen environment, like humans do. We present a simple training approach to improve the performance maintained when transferring a DQN agent trained in simulation to the real world. We also evaluate DQN agents trained under a variety of simulation environments to report for the first time how DQNs cope with perceptual noise, common to real world robotic applications.},
  thumb={tow16dqn.png}
}


@inproceedings{Hall15,
  title={Evaluation of features for leaf classification in challenging conditions},
  author={Hall, David and McCool, Chris and Dayoub, Feras and S\"underhauf, Niko and Upcroft, Ben},
  booktitle={Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on},
  pages={797--804},
  year={2015},
  organization={IEEE},
  link={http://ieeexplore.ieee.org/abstract/document/7045965/},
  thumb={hall15wacv.png},
  abstract={Fine-grained leaf classification has concentrated on the use of traditional shape and statistical features to classify ideal images. In this paper we evaluate the effectiveness of traditional hand-crafted features and propose the use of deep convolutional neural network (Conv Net) features. We introduce a range of condition variations to explore the robustness of these features, including: translation, scaling, rotation, shading and occlusion. Evaluations on the Flavia dataset demonstrate that in ideal imaging conditions, combining traditional and Conv Net features yields state-of-the art performance with an average accuracy of 97.3%±0:6% compared to traditional features which obtain an average accuracy of 91.2%±1:6%. Further experiments show that this combined classification approach consistently outperforms the best set of traditional features by an average of 5.7% for all of the evaluated condition variations.}

}

@inproceedings{Skinner16,
	title={High-Fidelity Simulation for Evaluating Robotic Vision Performance},
	author={John Robert Skinner and Sourav Garg and Niko S{\"u}nderhauf and Peter Corke and Ben Upcroft and Michael J Milford},
	year={2016},
	booktitle = {{Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS)}},
  link={https://ieeexplore.ieee.org/abstract/document/7759425/},
  abstract={For machine learning applications a critical bottleneck is the limited amount of real world image data that can be captured and labelled for both training and testing purposes. In this paper we investigate the use of a photo-realistic simulation tool to address these challenges, in three specific domains: robust place recognition, visual SLAM and object recognition. For the first two problems we generate images from a complex 3D environment with systematically varying camera paths, camera viewpoints and lighting conditions. For the first time we are able to systematically characterise the performance of these algorithms as paths and lighting conditions change. In particular, we are able to systematically generate varying camera viewpoint datasets that would be difficult or impossible to generate in the real world. We also compare algorithm results for a camera in a real environment and a simulated camera in a simulation model of that real environment. Finally, for the object recognition domain, we generate labelled image data and characterise the viewpoint dependency of a current convolution neural network in performing object recognition. Together these results provide a multi-domain demonstration of the beneficial properties of using simulation to characterise and analyse a wide range of robotic vision algorithms.},
  thumb={skinner16sim.png}
}


@inproceedings{Leitner16,
  title={{LunaRoo: Designing a Hopping Lunar Science Payload.}},
  author={J{\"u}rgen Leitner and Will Chamberlain and Donald G. Dansereau and Matthew Dunbabin and Markus Eich and Thierry Peynot and Jon Roberts and Raymond Russell and Niko S{\"u}nderhauf},
  booktitle={Proc. of IEEE Aerospace Conference},
  year={2016},
  link={http://ieeexplore.ieee.org/abstract/document/7500760/},
  abstract={We describe a hopping science payload solution designed to exploit the Moon's lower gravity to leap up to 20m above the surface. The entire solar-powered robot is compact enough to fit within a 10cm cube, whilst providing unique observation and mission capabilities by creating imagery during the hop. The LunaRoo concept is a proposed payload to fly onboard a Google Lunar XPrize entry. Its compact form is specifically designed for lunar exploration and science mission within the constraints given by PTScientists. The core features of LunaRoo are its method of locomotion - hopping like a kangaroo - and its imaging system capable of unique over-the-horizon perception. The payload will serve as a proof of concept, highlighting the benefits of alternative mobility solutions, in particular enabling observation and exploration of terrain not traversable by wheeled robots. in addition providing data for beyond line-of-sight planning and communications for surface assets, extending overall mission capabilities.},
  thumb={leitner16luna.png}
}


@inproceedings{Sergeant15,
  title={Multimodal Deep Autoencoders for Control of a Mobile Robot},
  author={Sergeant, James and S{\"u}nderhauf, Niko and Milford, Michael and Upcroft, Ben},
  booktitle={Proceedings of the Australasian Conference on Robotics and Automation (ACRA)},
  year={2015},
  link={http://www.araa.asn.au/acra/acra2015/papers/pap149.pdf},
  thumb={sergeant15.png},
  abstract={Robot navigation systems are typically engineered
to suit certain platforms, sensing suites
and environment types. In order to deploy a
robot in an environment where its existing navigation
system is insufficient, the system must
be modified manually, often at significant cost.
In this paper we address this problem, proposing
a system based on multimodal deep autoencoders
that enables a robot to learn how
to navigate by observing a dataset of sensor input
and motor commands collected while being
teleoperated by a human. Low-level features
and cross modal correlations are learned and
used in initialising two different architectures
with three operating modes. During operation,
these systems exploit the learned correlations
in generating suitable control signals based only
on the sensor information.}

}

@inproceedings{McMahon15b,
  title={TripNet: Detecting Trip Hazards on Construction Sites},
  author={McMahon, Sean and S{\"u}nderhauf, Niko and Milford, Michael and Upcroft, Ben},
  booktitle={Proceedings of the Australasian Conference on Robotics and Automation (ACRA)},
  year={2015},
  link={http://www.araa.asn.au/acra/acra2015/papers/pap158.pdf},
  abstract={This paper introduces TripNet, a robotic vision system that detects trip hazards using raw construction site images.
  TripNet performs trip hazard identification using only camera imagery and minimal training with a pre-trained Convolutional Neural Network (CNN) rapidly fine-tuned on a small corpus of labelled image regions from construction sites. There is no reliance on prior scene segmentation methods during deployment. Trip-Net achieves comparable performance to a human on a dataset recorded in two distinct real world construction sites. TripNet exhibits spatial and temporal generalization by functioning in previously unseen parts of a construction site and over time periods of several weeks.
  },
  thumb={mcmahon15.png}
}

@inproceedings{Rezazadegan15,
  title={Enhancing Human Action Recognition with Region Proposals},
  author={Rezazadegan, Fahimeh and Shirazi, Sareh and S{\"u}nderhauf, Niko and Milford, Michael and Upcroft, Ben},
  booktitle={Proceedings of the Australasian Conference on Robotics and Automation (ACRA)},
  year={2015}
}


@ARTICLE{Suenderhauf16,
  author = {Niko S\"underhauf and Feras Dayoub and Sean McMahon and Ben Talbot and Ruth Schulz and Peter Corke and Gordon Wyeth and Ben Upcroft and Michael Milford},
  title = {{Place Categorization and Semantic Mapping on a Mobile Robot}},
  journal = {{Proc. of IEEE International Conference on Robotics and Automation (ICRA)}},
  year = {2016},
  owner = {niko},
  timestamp = {2015.06.05},
  link={http://ieeexplore.ieee.org/abstract/document/7487796/},
  thumb={suenderhauf16mapping.png},
  abstract={In this paper we focus on the challenging problem of place categorization and semantic mapping on a robot without environment-specific training. Motivated by their ongoing success in various visual recognition tasks, we build our system upon a state-of-the-art convolutional network. We overcome its closed-set limitations by complementing the network with a series of one-vs-all classifiers that can learn to recognize new semantic classes online. Prior domain knowledge is incorporated by embedding the classification system into a Bayesian filter framework that also ensures temporal coherence. We evaluate the classification accuracy of the system on a robot that maps a variety of places on our campus in real-time. We show how semantic information can boost robotic object detection performance and how the semantic map can be used to modulate the robot's behaviour during navigation tasks. The system is made available to the community as a ROS module.}
}


@INPROCEEDINGS{Suenderhauf15a,
  author = {Niko S\"underhauf and Sareh Shirazi and Adam Jacobson and Feras Dayoub and Edward Pepperell and Ben Upcroft and Michael Milford},
  title = {{Place Recognition with ConvNet Landmarks: Viewpoint-Robust, Condition-Robust, Training-Free}},
  booktitle = {Proc. of Robotics: Science and Systems (RSS)},
  year = {2015},
  owner = {niko},
  timestamp = {2015.06.05},
  link={/assets/papers/rss15_placeRec.pdf},
  poster = {RSS-15-poster.pdf},
  thumb={suenderhauf15rss.png},
  abstract={Here
we present an approach that adapts state-of-the-art object
proposal techniques to identify potential landmarks within an
image for place recognition. We use the astonishing power
of convolutional neural network features to identify matching
landmark proposals between images to perform place recognition
over extreme appearance and viewpoint variations. Our system
does not require any form of training, all components are generic
enough to be used off-the-shelf. We present a range of challenging
experiments in varied viewpoint and environmental conditions.
We demonstrate superior performance to current state-of-the-art
techniques.}
}


@INPROCEEDINGS{Suenderhauf15,
  author = {Niko S\"underhauf and Feras Dayoub and Sareh Shirazi and Ben Upcroft
    and Michael Milford},
  title = {{On the Performance of ConvNet Features for Place Recognition}},
  booktitle = {{Proc. of IEEE International Conference on Intelligent Robots and Systems (IROS)}},
  year = {2015},
  owner = {niko},
  timestamp = {2015.01.19},
  link={/assets/papers/IROS15-placeRecognition.pdf},
  thumb={suenderhauf15iros.png},
  abstract={This paper comprehensively evaluates
and compares the utility of three state-of-the-art ConvNets on
the problems of particular relevance to navigation for robots;
viewpoint-invariance and condition-invariance, and for the first
time enables real-time place recognition performance using
ConvNets with large maps by integrating a variety of existing
(locality-sensitive hashing) and novel (semantic search space
partitioning) optimization techniques. We present extensive
experiments on four real world datasets cultivated to evaluate
each of the specific challenges in place recognition. The results
demonstrate that speed-ups of two orders of magnitude can
be achieved with minimal accuracy degradation, enabling
real-time performance. We confirm that networks trained for
semantic place categorization also perform better at (specific)
place recognition when faced with severe appearance changes
and provide a reference for which networks and layers are
optimal for different aspects of the place recognition problem.}
}


@INPROCEEDINGS{Suenderhauf14,
  author = {Niko S\"underhauf and Peer Neubert and Martina Truschzinski and Daniel
    Wunschel and Johannes P\"oschmann and Sven Lanve and Peter Protzel},
  title = {{Phobos and Deimos on Mars -- Two Autonomous Robots for the DLR SpaceBot
    Cup}},
  booktitle = {Proceedings of International Symposium on Artificial Intelligence,
    Robotics and Automation in Space (iSAIRAS)},
  year = {2014},
  owner = {niko},
  timestamp = {2014.06.12},
  link={/assets/papers/suenderhauf14spacebot.pdf},
  thumb={suenderhauf14spacebot.png},
  abstract={In 2013, ten teams from German universities and research
institutes participated in a national robot competition
called SpaceBot Cup organized by the DLR Space
Administration. The robots had one hour to autonomously
explore and map a challenging Mars-like environment,
find, transport, and manipulate two objects, and navigate
back to the landing site. Localization without GPS in an
unstructured environment was a major issue as was mobile
manipulation and very restricted communication. This paper describes our system of two rovers operating on the
ground plus a quadrotor UAV simulating an observing orbiting
satellite. We relied on ROS (robot operating system)
as the software infrastructure and describe the main
ROS components utilized in performing the tasks. Despite
(or because of) faults, communication loss and breakdowns,
it was a valuable experience with many lessons
learned.}
}

@INPROCEEDINGS{Suenderhauf13,
  author = {Niko S\"underhauf and Sven Lange and Peter Protzel},
  title = {{Incremental Sensor Fusion in Factor Graphs with Unknown Delays}},
  booktitle = {Proc. of ESA Symposium on Advanced Space Technologies in Robotics
    and Automation (ASTRA)},
  year = {2013},
  owner = {niko},
  timestamp = {2012.09.14},
  abstract={ Our paper addresses the problem of performing
incremental sensor fusion in factor graphs when
some of the sensor information arrive with a significant
unknown delay. We develop and compare two techniques
to handle such delayed measurements under mild conditions
on the characteristics of that delay: We consider
the unknown delay to be bounded and quantizable into
multiples of the state transition cycle time. The proposed
methods are evaluated using a simulation of a dynamic
3-DoF system that fuses odometry and GPS measurements.},
  thumb={suenderhauf13delays.png},
  link={/assets/papers/suenderhauf13delays.pdf}
}


@INPROCEEDINGS{Suenderhauf13c,
  author = {Niko S\"underhauf and Marcus Obst and Sven Lange and Gerd Wanielik
    and Peter Protzel},
  title = {{Switchable Constraints and Incremental Smoothing for Online Mitigation
    of Non-Line-of-Sight and Multipath Effects}},
  booktitle = {Proc. of IEEE Intelligent Vehicles Symposium (IV)},
  year = {2013},
  owner = {niko},
  timestamp = {2013.05.26},
  thumb={suenderhauf13iv.png},
  link={/assets/papers/IV13-NLOS-iSAM2.pdf},
  abstract={Reliable vehicle positioning is a crucial requirement
for many applications of advanced driver assistance
systems. While satellite navigation provides a reasonable performance
in general, it often suffers from multipath and non-line-of-sight
errors when it is applied in urban areas and therefore
does not guarantee consistent results anymore. Our paper
proposes a novel online method that identifies and excludes
the affected pseudorange measurements. Our approach does
not depend on additional sensors, maps, or environmental
models. We rather formulate the positioning problem as a
Bayesian inference problem in a factor graph and combine
the recently developed concept of switchable constraints with
an algorithm for efficient incremental inference in such graphs.
We furthermore introduce the concepts of auxiliary updates and
factor graph pruning in order to accelerate convergence while
keeping the graph size and required runtime bounded. A realworld
experiment demonstrates that the resulting algorithm is
able to successfully localize despite a large number of satellite
observations are influenced by NLOS or multipath effects.}
}

@INPROCEEDINGS{Suenderhauf13a,
  author = {Niko S\"underhauf and Peter Protzel},
  title = {{Switchable Constraints vs. Max-Mixture Models vs. RRR -- A Comparison
    of three Approaches to Robust Pose Graph SLAM}},
  booktitle = {Proc. of Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2013},
  owner = {niko},
  timestamp = {2012.09.20},
  abstract = {SLAM algorithms that can infer a correct map despite
the presence of outliers have recently attracted increasing
attention. In the context of SLAM, outlier constraints are typically
caused by a failed place recognition due to perceptional
aliasing. If not handled correctly, they can have catastrophic
effects on the inferred map. Since robust robotic mapping and
SLAM are among the key requirements for autonomous longterm
operation, inference methods that can cope with such data
association failures are a hot topic in current research. Our
paper compares three very recently published approaches to
robust pose graph SLAM, namely switchable constraints, maxmixture
models and the RRR algorithm. All three methods were
developed as extensions to existing factor graph-based SLAM
back-ends and aim at improving the overall system’s robustness
to false positive loop closure constraints. Due to the novelty of
the three proposed algorithms, no direct comparison has been
conducted so far.},
thumb={suenderhauf13icra.png},
link={/assets/papers/ICRA13-comparisonRobustSLAM.pdf}
}


@INPROCEEDINGS{Suenderhauf11,
  author = {Niko S\"underhauf and Peter Protzel},
  title = {{BRIEF-Gist -- Closing the Loop by Simple Means}},
  booktitle = {Proc. of IEEE Intl. Conf. on Intelligent Robots and Systems (IROS)},
  year = {2011},
  owner = {niko},
  timestamp = {2011.03.25},
  link={/assets/papers/briefGist.pdf}
}


@INPROCEEDINGS{Suenderhauf10a,
  author = {S{\"u}nderhauf, Niko and Neubert, Peer and Protzel, Peter},
  title = {{The Causal Update Filter -- A Novel Biologically Inspired Filter
    Paradigm for Appearance Based SLAM}},
  booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots
    and Systems (IROS)},
  year = {2010},
  owner = {niko},
  timestamp = {2010.04.14},
  link={https://ieeexplore.ieee.org/abstract/document/5653221/},
  thumb={suenderhauf10causal.png},
  abstract={Recently a SLAM algorithm based on biological principles (RatSLAM) has been proposed. It was proven to perform well in large and demanding scenarios. In this paper we establish a comparison of the principles underlying this algorithm with standard probabilistic SLAM approaches and identify the key difference to be an additive update step. Using this insight, we derive the novel, non-Bayesian Causal Update filter that is suitable for application in appearance-based SLAM. We successfully apply this new filter to two demanding vision-only urban SLAM problems of 5 and 66 km length. We show that it can functionally replace the core of RatSLAM, gaining a massive speed-up.}
}

@INPROCEEDINGS{Suenderhauf12c,
  author = {S{\"u}nderhauf, Niko and Obst, Marcus and Wanielik, Gerd and Protzel,
    Peter},
  title = {{Multipath Mitigation in GNSS-Based Localization using Robust Optimization}},
  booktitle = {Proc. of IEEE Intelligent Vehicles Symposium (IV)},
  year = {2012},
  owner = {niko},
  timestamp = {2012.05.11},
  abstract={Our paper adapts recent advances in the SLAM
(Simultaneous Localization and Mapping) literature to the
problem of multipath mitigation and proposes a novel approach
to successfully localize a vehicle despite a significant
number of multipath observations. We show that GNSS-based
localization problems can be modelled as factor graphs and
solved using efficient nonlinear least squares methods that
exploit the sparsity inherent in the problem formulation. Using
a recently developed novel approach for robust optimization,
satellite observations that are subject to multipath errors can be
successfully identified and rejected during the optimization process.
We demonstrate the feasibility of the proposed approach
on a real-world urban dataset and compare it to an existing
method of multipath detection.},
thumb={suenderhauf12iv.png},
link={/assets/papers/IV12-multipathMitigation.pdf}

}

@INPROCEEDINGS{Suenderhauf12,
  author = {Niko S{\"u}nderhauf and Peter Protzel},
  title = {{Towards a Robust Back-End for Pose Graph SLAM}},
  booktitle = {Proc. of IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2012},
  owner = {niko},
  timestamp = {2011.12.15},
  thumb = {suenderhauf12icra.png},
  abstract = {We propose a novel formulation that allows the back-end
to change parts of the topological structure of the graph
during the optimization process. The back-end can thereby
discard loop closures and converge towards correct solutions
even in the presence of false positive loop closures. This largely
increases the overall robustness of the SLAM system and closes
a gap between the sensor-driven front-end and the back-end
optimizers. We demonstrate the approach and present results
both on large scale synthetic and real-world dataset},
link={/assets/papers/ICRA12-robustSLAM.pdf}
}

@INPROCEEDINGS{Lange13,
  author = {Sven Lange and Niko S{\"u}nderhauf and Peter Protzel},
  title = {{Incremental Smoothing vs. Filtering for Sensor Fusion on an Indoor
    UAV}},
  booktitle = {Proc. of Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2013},
  pages = {1773 -- 1778},
  doi = {10.1109/ICRA.2013.6630810},
  owner = {niko},
  timestamp = {2012.09.19},
  link={https://ieeexplore.ieee.org/abstract/document/6630810/},
  abstract={Our paper explores the performance of a recently proposed incremental smoother in the context of nonlinear sensor fusion for a real-world UAV. This efficient factor graph based smoothing approach has a number of advantages compared to conventional filtering techniques like the EKF or its variants. It can more easily incorporate asynchronous and delayed measurements from sensors operating at different rates and is supposed to be less error-prone in highly nonlinear settings. We compare the novel incremental smoothing approach based on iSAM2 against our conventional EKF based sensor fusion framework. Unlike previously presented work, the experiments are not only performed in simulation, but also on a real-world quadrotor UAV system using IMU, optical flow and altitude measurements.},
  thumb={lange13smoothing.png}
}


@INPROCEEDINGS{Lange11,
  author = {Lange, Sven and S{\"u}nderhauf, Niko and Neubert, Peer and Drews, Sebastian and
    Protzel, Peter},
  title = {{Autonomous Corridor Flight of a UAV Using a Low-Cost and Light-Weight
    RGB-D Camera}},
  booktitle = {Proc. of Intl. Symposium on Autonomous Mini Robots for Research and
    Edutainment (AMiRE)},
  year = {2011},
  timestamp = {2011.12.15},
  link={/assets/papers/lange11corridorAMiRE.pdf},
  thumb={lange11corridorAMiRE.png},
  abstract={We describe the first application of the novel Kinect RGB-D sensor on a fully autonomous quadrotor UAV. In contrast to the established RGB-D devices that are both expensive and comparably heavy, the Kinect is light-weight and especially low-cost. It provides dense color and depth information and can be readily applied to a variety of tasks in the robotics domain. We apply the Kinect on a UAV in an indoor corridor scenario. The sensor extracts a 3D point cloud of the environment that is further processed on-board to identify walls, obstacles, and the position and orientation of the UAV inside the corridor. Subsequent controllers for altitude, position, velocity, and heading enable the UAV to autonomously operate in this indoor environment.}
}
@INPROCEEDINGS{Lange11a,
  author = {Sven Lange and Niko S\"underhauf and Peer Neubert and Sebastian Drews and Peter Protzel},
  title = {{Autonomous Corridor Flight of a UAV Using an RGB-D Camera}},
  booktitle = {{Proc. of EuRobotics RGB-D Workshop on 3D Perception in Robotics}},
  year = {2011},
  owner = {niko},
  timestamp = {2012.09.24},
  link={/assets/papers/lange11corridor.pdf},
  thumb={lange11corridor.png},
  abstract={We describe the first application of the novel Kinect RGB-D sensor on a fully autonomous quadrotor UAV. We apply
the UAV in an indoor corridor scenario. The position and
orientation of the UAV inside the corridor is extracted from
the RGB-D data. Subsequent controllers for altitude, posi-
tion, velocity, and heading enable the UAV to autonomously
operate in this indoor environment.}
}

@INPROCEEDINGS{Suenderhauf12e,
  author = {S{\"u}nderhauf, Niko and Protzel, Peter},
  title = {{Switchable Constraints for Robust Pose Graph SLAM}},
  booktitle = {Proc. of IEEE International Conference on Intelligent Robots and
    Systems (IROS)},
  year = {2012},
  address = {Vilamoura, Portugal},
  owner = {niko},
  timestamp = {2012.08.20},
  thumb={suenderhauf12iros.png},
  abstract={Current SLAM back-ends are based on least
squares optimization and thus are not robust against outliers
like data association errors and false positive loop closure
detections. Our paper presents and evaluates a robust back-end
formulation for SLAM using switchable constraints. Instead
of proposing yet another appearance-based data association
technique, our system is able to recognize and reject outliers
during the optimization. This is achieved by making
the topology of the underlying factor graph representation
subject to the optimization instead of keeping it fixed. The
evaluation shows that the approach can deal with up to 1000
false positive loop closure constraints on various datasets. This
largely increases the robustness of the overall SLAM system
and closes a gap between the sensor-driven front-end and the
back-end optimizers.},
link={/assets/papers/IROS12-switchableConstraints.pdf}
}
@INPROCEEDINGS{Suenderhauf12a,
  author = {Niko S{\"u}nderhauf and Peter Protzel},
  title = {{Towards Robust Graphical Models for GNSS-Based Localization in Urban
    Environments}},
  booktitle = {Proc. of IEEE International Multi-Conference on Systems, Signals
    and Devices (SSD)},
  year = {2012},
  address = {Chemnitz, Germany},
  month = mar,
  days = {20},
  owner = {niko},
  timestamp = {2012.02.06}
}


@INPROCEEDINGS{Suenderhauf10b,
  author = {Niko S{\"u}nderhauf and Peter Protzel},
  title = {{Beyond RatSLAM: Improvements to a Biologically Inspired SLAM System}},
  booktitle = {Proc of Intel. Conf. on Emerging Technologies and Factory Automation
    (ETFA)},
  year = {2010},
  owner = {niko},
  timestamp = {2010.07.09},
  link={https://ieeexplore.ieee.org/abstract/document/5641280/},
  abstract={A SLAM algorithm inspired by biological principles
has been recently proposed and shown to perform well
in a large and demanding scenario. We analyse and compare
this system (RatSLAM) and the established Bayesian
SLAM methods and identify the key difference to be an additive
update step. Using this insight, we derive a novel
filter scheme and successfully show that it can entirely replace
the core of the RatSLAM system while maintaining
its desirable robustness. This leads to a massive speedup,
as the novel filter can be calculated very efficiently.
We successfully applied the new algorithm to the same 66
km long dataset that was used with the original algorithm.},
  thumb={suenderhauf10beyond.png}
}



@INPROCEEDINGS{Suenderhauf10c,
  author = {S{\"u}nderhauf, Niko and Protzel, Peter},
  title = {{From Neurons to Robots: Towards Efficient Biologically Inspired
    Filtering and SLAM}},
  booktitle = {{Proc. of KI 2010: Advances in Artificial Intelligence}},
  year = {2010},
  owner = {niko},
  timestamp = {2012.09.24},
  link={/assets/papers/suenderhauf10efficient.pdf},
  thumb={suenderhauf10efficient.png},
  abstract={We discuss recently published models of neural information process-
ing under uncertainty and a SLAM system that was inspired by the neural struc-
tures underlying mammalian spatial navigation. We summarize the derivation of
a novel filter scheme that captures the important ideas of the biologically inspired
SLAM approach, but implements them on a higher level of abstraction. This leads
to a new and more efficient approach to biologically inspired filtering which we
successfully applied to real world urban SLAM challenge of 66 km length.}
}

@INPROCEEDINGS{Lange09,
  author = {Sven Lange and Niko S{\"u}nderhauf and Peter Protzel},
  title = {{A Vision Based Onboard Approach for Landing and Position Control
    of an Autonomous Multirotor UAV in GPS-Denied Environments}},
  booktitle = {Proc. of Intl. Conf. on Advanced Robotics (ICAR)},
  year = {2009},
  link = {https://ieeexplore.ieee.org/abstract/document/5174709/},
  thumb={lange09landing.png},
  abstract={We describe our work on multirotor UAVs and
focus on our method for autonomous landing and position
control. The paper describes the design of our landing pad
and the vision based detection algorithm that estimates the 3Dposition
of the UAV relative to the landing pad. A cascaded
controller structure stabilizes velocity and position in the
absence of GPS signals by using a dedicated optical flow sensor.
Practical experiments prove the quality of our approach.}
}

@INPROCEEDINGS{Suenderhauf09,
  author = {S{\"u}nderhauf, Niko and Protzel, Peter},
  title = {{Using Image Profiles and Integral Images for Efficient Calculation
    of Sparse Optical Flow Fields.}},
  booktitle = {Proceedings of the International Conference on Advanced Robotics},
  year = {2009},
  file = {:afs/tu-chemnitz.de/project/proaut/paperDB/2009/suenderhauf09-using_image_profiles_and_integral_images_for.pdf:PDF},
  owner = {niko},
  timestamp = {2010.04.14}
}

@INPROCEEDINGS{Suenderhauf07,
  author = {Niko S{\"u}nderhauf and Sven Lange and Peter Protzel},
  title = {{Using the Unscented Kalman Filter in Mono-SLAM with Inverse Depth
    Parametrization for Autonomous Airship Control}},
  booktitle = {Proc. of IEEE International Workshop on Safety Security and Rescue
    Robotics (SSRR)},
  year = {2007},
  doi = {10.1109/SSRR.2007.4381265},
  link={https://ieeexplore.ieee.org/abstract/document/4381265/},
  thumb={suenderhauf07.png},
  abstract={In this paper, we present an approach for aiding control of an autonomous airship by the means of SLAM. We show how the Unscented Kalman Filter can be applied in a SLAM context with monocular vision. The recently published Inverse Depth Parametrization is used for undelayed single-hypothesis landmark initialization and modelling. The novelty of the presented approach lies in the combination of UKF, Inverse Depth Parametrization and bearing-only SLAM and its application for autonomous airship control and UAV control in general.}
}

@INPROCEEDINGS{Suenderhauf06a,
  author = {S{\"u}nderhauf, Niko and Krause, T. and Protzel, P.},
  title = {{Bringing Robotics Closer to Students -- A Threefold Approach}},
  booktitle = {Proc. of Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2006},
  owner = {niko},
  timestamp = {2012.09.24},
  link={/assets/papers/icra06-roboking.pdf}
}


@INPROCEEDINGS{Krueger06,
  author = {Daniel Kr{\"u}ger and Ingo van Lil and Niko S{\"u}nderhauf and Robert
    Baumgartl and Peter Protzel},
  title = {Using and Extending the Miro Middleware for Autonomous Mobile Robots},
  booktitle = {Proceedings of Towards Autonomous Robotic Systems (TAROS06)},
  year = {2006},
  address = {Guildford, UK},
  month = {September}
}

@INPROCEEDINGS{Suenderhauf06,
  author = {Niko S{\"u}nderhauf and Peter Protzel},
  title = {{Towards Using Bundle Adjustment for Robust Stereo Odometry in Outdoor
    Terrain}},
  booktitle = {Proceedings of Towards Autonomous Robotic Systems (TAROS06)},
  year = {2006}
}

@INPROCEEDINGS{Suenderhauf05a,
  author = {S{\"u}nderhauf, Niko and Krause, Thomas and Protzel, Peter},
  title = {{RoboKing - Bringing Robotics Closer to Pupils}},
  booktitle = {Proc. of Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2005},
  doi = {10.1109/ROBOT.2005.1570774},
  owner = {niko},
  timestamp = {2012.09.25},
  link={/assets/papers/icra05-roboking.pdf}
}

@INPROCEEDINGS{Suenderhauf05,
  author = {Niko S{\"u}nderhauf and Kurt Konolige and Simon Lacroix and Peter
    Protzel},
  title = {{Visual Odometry using Sparse Bundle Adjustment on an Autonomous
    Outdoor Vehicle}},
  booktitle = {Tagungsband Autonome Mobile Systeme 2005},
  year = {2005},
  editor = {Levi and Schanz and Lafrenz and Avrutin},
  series = {Reihe Informatik aktuell},
  pages = {157--163},
  publisher = {Springer-Verlag}
}


@INPROCEEDINGS{Neubert13b,
  author = {Neubert, Peer and S\"underhauf, Niko and Protzel, Peter},
  title = {{Appearance Change Prediction for Long-Term Navigation Across Seasons}},
  booktitle = {Proceedings of European Conference on Mobile Robotics (ECMR)},
  year = {2013},
  owner = {niko},
  timestamp = {2013.07.02},
  link = {/assets/papers/ECMR13_ACP.pdf}
}

@INPROCEEDINGS{Neubert07,
  author = {Peer Neubert and Niko S\"{u}nderhauf and Peter Protzel},
  title = {{FastSLAM using SURF Features: An Efficient Implementation and Practical
    Experiences}},
  booktitle = {Proceedings of the International Conference on Intelligent and Autonomous
    Vehicles, IAV07},
  year = {2007},
  address = {Tolouse, France},
  month = {September}
}


@INPROCEEDINGS{Lange08,
  author = {Sven Lange and Niko S{\"u}nderhauf and Peter Protzel},
  title = {{Autonomous Landing for a Multirotor UAV Using Vision}},
  booktitle = {Workshop Proc. of SIMPAR 2008 Intl. Conf. on Simulation, Modeling
    and Programming for Autonomous Robots},
  year = {2008},
  pages = {482--491},
  address = {Venice, Italy},
  month = nov,
  link = {/assets/papers/lange08landing.pdf},
  abstract={We describe our work on multirotor UAVs and focus on our
method for autonomous landing. The paper describes the design of our
landing pad and its advantages. We explain how the landing pad detection
algorithm works and how the 3D-position of the UAV relative to
the landing pad is calculated. Practical experiments prove the quality of
these estimations.},
thumb={lange08landing.png},
  isbn = {978-88-95872-01-8}
}

@INPROCEEDINGS{Bauer07,
  author = {Johannes Bauer and Niko S\"{u}nderhauf and Peter Protzel},
  title = {Comparing several implementations of two recently published feature
    detectors},
  booktitle = {Proceedings of the International Conference on Intelligent and Autonomous
    Vehicles, IAV07},
  year = {2007},
  address = {Tolouse, France},
  month = {September},
  file = {:/afs/tu-chemnitz.de/project/proaut/paperDB/2007/bauer07-Comparing_Several_Implementations_of.pdf:PDF}
}
