---
---


@article{rana2021zero,
  title={Zero-Shot Uncertainty-Aware Deployment of Simulation Trained Policies on Real-World Robots},
  author={Rana, Krishan and Dasagi, Vibhavari and Haviland, Jesse and Talbot, Ben and Milford, MIchael and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2112.05299},
  arxiv={2112.05299},
  year={2022},
  abstract={While deep reinforcement learning (RL) agents have demonstrated incredible potential in attaining dexterous behaviours for robotics, they tend to make errors when deployed in the real world due to mismatches between the training and execution environments. In contrast, the classical robotics community have developed a range of controllers that can safely operate across most states in the real world given their explicit derivation. These controllers however lack the dexterity required for complex tasks given limitations in analytical modelling and approximations. In this paper, we propose Bayesian Controller Fusion (BCF), a novel uncertainty-aware deployment strategy that combines the strengths of deep RL policies and traditional handcrafted controllers.},
  link={https://arxiv.org/abs/2112.05299},
  thumb={rana2021zero.png}
}



@article{wilson2021hyperdimensional,
  title={Hyperdimensional Feature Fusion for Out-Of-Distribution Detection},
  author={Wilson, Samuel and S{\"u}nderhauf, Niko and Dayoub, Feras},
  journal={arXiv preprint arXiv:2112.05341},
  year={2021},
  arxiv={2112.05341},
  abstract={We introduce powerful ideas from Hyperdimensional Com-
puting into the challenging field of Out-of-Distribution (OOD) detection.
In contrast to most existing works that perform OOD detection based on
only a single layer of a neural network, we use similarity-preserving semi-
orthogonal projection matrices to project the feature maps from multiple
layers into a common vector space. By repeatedly applying the bundling
operation ⊕, we create expressive class-specific descriptor vectors for all
in-distribution classes. },
  thumb={wilson2021hyperdimensional.png},
  link={https://arxiv.org/abs/2112.05341}
}

@article{rana2021BCF,
  title={Bayesian Controller Fusion: Leveraging Control Priors in Deep Reinforcement Learning for Robotics},
  author={Krishan Rana and Vibhavari Dasagi and Jesse Haviland and Ben Talbot and Michael Milford and Niko S{\"u}nderhauf},
  journal={arXiv preprint arXiv:2107.09822},
  year={2021},
  arxiv={2107.09822},
  link={https://arxiv.org/abs/2107.09822},
  abstract={We present Bayesian Controller Fusion (BCF): a hybrid control strategy that combines the strengths of traditional hand-crafted controllers and model-free deep reinforcement learning (RL). BCF thrives in the robotics domain, where reliable but suboptimal control priors exist for many tasks, but RL from scratch remains unsafe and data-inefficient. By fusing uncertainty-aware distributional outputs from each system, BCF arbitrates control between them, exploiting their respective strengths. 
    As exploration is naturally guided by the prior in the early stages of training, BCF accelerates learning, while substantially improving beyond the performance of the control prior, as the policy gains more experience. 
    More importantly, given the risk-aversity of the control prior, BCF ensures safe exploration \emph{and} deployment, where the control prior naturally dominates the action distribution in states unknown to the policy. 
    We additionally show BCF's applicability to the zero-shot sim-to-real setting and its ability to deal with out-of-distribution states in the real-world. 
    BCF is a promising approach for combining the complementary strengths of deep RL and traditional robotic control, surpassing what either can achieve independently.},
  html={https://krishanrana.github.io/bcf},
  thumb={rana21bcf.png}
}




@article{zhang2020swa,
  title={Swa Object Detection},
  author={Zhang, Haoyang and Wang, Ying and Dayoub, Feras and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2012.12645},
  year={2020},
  arxiv={2012.12645},
  link={https://arxiv.org/abs/2012.12645},
  abstract={Do you want to improve 1.0 AP for your object detector without any inference cost and any change to your detector? It is surprisingly simple: train your detector for an extra 12 epochs using cyclical learning rates and then average these 12 checkpoints as your final detection model. This potent recipe is inspired by Stochastic Weights Averaging (SWA), which is proposed in arXiv:1803.05407 for improving generalization in deep neural networks. We found it also very effective in object detection. In this technique report, we systematically investigate the effects of applying SWA to object detection as well as instance segmentation. Through extensive experiments, we discover the aforementioned workable policy of performing SWA in object detection, and we consistently achieve ∼1.0 AP improvement over various popular detectors on the challenging COCO benchmark, including Mask RCNN, Faster RCNN, RetinaNet, FCOS, YOLOv3 and VFNet. We hope this work will make more researchers in object detection know this technique and help them train better object detectors.},
  html={https://github.com/hyz-xmaster/swa_object_detection}
}

@article{hall2020challenge,
  title={{The Robotic Vision Scene Understanding Challenge}},
  author={Hall, David  and Talbot, Ben and  Bista, Suman Raj and Zhang, Haoyang and Smith, Rohan and Dayoub, Feras and S\"underhauf, Niko},
  journal={arXiv preprint arXiv:2009.05246},
  year={2020},
  arxiv={2009.05246},
  link={https://arxiv.org/abs/2009.05246},
  thumb={hall2020challenge.png},
  abstract={Being able to explore an environment and understand the location and type of all objects therein is important for indoor robotic platforms that must interact closely with humans. However, it is difficult to evaluate progress in this area due to a lack of standardized testing which is limited due to the need for active robot agency and perfect object ground-truth. To help provide a standard for testing scene understanding systems, we present a new robot vision scene understanding challenge using simulation to enable repeatable experiments with active robot agency. We provide two challenging task types, three difficulty levels, five simulated environments and a new evaluation measure for evaluating 3D cuboid object maps.},
  html={https://nikosuenderhauf.github.io/roboticvisionchallenges/scene-understanding}
}



@article{talbot2020benchbot,
  title={{Benchbot: Evaluating robotics research in photorealistic 3d simulation and on real robots}},
  author={Talbot, Ben and Hall, David and Zhang, Haoyang and  Bista, Suman Raj and Smith, Rohan and Dayoub, Feras and S\"underhauf, Niko},
  journal={arXiv preprint arXiv:2008.00635},
  year={2020},
  arxiv={2008.00635},
  link={https://arxiv.org/abs/2008.00635},
  thumb={talbot2020benchbot.png},
  abstract={We introduce BenchBot, a novel software suite for benchmarking the performance of robotics research across both photorealistic 3D simulations and real robot platforms. BenchBot provides a simple interface to the sensorimotor capabilities of a robot when solving robotics research problems; an interface that is consistent regardless of whether the target platform is simulated or a real robot. In this paper we outline the BenchBot system architecture, and explore the parallels between its user-centric design and an ideal research development process devoid of tangential robot engineering challenges.},
  html={http://www.benchbot.org}
}

@article{rahman2020performance,
  title={{Performance Monitoring of Object Detection During Deployment}},
  author={Rahman, Quazi Marufur and S\"underhauf, Niko and Dayoub, Feras},
  journal={arXiv preprint arXiv:2009.08650},
  year={2020},
  arxiv={2009.08650},
  link={https://arxiv.org/abs/2009.08650},
  thumb={rahman2020performance.png},
  abstract={Performance monitoring of object detection is crucial for safety-critical applications such as autonomous vehicles that operate under varying and complex environmental conditions. Currently, object detectors are evaluated using summary metrics based on a single dataset that is assumed to be representative of all future deployment conditions. In practice, this assumption does not hold, and the performance fluctuates as a function of the deployment conditions. To address this issue, we propose an introspection approach to performance monitoring during deployment without the need for ground truth data. We do so by predicting when the per-frame mean average precision drops below a critical threshold using the detector's internal features.},
}


@article{corke2020can,
  title={{What can robotics research learn from computer vision research?}},
  author={Corke, Peter and Dayoub, Feras and Hall, David and Skinner, John and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2001.02366},
  year={2020},
  arxiv={2001.02366},
  link={https://arxiv.org/abs/2001.02366},
  thumb={corke2020research.png},
  abstract={The computer vision and robotics research communities are each strong. However progress in computer vision has become turbo-charged in recent years due to big data, GPU computing, novel learning algorithms and a very effective research methodology. By comparison, progress in robotics seems slower. It is true that robotics came later to exploring the potential of learning -- the advantages over the well-established body of knowledge in dynamics, kinematics, planning and control is still being debated, although reinforcement learning seems to offer real potential. However, the rapid development of computer vision compared to robotics cannot be only attributed to the former's adoption of deep learning. In this paper, we argue that the gains in computer vision are due to research methodology -- evaluation under strict constraints versus experiments; bold numbers versus videos.}
}


@article{suenderhauf19keys,
  title={{Where are the Keys? -- Learning Object-Centric Navigation Policies on Semantic Maps with Graph Convolutional Networks}},
  author={S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:1909.07376},
  year={2019},
  abstract={Emerging object-based SLAM algorithms can build a graph representation of an environment comprising nodes for robot poses and object landmarks. However, while this map will contain static objects such as furniture or appliances, many moveable objects (e.g. the car keys, the glasses, or a magazine), are not suitable as landmarks and will not be part of the map due to their non-static nature. We show that Graph Convolutional Networks can learn navigation policies to find such unmapped objects by learning to exploit the hidden probabilistic model that governs where these objects appear in the environment. The learned policies can generalise to object classes unseen during training by using word vectors that express semantic similarity as representations for object nodes in the graph. Furthermore, we show that the policies generalise to unseen environments with only minimal loss of performance. We demonstrate that pre-training the policy network with a proxy task can significantly speed up learning, improving sample efficiency.},
  link={https://arxiv.org/abs/1909.07376},
  arxiv={1909.07376},
  thumb={suenderhauf19keys.png}
}



@article{Jablonsky18geometric,
  title={{An Orientation Factor for Object-Oriented SLAM}},
  author={Natalie Jablonsky and Michael Milford and Niko S\"underhauf},
  year={2018},
  journal = {{arXiv preprint}},
  abstract = {Current approaches to object-oriented SLAM lack the ability to incorporate prior knowledge of the scene geometry, such as the expected global orientation of objects. We overcome this limitation by proposing a geometric factor that constrains the global orientation of objects in the map, depending on the objects’ semantics. This new geometric factor is a first example of how semantics can inform and improve geometry in object-oriented SLAM. We implement the geometric factor for the recently proposed QuadricSLAM that represents landmarks as dual quadrics. The factor probabilistically models the quadrics’ major axes to be either perpendicular to or aligned with the direction of gravity, depending on their semantic class. Our experiments on simulated and real-world datasets show that using the proposed factors to incorporate prior knowledge improves both the trajectory and landmark quality.},
link={https://arxiv.org/abs/1809.06977},
thumb={jablonsky18geometric.png},
arxiv={1809.06977},
html={http://semanticslam.ai/geometricfactors.html}
}
