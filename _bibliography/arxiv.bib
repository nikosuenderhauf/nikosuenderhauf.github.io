---
---

@article{miller2024open,
  title={Open-Set Recognition in the Age of Vision-Language Models},
  author={Miller, Dimity and S{\"u}nderhauf, Niko and Kenna, Alex and Mason, Keita},
  journal={arXiv preprint arXiv:2403.16528},
  year={2024},
  arxiv={2403.16528},
  link={https://arxiv.org/abs/2403.16528},
  abstract={Are vision-language models (VLMs) open-set models because they are trained on internet-scale datasets? We answer this question with a clear no – VLMs introduce closed-set assumptions via their finite query set, making them vulnerable to open-set conditions. We systematically evaluate VLMs for open-set recognition and find they frequently misclassify objects not contained in their query set, leading to alarmingly low precision when tuned for high recall and vice versa. We show that naively increasing the size of the query set to contain more and more classes does not mitigate this problem, but instead causes diminishing task performance and open-set performance. We establish a revised definition of the open-set problem for the age of VLMs, define a new benchmark and evaluation protocol to facilitate standardised evaluation and research in this important area, and evaluate promising baseline approaches based on predictive uncertainty and dedicated negative embeddings on a range of VLM classifiers and object detectors.},
  thumb={miller2024openset.png}
}


@article{ceola2023lhmanip,
  title={LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks in Cluttered Tabletop Environments},
  author={Ceola, Federico and Natale, Lorenzo and S{\"u}nderhauf, Niko and Rana, Krishan},
  arxiv={2312.12036},
  journal={arXiv preprint arXiv:2312.12036},
  year={2023},
  html={https://github.com/fedeceola/LHManip},
  thumb={ceola2023manip.png},
  abstract={We present the Long-Horizon Manipulation (LHManip) dataset comprising 200
episodes, demonstrating 20 different manipulation tasks via real robot teleoperation. The tasks entail multiple sub-tasks,
including grasping, pushing, stacking and throwing objects in highly cluttered environments. Each task is paired with a
natural language instruction and multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the dataset
comprises 176,278 observation-action pairs which form part of the Open X-Embodiment dataset.},
  link={https://arxiv.org/abs/2312.12036},
}


@article{deitke2022retro,
  title={Retrospectives on the Embodied AI Workshop},
  author={Deitke, Matt and Batra, Dhruv and Bisk, Yonatan and Campari, Tommaso and Chang, Angel X and Chaplot, Devendra Singh and Chen, Changan and D'Arpino, Claudia P{\'e}rez and Ehsani, Kiana and Farhadi, Ali and others},
  journal={arXiv preprint arXiv:2210.06849},
  year={2022},
  arxiv={2210.06849},
  abstract={We present a retrospective on the state of Embodied AI
research. Our analysis focuses on 13 challenges presented
at the Embodied AI Workshop at CVPR. These challenges
are grouped into three themes: (1) visual navigation, (2) rearrangement, and (3) embodied vision-and-language. We
discuss the dominant datasets within each theme, evaluation metrics for the challenges, and the performance of stateof-the-art models. We highlight commonalities between top
approaches to the challenges and identify potential future
directions for Embodied AI research.},
  thumb={deitke2022retro.png},
  link={http://arxiv.org/abs/2210.06849}
}




@article{abouchakra2022implicit,
  title={Implicit Object Mapping With Noisy Data},
  author={Abou-Chakra, Jad and Dayoub, Feras and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2204.10516},
  year={2022},
  arxiv={2204.10516},
  link={https://arxiv.org/abs/2204.10516},
  thumb={abouchakra2022implicit.png},
  abstract={This paper uses the outputs of an object-based SLAM system to bound objects in the scene with coarse primitives and - in concert with instance masks - identify obstructions in the training images. Objects are therefore automatically bounded, and non-relevant geometry is excluded from the NeRF representation. The method's performance is benchmarked under ideal conditions and tested against errors in the poses and instance masks. Our results show that object-based NeRFs are robust to pose variations but sensitive to the quality of the instance masks.}
}



@article{zhang2020swa,
  title={Swa Object Detection},
  author={Zhang, Haoyang and Wang, Ying and Dayoub, Feras and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2012.12645},
  year={2020},
  arxiv={2012.12645},
  link={https://arxiv.org/abs/2012.12645},
  abstract={Do you want to improve 1.0 AP for your object detector without any inference cost and any change to your detector? It is surprisingly simple: train your detector for an extra 12 epochs using cyclical learning rates and then average these 12 checkpoints as your final detection model. This potent recipe is inspired by Stochastic Weights Averaging (SWA), which is proposed in arXiv:1803.05407 for improving generalization in deep neural networks. We found it also very effective in object detection. In this technique report, we systematically investigate the effects of applying SWA to object detection as well as instance segmentation. Through extensive experiments, we discover the aforementioned workable policy of performing SWA in object detection, and we consistently achieve ∼1.0 AP improvement over various popular detectors on the challenging COCO benchmark, including Mask RCNN, Faster RCNN, RetinaNet, FCOS, YOLOv3 and VFNet. We hope this work will make more researchers in object detection know this technique and help them train better object detectors.},
  html={https://github.com/hyz-xmaster/swa_object_detection}
}

@article{hall2020challenge,
  title={{The Robotic Vision Scene Understanding Challenge}},
  author={Hall, David  and Talbot, Ben and  Bista, Suman Raj and Zhang, Haoyang and Smith, Rohan and Dayoub, Feras and S\"underhauf, Niko},
  journal={arXiv preprint arXiv:2009.05246},
  year={2020},
  arxiv={2009.05246},
  link={https://arxiv.org/abs/2009.05246},
  thumb={hall2020challenge.png},
  abstract={Being able to explore an environment and understand the location and type of all objects therein is important for indoor robotic platforms that must interact closely with humans. However, it is difficult to evaluate progress in this area due to a lack of standardized testing which is limited due to the need for active robot agency and perfect object ground-truth. To help provide a standard for testing scene understanding systems, we present a new robot vision scene understanding challenge using simulation to enable repeatable experiments with active robot agency. We provide two challenging task types, three difficulty levels, five simulated environments and a new evaluation measure for evaluating 3D cuboid object maps.},
  html={https://nikosuenderhauf.github.io/roboticvisionchallenges/scene-understanding}
}



@article{talbot2020benchbot,
  title={{Benchbot: Evaluating robotics research in photorealistic 3d simulation and on real robots}},
  author={Talbot, Ben and Hall, David and Zhang, Haoyang and  Bista, Suman Raj and Smith, Rohan and Dayoub, Feras and S\"underhauf, Niko},
  journal={arXiv preprint arXiv:2008.00635},
  year={2020},
  arxiv={2008.00635},
  link={https://arxiv.org/abs/2008.00635},
  thumb={talbot2020benchbot.png},
  abstract={We introduce BenchBot, a novel software suite for benchmarking the performance of robotics research across both photorealistic 3D simulations and real robot platforms. BenchBot provides a simple interface to the sensorimotor capabilities of a robot when solving robotics research problems; an interface that is consistent regardless of whether the target platform is simulated or a real robot. In this paper we outline the BenchBot system architecture, and explore the parallels between its user-centric design and an ideal research development process devoid of tangential robot engineering challenges.},
  html={http://www.benchbot.org}
}

@article{rahman2020performance,
  title={{Performance Monitoring of Object Detection During Deployment}},
  author={Rahman, Quazi Marufur and S\"underhauf, Niko and Dayoub, Feras},
  journal={arXiv preprint arXiv:2009.08650},
  year={2020},
  arxiv={2009.08650},
  link={https://arxiv.org/abs/2009.08650},
  thumb={rahman2020performance.png},
  abstract={Performance monitoring of object detection is crucial for safety-critical applications such as autonomous vehicles that operate under varying and complex environmental conditions. Currently, object detectors are evaluated using summary metrics based on a single dataset that is assumed to be representative of all future deployment conditions. In practice, this assumption does not hold, and the performance fluctuates as a function of the deployment conditions. To address this issue, we propose an introspection approach to performance monitoring during deployment without the need for ground truth data. We do so by predicting when the per-frame mean average precision drops below a critical threshold using the detector's internal features.},
}


@article{corke2020can,
  title={{What can robotics research learn from computer vision research?}},
  author={Corke, Peter and Dayoub, Feras and Hall, David and Skinner, John and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2001.02366},
  year={2020},
  arxiv={2001.02366},
  link={https://arxiv.org/abs/2001.02366},
  thumb={corke2020research.png},
  abstract={The computer vision and robotics research communities are each strong. However progress in computer vision has become turbo-charged in recent years due to big data, GPU computing, novel learning algorithms and a very effective research methodology. By comparison, progress in robotics seems slower. It is true that robotics came later to exploring the potential of learning -- the advantages over the well-established body of knowledge in dynamics, kinematics, planning and control is still being debated, although reinforcement learning seems to offer real potential. However, the rapid development of computer vision compared to robotics cannot be only attributed to the former's adoption of deep learning. In this paper, we argue that the gains in computer vision are due to research methodology -- evaluation under strict constraints versus experiments; bold numbers versus videos.}
}


@article{suenderhauf19keys,
  title={{Where are the Keys? -- Learning Object-Centric Navigation Policies on Semantic Maps with Graph Convolutional Networks}},
  author={S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:1909.07376},
  year={2019},
  abstract={Emerging object-based SLAM algorithms can build a graph representation of an environment comprising nodes for robot poses and object landmarks. However, while this map will contain static objects such as furniture or appliances, many moveable objects (e.g. the car keys, the glasses, or a magazine), are not suitable as landmarks and will not be part of the map due to their non-static nature. We show that Graph Convolutional Networks can learn navigation policies to find such unmapped objects by learning to exploit the hidden probabilistic model that governs where these objects appear in the environment. The learned policies can generalise to object classes unseen during training by using word vectors that express semantic similarity as representations for object nodes in the graph. Furthermore, we show that the policies generalise to unseen environments with only minimal loss of performance. We demonstrate that pre-training the policy network with a proxy task can significantly speed up learning, improving sample efficiency.},
  link={https://arxiv.org/abs/1909.07376},
  arxiv={1909.07376},
  thumb={suenderhauf19keys.png}
}



@article{Jablonsky18geometric,
  title={{An Orientation Factor for Object-Oriented SLAM}},
  author={Natalie Jablonsky and Michael Milford and Niko S\"underhauf},
  year={2018},
  journal = {{arXiv preprint}},
  abstract = {Current approaches to object-oriented SLAM lack the ability to incorporate prior knowledge of the scene geometry, such as the expected global orientation of objects. We overcome this limitation by proposing a geometric factor that constrains the global orientation of objects in the map, depending on the objects’ semantics. This new geometric factor is a first example of how semantics can inform and improve geometry in object-oriented SLAM. We implement the geometric factor for the recently proposed QuadricSLAM that represents landmarks as dual quadrics. The factor probabilistically models the quadrics’ major axes to be either perpendicular to or aligned with the direction of gravity, depending on their semantic class. Our experiments on simulated and real-world datasets show that using the proposed factors to incorporate prior knowledge improves both the trajectory and landmark quality.},
link={https://arxiv.org/abs/1809.06977},
thumb={jablonsky18geometric.png},
arxiv={1809.06977},
html={http://semanticslam.ai/geometricfactors.html}
}
