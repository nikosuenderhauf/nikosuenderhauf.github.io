---
---

@article{jayanga2024multi,
  title={Multi-View Pose-Agnostic Change Localization with Zero Labels},
  author={Jayanga Galappaththige, Chamuditha and Lai, Jason and Windrim, Lloyd and Dansereau, Donald and Suenderhauf, Niko and Miller, Dimity},
  journal={arXiv preprint arXiv:2412.03911},  
  arxiv={2412.03911},
  link={https://arxiv.org/abs/2412.03911},
  year={2024},
  abstract={We propose a novel label-free, pose-agnostic change detection method that integrates information from multiple viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS) representation of the scene. With as few as 5 images of the post-change scene, our approach can learn additional change channels in a 3DGS and produce change masks that outperform single-view techniques. Our change-aware 3D scene representation additionally enables the generation of accurate change masks for unseen viewpoints. Experimental results demonstrate state-of-the-art performance in complex multi-object scenes, achieving a 1.7 and 1.6 improvement in Mean Intersection Over Union and F1 score respectively over other baselines. We also contribute a new real-world dataset to benchmark change detection in diverse challenging scenes in the presence of lighting variations.},
  thumb={chamu2024multi.png}
}

@article{raine2024reducing,
  title={Reducing Label Dependency for Underwater Scene Understanding: A Survey of Datasets, Techniques and Applications},
  author={Raine, Scarlett and Maire, Frederic and Suenderhauf, Niko and Fischer, Tobias},
  journal={arXiv preprint arXiv:2411.11287},
  year={2024},
  arxiv={2411.11287},
  link={https://arxiv.org/abs/2411.11287},
  thumb={raine2024reducing.png},
  abstract={The complexity of underwater images, coupled with the specialist expertise needed to accurately identify species at the pixel level, makes the labeling process costly, time-consuming, and heavily dependent on domain experts. In recent years, some works have performed automated analysis of underwater imagery, and a smaller number of studies have focused on weakly supervised approaches which aim to reduce the expert-provided labelled data required. This survey focuses on approaches which reduce dependency on human expert input, while reviewing the prior and related approaches to position these works in the wider field of underwater perception. Further, we offer an overview of coastal ecosystems and the challenges of underwater imagery.}
}

@article{van2024open,
  title={Open-Vocabulary Part-Based Grasping},
  author={van Oort, Tjeard and Miller, Dimity and Browne, Will N and Marticorena, Nicolas and Haviland, Jesse and Suenderhauf, Niko},
  journal={arXiv preprint arXiv:2406.05951},
  year={2024},
  arxiv={2406.05951},
  link={https://arxiv.org/abs/2406.05951},
  thumb={van2024open.png},
  abstract={Many robotic applications require to grasp objects
not arbitrarily but at a very specific object part. This is especially
important for manipulation tasks beyond simple pick-and-place
scenarios or in robot-human interactions, such as object handovers. We propose AnyPart, a practical system that combines
open-vocabulary object detection, open-vocabulary part segmentation and 6DOF grasp pose prediction to infer a grasp pose on
a specific part of an object in 800 milliseconds. We contribute
two new datasets for the task of open-vocabulary part-based
grasping, a hand-segmented dataset containing 1014 object-part
segmentations, and a dataset of real-world scenarios gathered
during our robot trials for individual objects and table-clearing
tasks. We evaluate AnyPart on a mobile manipulator robot using
a set of 28 common household objects over 360 grasping trials.
AnyPart is capable of producing successful grasps 69.52 %, when
ignoring robot-based grasp failures, AnyPart predicts a grasp
location on the correct part 88.57% of the time.}
}


@article{marticorena2024rmmi,
  title={RMMI: Enhanced Obstacle Avoidance for Reactive Mobile Manipulation using an Implicit Neural Map},
  author={Marticorena, Nicolas and Fischer, Tobias and Haviland, Jesse and Suenderhauf, Niko},
  journal={arXiv preprint arXiv:2408.16206},
  year={2024},
  arxiv={2408.16206},
  link={https://arxiv.org/abs/2408.16206},
  html={https://rmmi.github.io/},
  thumb={marticorena2024rmmi.png},
  abstract={We introduce a novel reactive control framework for mobile manipulators operating in complex, static environments. Our approach leverages a neural Signed Distance Field (SDF) to model intricate environment details and incorporates this representation as inequality constraints within a Quadratic Program (QP) to coordinate robot joint and base motion. A key contribution is the introduction of an active collision avoidance cost term that maximises the total robot distance to obstacles during the motion. We first evaluate our approach in a simulated reaching task, outperforming previous methods that rely on representing both the robot and the scene as a set of primitive geometries. Compared with the baseline, we improved the task success rate by 25% in total, which includes increases of 10% by using the active collision cost. We also demonstrate our approach on a real-world platform, showing its effectiveness in reaching target poses in cluttered and confined spaces using environment models built directly from sensor data}
}



@article{ceola2023lhmanip,
  title={LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks in Cluttered Tabletop Environments},
  author={Ceola, Federico and Natale, Lorenzo and S{\"u}nderhauf, Niko and Rana, Krishan},
  arxiv={2312.12036},
  journal={arXiv preprint arXiv:2312.12036},
  year={2023},
  html={https://github.com/fedeceola/LHManip},
  thumb={ceola2023manip.png},
  abstract={We present the Long-Horizon Manipulation (LHManip) dataset comprising 200
episodes, demonstrating 20 different manipulation tasks via real robot teleoperation. The tasks entail multiple sub-tasks,
including grasping, pushing, stacking and throwing objects in highly cluttered environments. Each task is paired with a
natural language instruction and multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the dataset
comprises 176,278 observation-action pairs which form part of the Open X-Embodiment dataset.},
  link={https://arxiv.org/abs/2312.12036},
}


@article{deitke2022retro,
  title={Retrospectives on the Embodied AI Workshop},
  author={Deitke, Matt and Batra, Dhruv and Bisk, Yonatan and Campari, Tommaso and Chang, Angel X and Chaplot, Devendra Singh and Chen, Changan and D'Arpino, Claudia P{\'e}rez and Ehsani, Kiana and Farhadi, Ali and others},
  journal={arXiv preprint arXiv:2210.06849},
  year={2022},
  arxiv={2210.06849},
  abstract={We present a retrospective on the state of Embodied AI
research. Our analysis focuses on 13 challenges presented
at the Embodied AI Workshop at CVPR. These challenges
are grouped into three themes: (1) visual navigation, (2) rearrangement, and (3) embodied vision-and-language. We
discuss the dominant datasets within each theme, evaluation metrics for the challenges, and the performance of stateof-the-art models. We highlight commonalities between top
approaches to the challenges and identify potential future
directions for Embodied AI research.},
  thumb={deitke2022retro.png},
  link={http://arxiv.org/abs/2210.06849}
}


@article{zhang2020swa,
  title={Swa Object Detection},
  author={Zhang, Haoyang and Wang, Ying and Dayoub, Feras and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2012.12645},
  year={2020},
  arxiv={2012.12645},
  link={https://arxiv.org/abs/2012.12645},
  abstract={Do you want to improve 1.0 AP for your object detector without any inference cost and any change to your detector? It is surprisingly simple: train your detector for an extra 12 epochs using cyclical learning rates and then average these 12 checkpoints as your final detection model. This potent recipe is inspired by Stochastic Weights Averaging (SWA), which is proposed in arXiv:1803.05407 for improving generalization in deep neural networks. We found it also very effective in object detection. In this technique report, we systematically investigate the effects of applying SWA to object detection as well as instance segmentation. Through extensive experiments, we discover the aforementioned workable policy of performing SWA in object detection, and we consistently achieve ∼1.0 AP improvement over various popular detectors on the challenging COCO benchmark, including Mask RCNN, Faster RCNN, RetinaNet, FCOS, YOLOv3 and VFNet. We hope this work will make more researchers in object detection know this technique and help them train better object detectors.},
  html={https://github.com/hyz-xmaster/swa_object_detection}
}

@article{hall2020challenge,
  title={{The Robotic Vision Scene Understanding Challenge}},
  author={Hall, David  and Talbot, Ben and  Bista, Suman Raj and Zhang, Haoyang and Smith, Rohan and Dayoub, Feras and S\"underhauf, Niko},
  journal={arXiv preprint arXiv:2009.05246},
  year={2020},
  arxiv={2009.05246},
  link={https://arxiv.org/abs/2009.05246},
  thumb={hall2020challenge.png},
  abstract={Being able to explore an environment and understand the location and type of all objects therein is important for indoor robotic platforms that must interact closely with humans. However, it is difficult to evaluate progress in this area due to a lack of standardized testing which is limited due to the need for active robot agency and perfect object ground-truth. To help provide a standard for testing scene understanding systems, we present a new robot vision scene understanding challenge using simulation to enable repeatable experiments with active robot agency. We provide two challenging task types, three difficulty levels, five simulated environments and a new evaluation measure for evaluating 3D cuboid object maps.},
  html={https://nikosuenderhauf.github.io/roboticvisionchallenges/scene-understanding}
}



@article{talbot2020benchbot,
  title={{Benchbot: Evaluating robotics research in photorealistic 3d simulation and on real robots}},
  author={Talbot, Ben and Hall, David and Zhang, Haoyang and  Bista, Suman Raj and Smith, Rohan and Dayoub, Feras and S\"underhauf, Niko},
  journal={arXiv preprint arXiv:2008.00635},
  year={2020},
  arxiv={2008.00635},
  link={https://arxiv.org/abs/2008.00635},
  thumb={talbot2020benchbot.png},
  abstract={We introduce BenchBot, a novel software suite for benchmarking the performance of robotics research across both photorealistic 3D simulations and real robot platforms. BenchBot provides a simple interface to the sensorimotor capabilities of a robot when solving robotics research problems; an interface that is consistent regardless of whether the target platform is simulated or a real robot. In this paper we outline the BenchBot system architecture, and explore the parallels between its user-centric design and an ideal research development process devoid of tangential robot engineering challenges.},
  html={http://www.benchbot.org}
}

@article{rahman2020performance,
  title={{Performance Monitoring of Object Detection During Deployment}},
  author={Rahman, Quazi Marufur and S\"underhauf, Niko and Dayoub, Feras},
  journal={arXiv preprint arXiv:2009.08650},
  year={2020},
  arxiv={2009.08650},
  link={https://arxiv.org/abs/2009.08650},
  thumb={rahman2020performance.png},
  abstract={Performance monitoring of object detection is crucial for safety-critical applications such as autonomous vehicles that operate under varying and complex environmental conditions. Currently, object detectors are evaluated using summary metrics based on a single dataset that is assumed to be representative of all future deployment conditions. In practice, this assumption does not hold, and the performance fluctuates as a function of the deployment conditions. To address this issue, we propose an introspection approach to performance monitoring during deployment without the need for ground truth data. We do so by predicting when the per-frame mean average precision drops below a critical threshold using the detector's internal features.},
}


@article{corke2020can,
  title={{What can robotics research learn from computer vision research?}},
  author={Corke, Peter and Dayoub, Feras and Hall, David and Skinner, John and S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:2001.02366},
  year={2020},
  arxiv={2001.02366},
  link={https://arxiv.org/abs/2001.02366},
  thumb={corke2020research.png},
  abstract={The computer vision and robotics research communities are each strong. However progress in computer vision has become turbo-charged in recent years due to big data, GPU computing, novel learning algorithms and a very effective research methodology. By comparison, progress in robotics seems slower. It is true that robotics came later to exploring the potential of learning -- the advantages over the well-established body of knowledge in dynamics, kinematics, planning and control is still being debated, although reinforcement learning seems to offer real potential. However, the rapid development of computer vision compared to robotics cannot be only attributed to the former's adoption of deep learning. In this paper, we argue that the gains in computer vision are due to research methodology -- evaluation under strict constraints versus experiments; bold numbers versus videos.}
}


@article{suenderhauf19keys,
  title={{Where are the Keys? -- Learning Object-Centric Navigation Policies on Semantic Maps with Graph Convolutional Networks}},
  author={S{\"u}nderhauf, Niko},
  journal={arXiv preprint arXiv:1909.07376},
  year={2019},
  abstract={Emerging object-based SLAM algorithms can build a graph representation of an environment comprising nodes for robot poses and object landmarks. However, while this map will contain static objects such as furniture or appliances, many moveable objects (e.g. the car keys, the glasses, or a magazine), are not suitable as landmarks and will not be part of the map due to their non-static nature. We show that Graph Convolutional Networks can learn navigation policies to find such unmapped objects by learning to exploit the hidden probabilistic model that governs where these objects appear in the environment. The learned policies can generalise to object classes unseen during training by using word vectors that express semantic similarity as representations for object nodes in the graph. Furthermore, we show that the policies generalise to unseen environments with only minimal loss of performance. We demonstrate that pre-training the policy network with a proxy task can significantly speed up learning, improving sample efficiency.},
  link={https://arxiv.org/abs/1909.07376},
  arxiv={1909.07376},
  thumb={suenderhauf19keys.png}
}



@article{Jablonsky18geometric,
  title={{An Orientation Factor for Object-Oriented SLAM}},
  author={Natalie Jablonsky and Michael Milford and Niko S\"underhauf},
  year={2018},
  journal = {{arXiv preprint}},
  abstract = {Current approaches to object-oriented SLAM lack the ability to incorporate prior knowledge of the scene geometry, such as the expected global orientation of objects. We overcome this limitation by proposing a geometric factor that constrains the global orientation of objects in the map, depending on the objects’ semantics. This new geometric factor is a first example of how semantics can inform and improve geometry in object-oriented SLAM. We implement the geometric factor for the recently proposed QuadricSLAM that represents landmarks as dual quadrics. The factor probabilistically models the quadrics’ major axes to be either perpendicular to or aligned with the direction of gravity, depending on their semantic class. Our experiments on simulated and real-world datasets show that using the proposed factors to incorporate prior knowledge improves both the trajectory and landmark quality.},
link={https://arxiv.org/abs/1809.06977},
thumb={jablonsky18geometric.png},
arxiv={1809.06977},
html={http://semanticslam.ai/geometricfactors.html}
}
