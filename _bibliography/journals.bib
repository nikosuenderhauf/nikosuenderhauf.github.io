


@article{haviland2022holistic,
  title={A Holistic Approach to Reactive Mobile Manipulation},
  author={Haviland, Jesse and S\"underhauf, Niko and Corke, Peter},
  journal={IEEE Robotics and Automation Letters},
  year={2022},
  publisher={IEEE},
  arxiv={2109.04749},
  link={https://ieeexplore.ieee.org/document/9695298/},
  abstract={We present the design and implementation of a taskable reactive mobile manipulation system. In contrary to related work, we treat the arm and base degrees of freedom as a holistic structure which greatly improves the speed and fluidity of the resulting motion. At the core of this approach is a robust and reactive motion controller which can achieve a desired end-effector pose, while avoiding joint position and velocity limits, and ensuring the mobile manipulator is manoeuvrable throughout the trajectory. This can support sensor-based behaviours such as closed-loop visual grasping. As no planning is involved in our approach, the robot is never stationary thinking about what to do next. We show the versatility of our holistic motion controller by implementing a pick and place system using behaviour trees and demonstrate this task on a 9-degree-of-freedom mobile manipulator. },
  html={http://jhavl.github.io/holistic},
  thumb={haviland2022holistic.png}
}

@article{hall2022benchbot,
  title={BenchBot environments for active robotics (BEAR): Simulated data for active scene understanding research},
  author={Hall, David and Talbot, Ben and Bista, Suman Raj and Zhang, Haoyang and Smith, Rohan and Dayoub, Feras and S{\"u}nderhauf, Niko},
  journal={The International Journal of Robotics Research},
  pages={02783649211069404},
  year={2022},  
  publisher={SAGE Publications Sage UK: London, England},
  link={https://journals.sagepub.com/doi/abs/10.1177/02783649211069404},
  html={https://qcr.github.io/dataset/benchbot-bear-data/},
  thumb={hall2022benchbot.png},
  abstract={We present a platform to foster research in active scene understanding, consisting of high-fidelity simulated environments and a simple yet powerful API that controls a mobile robot in simulation and reality. In contrast to static, pre-recorded datasets that focus on the perception aspect of scene understanding, agency is a top priority in our work. We provide three levels of robot agency, allowing users to control a robot at varying levels of difficulty and realism. While the most basic level provides pre-defined trajectories and ground-truth localisation, the more realistic levels allow us to evaluate integrated behaviours comprising perception, navigation, exploration and SLAM. }
}

@article{rahman2021fsnet,
  title={{FSNet: A Failure Detection Framework for Semantic Segmentation}},
  author={Rahman, Quazi Marufur and S\"underhauf, Niko and Corke, Peter and Dayoub, Feras},
  journal={IEEE Robotics and Automation Letters},
  year={2022},
  arxiv={2108.08748},
  link={https://ieeexplore.ieee.org/document/9682519/},
  thumb={rahman2021fsnet.png},
  abstract={Semantic segmentation is an important task that helps autonomous vehicles understand their surroundings and navigate safely. However, during deployment, even the most mature segmentation models are vulnerable to various external factors that can degrade the segmentation performance with potentially catastrophic consequences for the vehicle and its surroundings. To address this issue, we propose a failure detection framework to identify pixel-level misclassification. We do so by exploiting internal features of the segmentation model and training it simultaneously with a failure detection network. During deployment, the failure detector flags areas in the image where the segmentation model has failed to segment correctly.},
}


@article{miller2021gmmdet,
  title={Uncertainty for Identifying Open-Set Errors in Visual Object Detection},
  author={Miller, Dimity and S{\"u}nderhauf, Niko and Milford, Michael and Dayoub, Feras},  
  abstract={We propose GMM-Det, a real-time method for extracting
epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a
structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified
by their low log-probability under all Gaussian Mixture Models.
We test two common detector architectures, Faster R-CNN and
RetinaNet, across three varied datasets spanning robotics and
computer vision. Our results show that GMM-Det consistently
outperforms existing uncertainty techniques for identifying and
rejecting open-set detections, especially at the low-error-rate
operating point required for safety-critical applications. GMMDet maintains object detection performance, and introduces
only minimal computational overhead.},
  year={2021},
  journal={{IEEE Robotics and Automation Letters (RA-L)}},
  link={https://doi.org/10.1109/LRA.2021.3123374},
  arxiv={2104.01328},
  thumb={miller21GMMDet.png},
  link={http://arxiv.org/abs/2104.01328}
}

@article{garg2021semantics,
  title={Semantics for robotic mapping, perception and interaction: A survey},
  author={Garg, Sourav and S{\"u}nderhauf, Niko and Dayoub, Feras and Morrison, Douglas and Cosgun, Akansel and Carneiro, Gustavo and Wu, Qi and Chin, Tat-Jun and Reid, Ian and Gould, Stephen and others},
  journal={Foundations and Trends in Robotics},
  year={2021},
  arxiv={2101.00443},
  link={https://doi.org/10.1561/2300000059},
  thumb={garg21semantics.png},
  abstract={This survey provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity, in which semantics are extracted, used, or both. Within these broad categories we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics, including mapping, navigation and interaction with the world. The survey also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where semantics is or is likely to play a key role. In creating this survey, we hope to provide researchers across academia and industry with a comprehensive reference that helps facilitate future research in this exciting field.},
}


@article{ming2021topometric,
  title={{Probabilistic Appearance-Invariant Topometric Localization with New Place Awareness}},
  author={Xu, Ming and Fischer, Tobias and Sünderhauf, Niko and Milford, Michael},
  journal={{IEEE Robotics and Automation Letters (RA-L)}},
  year={2021},
  thumb={ming21topometric.png},
  abstract={We present a new probabilistic topometric localization system which incorporates full 3-dof odometry into the motion model and furthermore, adds an “off-map” state within the state-estimation framework, allowing query traverses which feature significant route detours from the reference map to be successfully localized. We perform extensive evaluation on multiple query traverses from the Oxford RobotCar dataset exhibiting both significant appearance change and deviations from routes previously traversed.},
  link={https://ieeexplore.ieee.org/abstract/document/9484728},
  arxiv={2107.07707}
}


@article{ming2021probabilistic,
  title={{Probabilistic Visual Place Recognition for Hierarchical Localization}},
  author={Xu, Ming and Sünderhauf, Niko and Milford, Michael},
  journal={{IEEE Robotics and Automation Letters (RA-L)}},
  year={2021},
  thumb={ming2021probabilistic.png},
  abstract={We propose two methods which adapt image retrieval techniques used for visual place recognition to the Bayesian state estimation formulation for localization. We demonstrate significant improvements to the
  localization accuracy of the coarse localization stage using our methods, whilst retaining state-of-the-art performance under severe appearance change. Using extensive experimentation on the Oxford RobotCar dataset, results show that our approach
  outperforms comparable state-of-the-art methods in terms of precision-recall performance for localizing image sequences. In addition, our proposed methods provides the flexibility
  to contextually scale localization latency in order to achieve these improvements.},
  link={https://ieeexplore.ieee.org/abstract/document/9268070/},
  arxiv={2105.03091}
}

@article{suenderhauf2019probabilistic,
  title={A Probabilistic Challenge for Object Detection},
  author={S{\"u}nderhauf, Niko and Dayoub, Feras and Hall, David and Skinner, John and Zhang, Haoyang and Carneiro, Gustavo and Corke, Peter},
  journal={Nature Machine Intelligence},
  volume={1},
  number={9},
  pages={443--443},
  year={2019},
  publisher={Nature Research},
  link={https://rdcu.be/bQR84},
  thumb={suenderhauf19probabilistic.png},
  abstract={To safely operate in the real world, robots need to evaluate how confident they are about what they see.
  A new competition challenges computer vision algorithms to not just detect and localize objects, but also report how certain they are.
  To this end, we introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections.}
}


@article{garg2019semantic,
  title={Semantic--geometric visual place recognition: a new perspective for reconciling opposing views},
  author={Garg, Sourav and S{\"u}nderhauf, Niko and Milford, Michael},
  journal={The International Journal of Robotics Research (IJRR)},
  pages={0278364919839761},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England},
  link={https://doi.org/10.1177%2F0278364919839761},
  abstract={We propose a hybrid image descriptor that semantically aggregates salient visual information, complemented by appearance-based description, and augment a conventional coarse-to-fine recognition pipeline with keypoint correspondences extracted from within the convolutional feature maps of a pre-trained network. Finally, we introduce descriptor normalization and local score enhancement strategies for improving the robustness of the system. Using both existing benchmark datasets and extensive new datasets that for the first time combine the three challenges of opposing viewpoints, lateral viewpoint shifts, and extreme appearance change, we show that our system can achieve practical place recognition performance where existing state-of-the-art methods fail.},
  thumb={garg18rss.png}
}


@article{nicholson18quadricslam,
  title={{QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Object-oriented SLAM}},
  author={Nicholson, Lachlan and Milford, Michael and S{\"u}nderhauf, Niko},
  year={2018},
  journal={IEEE Robotics and Automation Letters (RA-L)},
  year={2018},
  publisher={IEEE},
  link = {https://ieeexplore.ieee.org/document/8440105/},
  abstract = {In this paper, we use 2D object detections from multiple views to simultaneously estimate a 3D quadric surface for each object and localize the camera position. We derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D object detections can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for object detectors that addresses the challenge of partially visible objects, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.},
  thumb={nicholson18quadricslam.png},
  html={http://semanticslam.ai/quadricslam.html},
  arxiv={1804.04011}
}

@article{sunderhauf2018limits,
  title={The Limits and Potentials of Deep Learning for Robotics},
  author={S{\"u}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"u}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and others},
  journal={The International Journal of Robotics Research},
  volume={37},
  number={4-5},
  pages={405--420},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England},
  abstract={The application of deep learning in robotics leads
to very specific problems and research questions that are
typically not addressed by the computer vision and machine
learning communities. In this paper we discuss a number
of robotics-specific learning, reasoning, and embodiment challenges
for deep learning. We explain the need for better evaluation
metrics, highlight the importance and unique challenges for
deep robotic learning in simulation, and explore the spectrum
between purely data-driven and model-driven approaches. We
hope this paper provides a motivating overview of important
research directions to overcome the current limitations, and
help fulfill the promising potentials of deep learning in robotics.},
  thumb={suenderhauf18limitsandpotentials.png},
  link={http://journals.sagepub.com/doi/abs/10.1177/0278364918770733},
  arxiv={1804.06557}
}


@article{McMahon17,
  title={{Multi-Modal Trip Hazard Affordance Detection On Construction Sites}},
  author={McMahon, Sean and S\"underhauf, Niko and Upcroft, Ben and Milford, Michael J},
  journal={IEEE Robotics and Automation Letters (RA-L)},
  year={2017},
  publisher={IEEE},
  abstract={Trip hazards are a significant contributor to accidents on construction and manufacturing sites. We conduct a comprehensive investigation into the performance characteristics of 11 different colors and depth fusion approaches, including four fusion and one nonfusion approach, using color and two types of depth images. Trained and tested on more than 600 labeled trip hazards over four floors and 2000 m2 in an active construction site, this approach was able to differentiate between identical objects in different physical configurations. Outperforming a color-only detector, our multimodal trip detector fuses color and depth information to achieve a 4% absolute improvement in F1-score. These investigative results and the extensive publicly available dataset move us one step closer to assistive or fully automated safety inspection systems on construction sites.},
  link={https://ieeexplore.ieee.org/document/7959072/},
  thumb={mcmahon17hazards.png}
}


@article{Lowry15,
  title={{Visual Place Recognition: A Survey}},
  author={Lowry, Stephanie and S\"underhauf, Niko and Newman, Paul and Leonard, John J and Cox, David and Corke, Peter and Milford, Michael J},
  journal={Transactions on Robotics (TRO)},
  year={2015},
  publisher={IEEE},
  link={/assets/papers/visual_place_recognition_a_survey.pdf},
  thumb={lowry15survey.png},
  abstract={This paper presents a survey of the visual place
recognition research landscape. We start by introducing the
concepts behind place recognition – the role of place recognition
in the animal kingdom, how a “place” is defined in a robotics
context, and the major components of a place recognition system.
We then survey visual place recognition solutions for
environments where appearance change is assumed to be
negligible. Long term robot operations have revealed that
environments continually change; consequently we survey place
recognition solutions that implicitly or explicitly account for
appearance change within the environment. Finally we close with
a discussion of the future of visual place recognition, in particular
with respect to the rapid advances being made in the related
fields of deep learning, semantic scene understanding and video
description.}
}

@article{neubert2015superpixel,
  title={Superpixel-based appearance change prediction for long-term navigation across seasons},
  author={Neubert, Peer and S{\"u}nderhauf, Niko and Protzel, Peter},
  journal={Robotics and Autonomous Systems},
  volume={69},
  pages={15--27},
  year={2015},
  publisher={Elsevier},
  link={/assets/papers/ACP_RAS.pdf},
  thumb={neubert15change.png},
  abstract={The goal of our work is to support
existing approaches to place recognition by learning how the
visual appearance of an environment changes over time and by
using this learned knowledge to predict its appearance under
different environmental conditions. We describe the general
idea of appearance change prediction (ACP) and investigate
properties of our novel implementation based on vocabularies
of superpixels (SP-ACP). This paper deepens the
understanding of the proposed SP-ACP system and evaluates
the influence of its parameters. We present the results of a largescale
experiment on the complete 10 hour Nordland dataset and
appearance change predictions between different combinations
of seasons.}
}

@article{Suenderhauf10,
  author = {S{\"u}nderhauf, Niko and Protzel, Peter},
  title = {{Learning from Nature: Biologically Inspired Robot Navigation and
    SLAM -- A Review}},
  booktitle = {K{\"u}nstliche Intelligenz (German Journal on Artificial Intelligence),
    Special Issue on SLAM},
  publisher = {Springer Verlag},
  year = {2010},
  address = {Heidelberg},
  journal = {K\"unstliche Intelligenz (German Journal on Artificial Intelligence), Special Issue on SLAM},
  owner = {niko},
  timestamp = {2010.04.14},
  link={/assets/papers/suenderhauf10review.pdf},
  abstract={In this paper we summarize the most important
neuronal fundamentals of navigation in rodents, primates
and humans. We review a number of brain cells that are
involved in spatial navigation and their properties. Furthermore,
we review RatSLAM, a working SLAM system that is
partially inspired by neuronal mechanisms underlying mammalian
spatial navigation.},
thumb={suenderhauf10review.png}
}
