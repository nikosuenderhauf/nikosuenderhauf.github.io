@article{ming2021probabilistic,
  title={{Probabilistic Visual Place Recognition for Hierarchical Localization}},
  author={Xu, Ming and Sünderhauf, Niko and Milford, Michael},
  journal={{IEEE Robotics and Automation Letters (RA-L)}},
  year={2021},
  thumb={ming2021probabilistic.png},
  abstract={We propose two methods which adapt image retrieval techniques used for visual place recognition to the Bayesian state estimation formulation for localization. We demonstrate significant improvements to the
  localization accuracy of the coarse localization stage using our methods, whilst retaining state-of-the-art performance under severe appearance change. Using extensive experimentation on the Oxford RobotCar dataset, results show that our approach
  outperforms comparable state-of-the-art methods in terms of precision-recall performance for localizing image sequences. In addition, our proposed methods provides the flexibility
  to contextually scale localization latency in order to achieve these improvements.},
  link={https://ieeexplore.ieee.org/abstract/document/9268070/}
}

@article{suenderhauf2019probabilistic,
  title={A Probabilistic Challenge for Object Detection},
  author={S{\"u}nderhauf, Niko and Dayoub, Feras and Hall, David and Skinner, John and Zhang, Haoyang and Carneiro, Gustavo and Corke, Peter},
  journal={Nature Machine Intelligence},
  volume={1},
  number={9},
  pages={443--443},
  year={2019},
  publisher={Nature Research},
  link={https://rdcu.be/bQR84},
  thumb={suenderhauf19probabilistic.png},
  abstract={To safely operate in the real world, robots need to evaluate how confident they are about what they see.
  A new competition challenges computer vision algorithms to not just detect and localize objects, but also report how certain they are.
  To this end, we introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections.}
}


@article{garg2019semantic,
  title={Semantic--geometric visual place recognition: a new perspective for reconciling opposing views},
  author={Garg, Sourav and S{\"u}nderhauf, Niko and Milford, Michael},
  journal={The International Journal of Robotics Research (IJRR)},
  pages={0278364919839761},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England},
  link={https://doi.org/10.1177%2F0278364919839761},
  abstract={We propose a hybrid image descriptor that semantically aggregates salient visual information, complemented by appearance-based description, and augment a conventional coarse-to-fine recognition pipeline with keypoint correspondences extracted from within the convolutional feature maps of a pre-trained network. Finally, we introduce descriptor normalization and local score enhancement strategies for improving the robustness of the system. Using both existing benchmark datasets and extensive new datasets that for the first time combine the three challenges of opposing viewpoints, lateral viewpoint shifts, and extreme appearance change, we show that our system can achieve practical place recognition performance where existing state-of-the-art methods fail.},
  thumb={garg18rss.png}
}


@article{nicholson18quadricslam,
  title={{QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Object-oriented SLAM}},
  author={Nicholson, Lachlan and Milford, Michael and S{\"u}nderhauf, Niko},
  year={2018},
  journal={IEEE Robotics and Automation Letters (RA-L)},
  year={2018},
  publisher={IEEE},
  link = {https://ieeexplore.ieee.org/document/8440105/},
  abstract = {In this paper, we use 2D object detections from multiple views to simultaneously estimate a 3D quadric surface for each object and localize the camera position. We derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D object detections can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for object detectors that addresses the challenge of partially visible objects, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.},
  thumb={nicholson18quadricslam.png},
  html={http://semanticslam.ai/quadricslam.html},
  arxiv={1804.04011}
}

@article{sunderhauf2018limits,
  title={The Limits and Potentials of Deep Learning for Robotics},
  author={S{\"u}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"u}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and others},
  journal={The International Journal of Robotics Research},
  volume={37},
  number={4-5},
  pages={405--420},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England},
  abstract={The application of deep learning in robotics leads
to very specific problems and research questions that are
typically not addressed by the computer vision and machine
learning communities. In this paper we discuss a number
of robotics-specific learning, reasoning, and embodiment challenges
for deep learning. We explain the need for better evaluation
metrics, highlight the importance and unique challenges for
deep robotic learning in simulation, and explore the spectrum
between purely data-driven and model-driven approaches. We
hope this paper provides a motivating overview of important
research directions to overcome the current limitations, and
help fulfill the promising potentials of deep learning in robotics.},
  thumb={suenderhauf18limitsandpotentials.png},
  link={http://journals.sagepub.com/doi/abs/10.1177/0278364918770733},
  arxiv={1804.06557}
}


@article{McMahon17,
  title={{Multi-Modal Trip Hazard Affordance Detection On Construction Sites}},
  author={McMahon, Sean and S\"underhauf, Niko and Upcroft, Ben and Milford, Michael J},
  journal={IEEE Robotics and Automation Letters (RA-L)},
  year={2017},
  publisher={IEEE},
  abstract={Trip hazards are a significant contributor to accidents on construction and manufacturing sites. We conduct a comprehensive investigation into the performance characteristics of 11 different colors and depth fusion approaches, including four fusion and one nonfusion approach, using color and two types of depth images. Trained and tested on more than 600 labeled trip hazards over four floors and 2000 m2 in an active construction site, this approach was able to differentiate between identical objects in different physical configurations. Outperforming a color-only detector, our multimodal trip detector fuses color and depth information to achieve a 4% absolute improvement in F1-score. These investigative results and the extensive publicly available dataset move us one step closer to assistive or fully automated safety inspection systems on construction sites.},
  link={https://ieeexplore.ieee.org/document/7959072/},
  thumb={mcmahon17hazards.png}
}


@article{Lowry15,
  title={{Visual Place Recognition: A Survey}},
  author={Lowry, Stephanie and S\"underhauf, Niko and Newman, Paul and Leonard, John J and Cox, David and Corke, Peter and Milford, Michael J},
  journal={Transactions on Robotics (TRO)},
  year={2015},
  publisher={IEEE},
  link={/assets/papers/visual_place_recognition_a_survey.pdf},
  thumb={lowry15survey.png},
  abstract={This paper presents a survey of the visual place
recognition research landscape. We start by introducing the
concepts behind place recognition – the role of place recognition
in the animal kingdom, how a “place” is defined in a robotics
context, and the major components of a place recognition system.
We then survey visual place recognition solutions for
environments where appearance change is assumed to be
negligible. Long term robot operations have revealed that
environments continually change; consequently we survey place
recognition solutions that implicitly or explicitly account for
appearance change within the environment. Finally we close with
a discussion of the future of visual place recognition, in particular
with respect to the rapid advances being made in the related
fields of deep learning, semantic scene understanding and video
description.}
}

@article{neubert2015superpixel,
  title={Superpixel-based appearance change prediction for long-term navigation across seasons},
  author={Neubert, Peer and S{\"u}nderhauf, Niko and Protzel, Peter},
  journal={Robotics and Autonomous Systems},
  volume={69},
  pages={15--27},
  year={2015},
  publisher={Elsevier},
  link={/assets/papers/ACP_RAS.pdf},
  thumb={neubert15change.png},
  abstract={The goal of our work is to support
existing approaches to place recognition by learning how the
visual appearance of an environment changes over time and by
using this learned knowledge to predict its appearance under
different environmental conditions. We describe the general
idea of appearance change prediction (ACP) and investigate
properties of our novel implementation based on vocabularies
of superpixels (SP-ACP). This paper deepens the
understanding of the proposed SP-ACP system and evaluates
the influence of its parameters. We present the results of a largescale
experiment on the complete 10 hour Nordland dataset and
appearance change predictions between different combinations
of seasons.}
}

@article{Suenderhauf10,
  author = {S{\"u}nderhauf, Niko and Protzel, Peter},
  title = {{Learning from Nature: Biologically Inspired Robot Navigation and
    SLAM -- A Review}},
  booktitle = {K{\"u}nstliche Intelligenz (German Journal on Artificial Intelligence),
    Special Issue on SLAM},
  publisher = {Springer Verlag},
  year = {2010},
  address = {Heidelberg},
  journal = {K\"unstliche Intelligenz (German Journal on Artificial Intelligence), Special Issue on SLAM},
  owner = {niko},
  timestamp = {2010.04.14},
  link={/assets/papers/suenderhauf10review.pdf},
  abstract={In this paper we summarize the most important
neuronal fundamentals of navigation in rodents, primates
and humans. We review a number of brain cells that are
involved in spatial navigation and their properties. Furthermore,
we review RatSLAM, a working SLAM system that is
partially inspired by neuronal mechanisms underlying mammalian
spatial navigation.},
thumb={suenderhauf10review.png}
}
