
@inproceedings{raine2024coral,
  title={Human-in-the-Loop Segmentation of Multi-species Coral Imagery},
  author={Raine, Scarlett and Marchant, Ross and Kusy, Brano and Maire, Frederic and S{\"u}nderhauf, Niko and Fischer, Tobias},
  booktitle={CVPR workshop on Learning with Limited Labelled Data for Image and Video Understanding},
  year={2024},
  thumb={raine2024coral.png},
  abstract={We first demonstrate that recent advances in foundation models enable generation of multi-species coral augmented ground truth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN), without the need for any pre-training or custom-designed algorithms. For extremely sparsely labeled images, we propose a labeling regime based on human-in-the-loop principles, resulting in significant improvement in annotation efficiency.},
  arxiv={2404.09406},
  link={https://arxiv.org/abs/2404.09406}
}



@inproceedings{abouchakra2023splatting,
  title={Physically Embodied Gaussian Splatting: Embedding Physical Priors Into a Visual 3d World Model for Robotics},
  author={Abou-Chakra, Jad and Rana, Krishan and Dayoub, Feras and S{\"u}nderhauf, Niko},
  booktitle={Workshop for Neural Representation Learning for Robot Manipulation, Conference on Robot Learning (CoRL)},
  year={2023},
  thumb={abouchakra2023splatting.png},
  link={https://eprints.qut.edu.au/247354/1/Physically_Embodied_Gaussian_Splatting_Embedding_Physical_Priors_into_a_Visual_3D_World_Model_For_Robotics.pdf},
  html={https://embodied-gaussians.github.io/},
  abstract={Our dual Gaussian-Particle representation captures visual (Gaussians) and physical (particles) aspects of the world and enables forward prediction of robot interactions with the world.
     A photometric loss between rendered Gaussians and observed images is computed (Gaussian Splatting) and converted into visual forces. These and other physical phenomena such as gravity, collisions, and mechanical forces are resolved by the always-active physics system and applied to the particles, which in turn influence the position of their associated Gaussians.},
}


@inproceedings{taras2023privacy,
  title={The Need for Inherently Privacy-Preserving Vision in Trustworthy Autonomous Systems},
  author={Taras, Adam K and Suenderhauf, Niko and Corke, Peter and Dansereau, Donald G},
  booktitle={{ICRA Workshop on Multidisciplinary Approaches to Co-Creating Trustworthy Autonomous Systems}},
  arxiv={2303.16408},
  link={https://arxiv.org/pdf/2303.16408.pdf},
  year={2023},
  abstract={This paper is a call to action to consider privacy
in the context of robotic vision. We propose a specific form
privacy preservation in which no images are captured or could
be reconstructed by an attacker even with full remote access.
We present a set of principles by which such systems can be
designed, and through a case study in localisation demonstrate
in simulation a specific implementation that delivers an important robotic capability in an inherently privacy-preserving
manner. This is a first step, and we hope to inspire future works
that expand the range of applications open to sighted robotic
systems.},
  award={Best Poster Award},
  thumb={taras2023privacy.png}
}

@inproceedings{rana2023contrastive,
title={{Contrastive Language, Action, and State Pre-training for Robot Learning}},
author={Krishan Rana and Andrew Melnik and Niko Suenderhauf},
booktitle={ICRA Workshop on Pretraining for Robotics (PT4R)},
year={2023},
link={https://openreview.net/forum?id=sxKR6zhBDH},
thumb={rana2023contrastive.png},
arxiv={2304.10782},
abstract={We introduce a method for unifying language, action, and state information in a shared embedding space to facilitate a range of downstream tasks in robot learning.  Our method, Contrastive Language, Action, and State Pre-training (CLASP), extends the CLIP formulation by incorporating distributional learning, capturing the inherent complexities and one-to-many relationships in behaviour-text alignment. By employing distributional outputs for both text and behaviour encoders, our model effectively associates diverse textual commands with a single behaviour and vice-versa. We demonstrate the utility of our method for the following downstream tasks: zero-shot text-behaviour retrieval, captioning unseen robot behaviours, and learning a behaviour prior for language-conditioned reinforcement learning.}
}



@inproceedings{abouchakra2022implicit,
  title={Implicit Object Mapping With Noisy Data},
  author={Abou-Chakra, Jad and Dayoub, Feras and S{\"u}nderhauf, Niko},
  booktitle={RSS Workshop on Implicit Representations for Robotic Manipulation},
  year={2022},
  arxiv={2204.10516},
  link={https://arxiv.org/abs/2204.10516},
  thumb={abouchakra2022implicit.png},
  abstract={This paper uses the outputs of an object-based SLAM system to bound objects in the scene with coarse primitives and - in concert with instance masks - identify obstructions in the training images. Objects are therefore automatically bounded, and non-relevant geometry is excluded from the NeRF representation. The method's performance is benchmarked under ideal conditions and tested against errors in the poses and instance masks. Our results show that object-based NeRFs are robust to pose variations but sensitive to the quality of the instance masks.}
}



@inproceedings{rana2021zero,
  title={Zero-Shot Uncertainty-Aware Deployment of Simulation Trained Policies on Real-World Robots},
  author={Rana, Krishan and Dasagi, Vibhavari and Haviland, Jesse and Talbot, Ben and Milford, Michael and S{\"u}nderhauf, Niko},
  booktitle={{NeuIPS Workshop on Deployable Decision Makig in Embodied Systems}},
  arxiv={2112.05299},
  year={2021},
  abstract={While deep reinforcement learning (RL) agents have demonstrated incredible potential in attaining dexterous behaviours for robotics, they tend to make errors when deployed in the real world due to mismatches between the training and execution environments. In contrast, the classical robotics community have developed a range of controllers that can safely operate across most states in the real world given their explicit derivation. These controllers however lack the dexterity required for complex tasks given limitations in analytical modelling and approximations. In this paper, we propose Bayesian Controller Fusion (BCF), a novel uncertainty-aware deployment strategy that combines the strengths of deep RL policies and traditional handcrafted controllers.},
  link={https://arxiv.org/abs/2112.05299},
  thumb={rana2021zero.png}
}


@inproceedings{miller2019benchmarking,
  title={Benchmarking Sampling-based Probabilistic Object Detectors},
  author={Miller, Dimity and S{\"u}nderhauf, Niko and Zhang, Haoyang and Hall, David and Dayoub, Feras},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={42--45},
  year={2019},
  abstract={This paper provides the first benchmark for sampling-based probabilistic object detectors. A probabilistic object
  detector expresses uncertainty for all detections that reliably indicates object localisation and classification performance. We compare performance for two sampling-based
  uncertainty techniques, namely Monte Carlo Dropout and Deep Ensembles, when implemented into one-stage and
  two-stage object detectors, Single Shot MultiBox Detector and Faster R-CNN.},
  thumb={miller2019benchmarking.png},
  link={http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Miller_Benchmarking_Sampling-based_Probabilistic_Object_Detectors_CVPRW_2019_paper.pdf}
}


@inproceedings{nicholson18quadricslam,
  title={{QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Semantic SLAM}},
  author={Nicholson, Lachlan and Milford, Michael and S{\"u}nderhauf, Niko},
  booktitle = {{Workshop on Representing a Complex World, International Conference on Robotics and Automation (ICRA)}},
  year={2018},
  link = {https://arxiv.org/abs/1804.04011v1},
  abstract = {We derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D bounding boxes (such as those typically obtained from visual object detection systems) can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for deep-learned object detectors that addresses the challenge of partial object detections often encountered in robotics applications, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.},
  thumb={nicholson18quadricslamworkshop.png},
  award={Best Workshop Paper Award},
  html={http://semanticslam.ai/quadricslam.html},
  arxiv={1804.04011}
}

@inproceedings{Miller17a,
  title={{Dropout Variational Inference Improves Object Detection in Open-Set Conditions}},
  author={Dimity Miller and Lachlan Nicholson and Feras Dayoub and Niko S\"underhauf},
  year={2017},
  booktitle = {{Proc. of NIPS Workshop on Bayesian Deep Learning}},
  status = {workshop},
  link={http://bayesiandeeplearning.org/2017/papers/20.pdf},
  abstract={One of the biggest current challenges of visual object detection is reliable operation in open-set
conditions. One way to handle the open-set problem is to utilize the uncertainty of the model to reject predictions
with low probability. Bayesian Neural Networks (BNNs), with variational inference commonly
used as an approximation, is an established approach to estimate model uncertainty. Here we extend the concept of Dropout sampling to object detection for the first time. We evaluate
Bayesian object detection on a large synthetic and a real-world dataset and show how the estimated
label uncertainty can be utilized to increase object detection performance under open-set conditions.},
  thumb={miller17dropout.png}
}

@inproceedings{Dayoub17,
  title={{Episode-Based Active Learning with Bayesian Neural Networks}},
  author={Dayoub, Feras and S\"underhauf, Niko and Corke, Peter},
  booktitle={Workshop on Deep Learning for Robotic Vision, Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  status = {workshop},
  link={http://openaccess.thecvf.com/content_cvpr_2017_workshops/w5/papers/Dayoub_Episode-Based_Active_Learning_CVPR_2017_paper.pdf},
  abstract={We investigate different strategies for active learning
with Bayesian deep neural networks. We focus our analysis
on scenarios where new, unlabeled data is obtained episodically,
such as commonly encountered in mobile robotics
applications. An evaluation of different strategies for acquisition,
updating, and final training on the CIFAR-10 dataset
shows that incremental network updates with final training
on the accumulated acquisition set are essential for best
performance, while limiting the amount of required human
labeling labor.},
thumb={dayoub17active.png}
}

@inproceedings{Bruce17,
  title={{One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay}},
  author={Jacob Bruce and Niko S\"underhauf and Piotr Mirowski and Raia Hadsell and Michael Milford},
  year={2017},
  booktitle = {{Proc. of NIPS Workshop on Acting and Interacting in the Real World: Challenges in Robot Learning}},
  status = {workshop},
  link={https://arxiv.org/abs/1711.10137},
  abstract={Recently, model-free reinforcement learning algorithms have been shown to solve challenging problems by learning from extensive interaction with the environment. A significant issue with transferring this success to the robotics domain is that interaction with the real world is costly, but training on limited experience is prone to overfitting. We present a method for learning to navigate, to a fixed goal and in a known environment, on a mobile robot. The robot leverages an interactive world model built from a single traversal of the environment, a pre-trained visual feature encoder, and stochastic environmental augmentation, to demonstrate successful zero-shot transfer under real-world environmental variations without fine-tuning.},
  thumb={bruce17navigation.png}
}

@inproceedings{Milford15,
  title={{Sequence Searching with Deep-learnt Depth for Condition-and Viewpoint-invariant Route-based Place Recognition}},
  author={Milford, Michael and Lowry, Stephanie and S\"underhauf, Niko and Shirazi, Sareh and Pepperell, Edward and Upcroft, Ben and Shen, Chunhua and Lin, Guosheng and Liu, Fayao and Cadena, Cesar and Reid, Ian},
  booktitle={Workshop on Computer Vision in Vehicle Technology (CVVT), Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  link={https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W11/papers/Milford_Sequence_Searching_With_2015_CVPR_paper.pdf},
  abstract={Vision-based localization on robots and vehicles remains unsolved when extreme appearance change and viewpoint change are present simultaneously. In this paper we significantly improve the viewpoint invariance of the SeqSLAM algorithm by using state-of-the-art deep learning techniques to generate synthetic viewpoints. Our approach is different to other deep learning approaches in that it does not rely on the ability of the CNN network to learn invariant features, but only to produce“good enough” depth images from day-time imagery only. We evaluate the system on a new multi-lane day-night car dataset specifically gathered to simultaneously test both appearance and viewpoint change.},
  thumb={milford15.png}
}


@INPROCEEDINGS{Suenderhauf15b,
  author = {Niko S\"underhauf and Ben Upcroft and Michael Milford},
  title = {{Continuous Factor Graphs For Holistic Scene Understanding}},
  booktitle = {Workshop on Scene Understanding (SUNw), Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)},
  year = {2015},
  owner = {niko},
  timestamp = {2015.06.05},
  thumb={suenderhauf15b.png},
  link={https://eprints.qut.edu.au/109683/1/109683.pdf},
  abstract={We propose a novel mathematical formulation for the
holistic scene understanding problem and transform it from
the discrete into the continuous domain. The problem can
then be modeled with a nonlinear continuous factor graph,
and the MAP solution is found via least squares optimization.
We evaluate our method on the realistic NYU2 dataset.}
}



@INPROCEEDINGS{McMahon15,
  author = {Sean McMahon and Niko S\"underhauf and Ben Upcroft and Michael Milford},
  title = {{How Good Are EdgeBoxes, Really?}},
  booktitle = {Workshop on Scene Understanding (SUNw), Intl. Conf. on Computer Vision and Pattern Recognition (CVPR)},
  year = {2015},
  owner = {niko},
  timestamp = {2015.06.05}
}


@InProceedings{Suenderhauf15d,
  Title                    = {{SLAM -- Quo Vadis? In Support of Object Oriented and Semantic SLAM}},
  Author                   = {Niko S\"underhauf and Feras Dayoub and Sean McMahon and Markus Eich and Ben Upcroft and Michael Milford},
  Booktitle                = {Workshop on The Problem of Moving Sensors, Robotics: Science and Systems (RSS)},
  Year                     = {2015},

  Owner                    = {niko},
  Timestamp                = {2015.06.05},
  link={https://eprints.qut.edu.au/109668/1/109668.pdf},
  thumb={suenderhauf15d.png},
  abstract={Most current SLAM systems are still based on
primitive geometric features such as points, lines, or planes.
The created maps therefore carry geometric information, but
no immediate semantic information. With the recent significant
advances in object detection and scene classification we think the
time is right for the SLAM community to ask where the SLAM
research should be going during the next years. As a possible
answer to this question, we advocate developing SLAM systems
that are more object oriented and more semantically enriched
than the current state of the art. This paper provides an overview
of our ongoing work in this direction.}
}

@inproceedings{sunderhauf2014fine,
  title={Fine-Grained Plant Classification Using Convolutional Neural Networks for Feature Extraction.},
  author={S{\"u}nderhauf, Niko and McCool, Chris and Upcroft, Ben and Perez, Tristan},
  booktitle={CLEF (Working Notes)},
  pages={756--762},
  year={2014}
}

@INPROCEEDINGS{Suenderhauf05b,
  author = {Niko S{\"u}nderhauf and Kurt Konolige and Thomas Lemaire and Simon
    Lacroix},
  title = {{Comparison of Stereovision Odometry Approaches.}},
  booktitle = {Proceedings of IEEE International Conference on Robotics and Automation,
    Planetary Rover Workshop},
  year = {2005}
}


@INPROCEEDINGS{Suenderhauf13d,
  author = {Niko S\"underhauf and Peer Neubert and Peter Protzel},
  title = {{Predicting the Change -- A Step Towards Life-Long Operation in Everyday Environments}},
  booktitle = {Proceedings of Robotics: Science and Systems (RSS) Robotics Challenges
    and Vision Workshop},
  year = {2013},
  owner = {niko},
  timestamp = {2013.07.02},
  link={/assets/papers/rss13Workshop.pdf}
}

@INPROCEEDINGS{Suenderhauf13b,
  author = {S\"underhauf, Niko and Neubert, Peer and Protzel, Peter},
  title = {{Are We There Yet? Challenging SeqSLAM on a 3000 km Journey Across All
    Four Seasons.}},
  booktitle = {Proceedings of Workshop on Long-Term Autonomy, IEEE International
    Conference on Robotics and Automation (ICRA)},
  year = {2013},
  owner = {niko},
  timestamp = {2013.04.29},
  link={/assets/papers/openseqslam.pdf}
}





@INPROCEEDINGS{Suenderhauf12d,
  author = {S{\"u}nderhauf, Niko and Protzel, Peter},
  title = {{A Generic Approach for Robust Probabilistic Estimation with Graphical
    Models}},
  booktitle = {Proc. of RSS Workshop on Long-term Operation of Autonomous Robotic
    Systems in Changing Environments},
  year = {2012},
  address = {Sydney, Australia},
  owner = {niko},
  timestamp = {2012.08.20}
}
